{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe59b869",
   "metadata": {},
   "source": [
    "# Data Processing and Transformation\n",
    "\n",
    "\n",
    "Data processing and transformation tasks are the foundation of many data engineering activities. This involves a variety of operations such as:\n",
    "\n",
    "<ul>\n",
    "    <li><b>Cleaning:</b> Correcting or removing any inaccuracies or errors in the data.</li>\n",
    "    <li><b>Filtering:</b> Selecting a subset of the data based on certain conditions.</li>\n",
    "    <li><b>Aggregation:</b> Combining multiple data points into a single data point (e.g., summing, averaging).</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## Tools and libraries for data processing and transformation in Python\n",
    "\n",
    "\n",
    "`Python` offers several libraries that are widely used for data processing and transformation tasks:\n",
    "\n",
    "<ol>\n",
    "    <li><b>Pandas</b> is a popular library for data manipulation and analysis. It provides data structures and functions needed to manipulate structured data.</li>&nbsp;\n",
    "    <li><b>NumPy</b> is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.</li>&nbsp;\n",
    "    <li><b>Dask</b> is a flexible library for parallel computing in Python that makes it easy to build intuitive workflows for ingesting, filtering, and manipulating data.</li>&nbsp;\n",
    "</ol>    \n",
    "\n",
    "\n",
    "<hr style=\"background: linear-gradient(to right, #f00, #00f); height: 5px; border: none;\" />\n",
    "\n",
    "\n",
    "### Using `Pandas` for \n",
    "\n",
    "#### 1. Data Cleaning\n",
    "\n",
    "Let's consider a simple example where we clean a dataset using Pandas.\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> pandas <font color=\"blue\">as</font> pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(<font color=\"green\">'data.csv'</font>)\n",
    "\n",
    "# Fill NA/NaN values using the specified method\n",
    "df.fillna(<font color=\"magenta\">0</font>, inplace=<font color=\"blue\">True</font>)\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=<font color=\"blue\">True</font>)\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "In this code snippet, we first load the dataset using the pd.read_csv() function. Then, we replace all NA/NaN values with 0 using the `df.fillna()` function. Finally, we remove duplicate rows using the `df.drop_duplicates()` function.\n",
    "\n",
    "\n",
    "Another example\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> pandas <font color=\"blue\">as</font> pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(<font color=\"green\">'data.csv'</font>)\n",
    "\n",
    "# Replace \"unknown\" values with NaN\n",
    "df.replace(<font color=\"green\">\"unknown\"</font>, pd.NaT, inplace=<font color=\"blue\">True</font>)\n",
    "\n",
    "# Drop rows with any NaN in the 'age' and 'income' columns\n",
    "df.dropna(subset=[<font color=\"green\">'age'</font>, <font color=\"green\">'income'</font>], inplace=<font color=\"blue\">True</font>)\n",
    "\n",
    "# Replace outlier ages above 100 with the median age\n",
    "median_age = df[<font color=\"green\">'age'</font>].median()\n",
    "df.loc[df[<font color=\"green\">'age'</font>] > <font color=\"magenta\">100</font>, <font color=\"green\">'age'</font>] = median_age\n",
    "\n",
    "# Convert income column to numeric, errors to NaN\n",
    "df[<font color=\"green\">'income'</font>] = pd.to_numeric(df[<font color=\"green\">'income'</font>], errors=<font color=\"green\">'coerce'</font>)\n",
    "</code></pre>\n",
    "\n",
    "In this snippet, we first replace \"unknown\" values with Pandas' representation for missing data, NaN. We then drop rows with any missing age or income data. We also replace ages over 100 (assumed to be outlier data) with the median age. Finally, we ensure the income column is numeric, with any conversion errors resulting in NaN values.\n",
    "\n",
    "\n",
    "#### 2. Filtering\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> pandas <font color=\"blue\">as</font> pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(<font color=\"green\">'data.csv'</font>)\n",
    "\n",
    "# Filtering the data where 'age' is greater than 30\n",
    "df_filtered = df[df[<font color=\"green\">'age'</font>] > <font color=\"magenta\">30</font>]\n",
    "\n",
    "# Filtering data where 'income' is greater than 50000 and 'age' is less than 50\n",
    "df_filtered = df[(df[<font color=\"green\">'income'</font>] > <font color=\"magenta\">50000</font>) &amp; (df[<font color=\"green\">'age'</font>] &lt; <font color=\"magenta\">50</font>)]\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "#### 3. Aggregation\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> pandas <font color=\"blue\">as</font> pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(<font color=\"green\">'data.csv'</font>)\n",
    "\n",
    "# Calculate the mean 'income' by 'age'\n",
    "df_grouped = df.groupby(<font color=\"green\">'age'</font>)[<font color=\"green\">'income'</font>].mean()\n",
    "\n",
    "# Calculate the total 'income' and average 'spending' by 'region'\n",
    "df_grouped = df.groupby(<font color=\"green\">'region'</font>).agg({<font color=\"green\">'income'</font>: <font color=\"green\">'sum'</font>, <font color=\"green\">'spending'</font>: <font color=\"green\">'mean'</font>})\n",
    "</code></pre>\n",
    "\n",
    "These are the basics of filtering and aggregation with Pandas, which are integral to many data processing and transformation tasks. The filtering allows you to select subsets of your data that meet certain criteria, while aggregation functions let you compute summary statistics over different groups in your data.\n",
    "\n",
    "\n",
    "### Using `numpy` for\n",
    "\n",
    "#### 1. Cleaning\n",
    "\n",
    "Data cleaning with `NumPy` is possible but less straightforward and typically more difficult than using Pandas, especially when dealing with real-world data. If the data is purely numerical without any missing values, NumPy could still be used, but Pandas is generally the better option for data cleaning tasks.\n",
    "\n",
    "\n",
    "#### 2. Filtering\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> numpy <font color=\"blue\">as</font> np\n",
    "\n",
    "# Create an array\n",
    "arr = np.array([<font color=\"magenta\">1</font>, <font color=\"magenta\">2</font>, <font color=\"magenta\">3</font>, <font color=\"magenta\">4</font>, <font color=\"magenta\">5</font>])\n",
    "\n",
    "# Filtering the array where value is greater than 3\n",
    "filtered_arr = arr[arr > <font color=\"magenta\">3</font>]\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "#### 3. Aggregation\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> numpy <font color=\"blue\">as</font> np\n",
    "\n",
    "# Create an array\n",
    "arr = np.array([<font color=\"magenta\">1</font>, <font color=\"magenta\">2</font>, <font color=\"magenta\">3</font>, <font color=\"magenta\">4</font>, <font color=\"magenta\">5</font>])\n",
    "\n",
    "# Calculate the mean of the array\n",
    "mean_arr = np.mean(arr)\n",
    "\n",
    "# Calculate the sum of the array\n",
    "sum_arr = np.sum(arr)\n",
    "</code></pre>\n",
    "\n",
    "The key takeaway is that while you can use NumPy for these data processing tasks, it's typically more efficient and easier to use a library like Pandas that's specifically designed for this type of work. `NumPy`'s strength lies more in numerical computations on arrays and matrices, rather than general data processing and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f6705",
   "metadata": {},
   "source": [
    "### Using `Dask` for \n",
    "\n",
    "#### 1. Cleaning\n",
    "\n",
    "Dask, being a parallel computing library, can efficiently handle larger datasets, making it a suitable tool for data cleaning tasks, especially when the data volume exceeds the memory capacity. Here, we illustrate a basic example of using Dask to clean data:\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> dask.dataframe <font color=\"blue\">as</font> dd\n",
    "\n",
    "# Load the dataset\n",
    "df = dd.read_csv(<font color=\"green\">'large_data.csv'</font>)\n",
    "\n",
    "# Replace \"unknown\" values with NaN\n",
    "df = df.replace(<font color=\"green\">\"unknown\"</font>, dd.NA)\n",
    "\n",
    "# Drop rows with any NaN in the 'age' and 'income' columns\n",
    "df = df.dropna(subset=[<font color=\"green\">'age'</font>, <font color=\"green\">'income'</font>])\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "#### 2. Filtering\n",
    "\n",
    "Dask also offers powerful data filtering options. Here, we showcase how to perform data filtering using Dask:\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> dask.dataframe <font color=\"blue\">as</font> dd\n",
    "\n",
    "# Load the dataset\n",
    "df = dd.read_csv(<font color=\"green\">'large_data.csv'</font>)\n",
    "\n",
    "# Filtering the data where 'age' is greater than 30\n",
    "df_filtered = df[df[<font color=\"green\">'age'</font>] > <font color=\"magenta\">30</font>]\n",
    "\n",
    "# Filtering data where 'income' is greater than 50000 and 'age' is less than 50\n",
    "df_filtered = df[(df[<font color=\"green\">'income'</font>] > <font color=\"magenta\">50000</font>) &amp; (df[<font color=\"green\">'age'</font>] &lt; <font color=\"magenta\">50</font>)]\n",
    "</code></pre>\n",
    "\n",
    "#### 3. Aggregation\n",
    "\n",
    "For data aggregation, Dask can be used to efficiently group and aggregate large datasets. Here's an example:\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> dask.dataframe <font color=\"blue\">as</font> dd\n",
    "\n",
    "# Load the dataset\n",
    "df = dd.read_csv(<font color=\"green\">'large_data.csv'</font>)\n",
    "\n",
    "# Calculate the mean 'income' by 'age'\n",
    "df_grouped = df.groupby(<font color=\"green\">'age'</font>)[<font color=\"green\">'income'</font>].mean().compute()\n",
    "\n",
    "# Calculate the total 'income' and average 'spending' by 'region'\n",
    "df_grouped = df.groupby(<font color=\"green\">'region'</font>).agg({<font color=\"green\">'income'</font>: <font color=\"green\">'sum'</font>, <font color=\"green\">'spending'</font>: <font color=\"green\">'mean'</font>}).compute()\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "### Best Practices for Optimizing Data Processing and Transformation Pipelines\n",
    "\n",
    "#### 1. Introduction\n",
    "\n",
    "The effectiveness of data pipelines can be significantly enhanced by following best practices for optimization. These include parallelizing data flow, implementing data quality checks, and creating reusable generic pipelines.\n",
    "\n",
    "#### 2. Optimizing Data Processing with `Pandas`\n",
    "\n",
    "When working with Pandas, optimization can be achieved by using appropriate data types, avoiding looping over dataframes, and leveraging built-in functions for data manipulation. Here's an advanced example demonstrating optimization techniques with Pandas:\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> pandas <font color=\"blue\">as</font> pd\n",
    "\n",
    "# Load the dataset in chunks to optimize memory usage\n",
    "df = pd.read_csv(<font color=\"green\">'large_data.csv'</font>, chunksize=<font color=\"magenta\">10000</font>)\n",
    "\n",
    "# Using apply function to optimize data transformation\n",
    "df['new_column'] = df.apply(lambda row: row['column1'] + row['column2'], axis=<font color=\"magenta\">1</font>)\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "#### 3. Optimizing Data Processing with `NumPy`\n",
    "\n",
    "In NumPy, optimization can be achieved by utilizing vectorized operations, using appropriate data types, and avoiding unnecessary copies of arrays. Here's an example demonstrating optimization techniques with NumPy:\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> numpy <font color=\"blue\">as</font> np\n",
    "\n",
    "# Create a large array\n",
    "arr = np.arange(<font color=\"magenta\">1000000</font>)\n",
    "\n",
    "# Using vectorized operations for optimization\n",
    "result = np.sum(arr)\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "#### 4. Optimizing Data Processing with `Dask`\n",
    "\n",
    "With Dask, optimization can be achieved by utilizing its lazy evaluation feature, which allows for efficient memory usage and parallel computation. Here's an example demonstrating optimization techniques with Dask:\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> dask.array <font color=\"blue\">as</font> da\n",
    "\n",
    "# Create a large Dask array\n",
    "arr = da.ones((<font color=\"magenta\">1000000</font>, <font color=\"magenta\">1000</font>), chunks=(<font color=\"magenta\">100000</font>, <font color=\"magenta\">1000</font>))\n",
    "\n",
    "# Using lazy evaluation for optimization\n",
    "result = arr.sum().compute()\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c4f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
