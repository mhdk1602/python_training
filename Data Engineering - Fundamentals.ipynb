{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380e4c74",
   "metadata": {},
   "source": [
    "# Whats is Data Engineering?\n",
    "\n",
    "It is simply a more glamorized term for <code>ETL/ELT</code> practices. Tradtional ETL/ELT were implemented on predominantly on-prem. Modern Data Engineering spans from on-prem to be hosted on Cloud services to being serverless.<br><br>\n",
    "\n",
    "\n",
    "Data Eningeering as a principle consists of the following. <br><br>\n",
    "\n",
    "<div><font color = 'dodgerblue'><b>1. Data Collection<br>\n",
    "2. Data Processing<br>\n",
    "3. Data Storage<br>\n",
    "4. Data Modeling<br>\n",
    "5. Data Analysis<br>\n",
    "6. Data Visualization<br>\n",
    "    7. Data Consumption</b></font></div>\n",
    "\n",
    "\n",
    "A Data pipeline is a collective representation of all or some of the steps listed above. <br><br>\n",
    "\n",
    "<font color = 'dodgerblue'><b>1. Data Collection</b></font><br>\n",
    "The first step in a data-pipeline is data collection from various heterogenous sources such as files, databases, APIs and streamsets. It is important to understand how to securely collect the data and scale the system as the input volume increases. <br>\n",
    "\n",
    "As a data engineer you should be familiar with parsing input schemas and file handling. The schema defines the structure of the data and type of fields being ingested. The input schema also consists of column and line delimiters.<br> \n",
    "\n",
    "For example if you are reading a csv file, it is a comma separated schema. \n",
    "\n",
    "<div><code><font color = 'indigo'>import csv<br>\n",
    "#Define the input schema<br>\n",
    "schema = {\n",
    "    'name': str,\n",
    "    'age': int,\n",
    "    'gender': str,\n",
    "    'city': str,\n",
    "    'country': str\n",
    "}<br>\n",
    "#Open the CSV file\n",
    "with open('data.csv', newline='') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    # Loop through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # Parse the row using the input schema\n",
    "        parsed_row = {key: schema[key](row[key]) for key in schema}\n",
    "        # Process the parsed row\n",
    "        print(parsed_row)</font></code></div><br>\n",
    "\n",
    "\n",
    "Reading input files through inferring the source schema without having to explicitly specify the same.<br><br>\n",
    "\n",
    "<div><code><font color = 'indigo'>import csv<br>\n",
    "#Open the CSV file\n",
    "with open('data.csv', newline='') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Keep track of the header row\n",
    "    header = next(reader)\n",
    "    # Loop through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # Create a dictionary to hold the parsed row\n",
    "        parsed_row = {}\n",
    "        # Loop through each field in the row\n",
    "        for i in range(len(row)):\n",
    "            # Check if the field is in the header\n",
    "            if i < len(header):\n",
    "                # If it is, use the field name as the key and parse the value\n",
    "                parsed_row[header[i]] = int(row[i])\n",
    "            else:\n",
    "                # If it's not, assume it's a new field and parse the value as a string\n",
    "                parsed_row[f'new_field_{i - len(header)}'] = str(row[i])\n",
    "        # Process the parsed row as needed\n",
    "        print(parsed_row)</font></code></div><br>\n",
    "\n",
    "\n",
    "Inferring dynamic schema however can cause issues in the downstream if the data-pipelines do not have the same dynamic schema inference. It would be best to process the data but to send an error message both downstream and upstream notifying the change to conclude if this is an error or needs a change management to process the new schema going foreward.<br><br>\n",
    "\n",
    "\n",
    "<font color = 'dodgerblue'><b>2. Data processing</b></font> is the second step in the data engineering pipeline, after data collection. It involves cleaning, transforming, and enriching the data so that it can be used for analysis.\n",
    "\n",
    "<b>1. Data Cleaning:</b> Data collected from different sources may contain errors, missing values, or outliers that need to be cleaned. Data cleaning involves identifying and correcting errors, filling in missing values, and removing outliers.\n",
    "\n",
    "Lets assume I'm reading a csv file and the downstream consumers wants 100% data availability. You can help set this up by discussing what needs to be set in place of missing values. In the below example we assume the consumers want the missing values to be the mean of the column value.<br><br>\n",
    "\n",
    "<div><code><font color = 'indigo'>import pandas as pd<br>\n",
    "#Read in the data\n",
    "df = pd.read_csv('data.csv')<br>\n",
    "#Replace missing values with the mean of the column\n",
    "df.fillna(df.mean(), inplace=True)</font></code></div><br>\n",
    "    \n",
    "    \n",
    "<b>2. Data transformation. </b>Once the data is cleaned, it needs to be transformed into a format that is suitable for analysis. This can involve converting data types, aggregating data, or splitting data into multiple tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4062bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
