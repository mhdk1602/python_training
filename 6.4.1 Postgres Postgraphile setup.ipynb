{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c093ab43",
   "metadata": {},
   "source": [
    "# Streamlining GraphQL APIs with PostGraphile and Docker\n",
    "\n",
    "Embarking on a journey to explore the synergies between GraphQL and PostgreSQL, we have chosen to leverage PostGraphile, a powerful tool that bridges the gap between PostgreSQL databases and GraphQL APIs, promising rapid and seamless development cycles. Here's how our expedition is set to unfold:\n",
    "\n",
    "1. **Setting Up a Docker Container**: We initiate our project by orchestrating a Docker container equipped with a PostgreSQL service. This foundational step creates a robust and manageable development environment, paving the way for seamless integration with PostGraphile.\n",
    "\n",
    "2. **Understanding Idempotency**: As we venture deeper, we turn our focus towards grasping the pivotal concept of idempotency. Understanding its significance in database initialization scripts is crucial, as it ensures that our setup scripts can be executed multiple times without causing adverse effects, thereby promising stability and reliability.\n",
    "\n",
    "3. **Loading Tables**: As we progress, our attention shifts to formulating and executing Data Definition Language (DDL) scripts. This step is vital in initializing our PostgreSQL database with the requisite tables, ensuring operations adhere to idempotent principles, thereby fostering consistency and reliability.\n",
    "\n",
    "4. **Initializing PostGraphile**: With our database structured, we now focus on initializing PostGraphile. This powerful tool automatically generates a GraphQL schema from our PostgreSQL database, significantly simplifying the setup process and facilitating seamless querying capabilities, all while maintaining the flexibility to customize and extend the schema as needed.\n",
    "\n",
    "5. **Querying with a GraphQL Client**: In the final leg of our journey, we explore the querying capabilities facilitated by PostGraphile. We will navigate through the process of crafting and executing queries using a GraphQL client, enabling flexible and efficient data retrieval from our database, thus showcasing the powerful integration of GraphQL and PostgreSQL orchestrated through Docker.\n",
    "\n",
    "\n",
    "By the end of this guide, you will be adept at setting up a streamlined GraphQL API using PostGraphile and Docker, marking a significant milestone in your journey with GraphQL and relational databases.y.\n",
    "\n",
    "\n",
    "### Setting Up a Docker Container\n",
    "\n",
    "In the initial phase of our project, we will be setting up a Docker container to host our PostgreSQL service. But why choose Docker in the first place? Here are a few compelling reasons:\n",
    "\n",
    "1. **Environment Consistency**: Docker ensures that your application runs the same regardless of where Docker is running. This eliminates the classic problem of \"it works on my machine\" scenarios, fostering consistency across development, testing, and production environments.\n",
    "   \n",
    "2. **Isolation and Security**: Docker containers are isolated from each other, which means that they have their own environments and file systems. This isolation enhances security by containing any potential application breaches to the individual container.\n",
    "\n",
    "3. **Ease of Setup and Use**: Setting up databases can sometimes be a complex task involving many steps. Docker containers can encapsulate all these complexities, allowing you to set up services like PostgreSQL with just a few commands. This ease of use accelerates the development cycle significantly.\n",
    "\n",
    "4. **Resource Efficiency**: Docker containers share the host system's kernel, rather than including their own operating system. This makes them lightweight and efficient in terms of system resources, which allows running many containers on a host machine without straining system resources.\n",
    "\n",
    "5. **Community and Ecosystem**: Docker has a vibrant community and a rich ecosystem of pre-built images available on Docker Hub. This means you can leverage the work of thousands of others to quickly and easily set up and deploy services.\n",
    "\n",
    "With these advantages in mind, our first step is to set up a Docker container running a PostgreSQL service. This process involves creating a `docker-compose.yml` file to define the service configurations and using Docker Compose commands to manage the lifecycle of our containers. \n",
    "\n",
    "Our PostgreSQL service configuration in the `docker-compose.yml` file will look something like this:\n",
    "\n",
    "```yaml\n",
    "version: '3.1'\n",
    "\n",
    "services:\n",
    "  postgres:\n",
    "    build: \n",
    "      context: ./postgres\n",
    "      dockerfile: Dockerfile\n",
    "    ports:\n",
    "      - \"5437:5432\"\n",
    "    environment:\n",
    "      POSTGRES_USER: godzilla\n",
    "      POSTGRES_PASSWORD: Mrawww\n",
    "      POSTGRES_DB: monsterverse\n",
    "      DATABASE_URL: postgres://godzilla:Mrawww@postgres/monsterverse\n",
    "      SHADOW_DATABASE_URL: postgres://godzilla:Mrawww@postgres/monsterverse_shadow\n",
    "      ROOT_DATABASE_URL: postgres://godzilla:Mrawww@postgres/postgres\n",
    "    volumes:\n",
    "      - ./postgres/data:/var/lib/postgresql/data\n",
    "      - ./postgres/init:/docker-entrypoint-initdb.d/\n",
    "    networks:\n",
    "      - my-network\n",
    "    restart: on-failure:10\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"pg_isready -U godzilla -d monsterverse -q && psql -U godzilla -d monsterverse -c 'SELECT 1' | grep 1\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 5\n",
    "\n",
    "networks:\n",
    "  my-network:\n",
    "    driver: bridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d481cf2",
   "metadata": {},
   "source": [
    "<br>\n",
    "In this configuration file:\n",
    "\n",
    "- The `services` block defines the PostgreSQL service, including build context, Dockerfile location, port mapping, and environment variables.\n",
    "- The `volumes` directive maps local folders to folders inside the container, facilitating data persistence and initialization script execution.\n",
    "- The `networks` block defines a custom network for facilitating communication between different services in Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416a2d8",
   "metadata": {},
   "source": [
    "### Structuring Your Docker Project\n",
    "\n",
    "Before we delve into idempotency, it's important to understand the structure of our Docker project and the purpose behind each component. Let's break down the elements of the project:\n",
    "\n",
    "#### 1. **Postgres Folder**\n",
    "\n",
    "This folder serves as the central location where all PostgreSQL related files are stored. It helps in organizing your project by segregating the database files from other components of your application. Hereâ€™s a closer look at the important files and folders within the `postgres` folder:\n",
    "\n",
    "##### a. **Dockerfile**\n",
    "\n",
    "The `Dockerfile` is a blueprint that contains instructions for building a Docker image, which in turn is used to create containers. In our case, the `Dockerfile` includes instructions for setting up a PostgreSQL service. It specifies the base image to use (PostgreSQL in our case), environment variables, and other configurations necessary to run the PostgreSQL service.\n",
    "\n",
    "\n",
    "<pre><code>\n",
    "# Use the official image as a parent image\n",
    "FROM postgres:latest\n",
    "\n",
    "# Set the working directory in the container to /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY . /app\n",
    "\n",
    "# Make port 5432 available to the world outside this container\n",
    "EXPOSE 5432</code></pre>\n",
    "\n",
    "##### b. **init Folder**\n",
    "\n",
    "The `init` folder contains SQL scripts that are executed when the PostgreSQL container is initialized. These scripts are used to set up the database schema, create tables, and seed initial data. The scripts in this folder are executed in alphabetical order, hence naming them as `01_init.sql`, `02_init.sql`, etc., helps in controlling the execution order.\n",
    "\n",
    "##### c. **data Folder**\n",
    "\n",
    "This folder is mapped to the data directory of the PostgreSQL service inside the container. By mapping this folder as a volume, the data stored in the database persists even when the container is removed, ensuring that you don't lose your data when you bring down the Docker container.\n",
    "\n",
    "##### d. **init.sql Files**\n",
    "\n",
    "Files like `01_init.sql` and `02_init.sql` in the `init` folder contain SQL scripts to initialize the database. These scripts can include commands to create databases, tables, and populate them with initial data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8c77f",
   "metadata": {},
   "source": [
    "### Understanding Idempotency\n",
    "\n",
    "As we venture deeper into setting up our Dockerized PostgreSQL service, it's critical to grasp the concept of idempotency, a fundamental property that ensures the stability and reliability of our database initialization scripts.\n",
    "\n",
    "#### **What is Idempotency?**\n",
    "\n",
    "In the context of database operations, idempotency refers to the property of certain operations where they can be applied multiple times without changing the result beyond the initial application. In simpler terms, an idempotent operation, when executed multiple times, has the same effect as if it were executed just once.\n",
    "\n",
    "#### **Why is Idempotency Important?**\n",
    "\n",
    "1. **Reliable Initialization**: Ensuring that our initialization scripts are idempotent means that we can run them multiple times without worrying about adverse effects or inconsistencies in our database. This is particularly useful during the development phase where the database setup might change frequently.\n",
    "\n",
    "2. **Error Recovery**: In case of errors or interruptions during the initialization process, idempotent scripts allow for safe retries without the risk of duplicating data or corrupting the database state.\n",
    "\n",
    "3. **Simplified Maintenance**: Idempotent scripts simplify maintenance and updates, as they can be rerun safely whenever changes are made, without requiring complex checks or conditional logic.\n",
    "\n",
    "#### **Implementing Idempotency in SQL Scripts**\n",
    "\n",
    "To implement idempotency in SQL scripts, we can use conditional statements to check the existence of objects (like tables or databases) before attempting to create them. For instance, before creating a table, we can check if it already exists, and only create it if it doesn't. Hereâ€™s how you can do this in PostgreSQL:\n",
    "\n",
    "```sql\n",
    "DO\n",
    "$$\n",
    "BEGIN\n",
    "    IF NOT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'mytable' AND table_schema = 'public') THEN\n",
    "        CREATE TABLE public.mytable (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            name VARCHAR(50)\n",
    "        );\n",
    "    END IF;\n",
    "END\n",
    "$$;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289945ca",
   "metadata": {},
   "source": [
    "In this script, the IF NOT EXISTS clause checks if the table 'mytable' already exists in the 'public' schema, and if not, it proceeds to create the table. This ensures that the script is idempotent and can be run multiple times without causing errors or duplications.\n",
    "\n",
    "As we move forward, we will be applying the principle of idempotency to our database initialization scripts, ensuring a robust and reliable setup process.\n",
    "\n",
    "Now before we move to the next step of loading data into the postgres instance. Let's walk through designing the data model and exploring some code for data prep.\n",
    "\n",
    "<b> Data Model: </b><br><br>\n",
    "<img src=\"https://i.postimg.cc/FFYfdNry/ticker-schema.png\" height = \"800\" width = \"1000\"><br><br>\n",
    "\n",
    "### Exploring the Data Model\n",
    "\n",
    "Our data model, depicted in the ER diagram above, is a meticulous representation of a financial database capturing detailed information about stock portfolios, company overviews, earnings, and stock prices at daily and intraday levels. Let's delve deeper into each entity in the model and understand their roles and relationships:\n",
    "\n",
    "#### **Entities and Attributes**\n",
    "\n",
    "1. **stocks**: \n",
    "    - **ticker (PK)**: A unique identifier representing the stock symbol of a company.\n",
    "    - **name**: The name of the company corresponding to the stock symbol.\n",
    "\n",
    "2. **portfolio**:\n",
    "    - **portfolio_id (PK)**: A unique identifier for each transaction in the portfolio.\n",
    "    - **ticker (FK)**: The stock symbol involved in the transaction, referencing the stocks entity.\n",
    "    - **transaction_date**: The date of the transaction.\n",
    "    - **action**: The type of transaction - either \"Buy\" or \"Sell\".\n",
    "    - **volume**: The number of shares involved in the transaction.\n",
    "    - **time**: The time at which the transaction took place.\n",
    "    - **close (optional)**: The per-share price at the time of the transaction.\n",
    "    - **total_transaction_amount (optional)**: The total value of the transaction.\n",
    "\n",
    "3. **company_overview**:\n",
    "    - **ticker (PK, FK)**: The stock symbol, serving as a foreign key referencing the stocks entity.\n",
    "    - **sector**: The business sector of the company.\n",
    "    - **industry**: The specific industry category within the sector.\n",
    "    - **market_cap**: The market capitalization value, formatted as a string.\n",
    "    - **description**: A description of the company.\n",
    "    - **as_of_date**: The date as of which the data is valid.\n",
    "\n",
    "4. **earnings**:\n",
    "    - **earnings_id (PK)**: A unique identifier for each earnings record.\n",
    "    - **ticker (FK)**: The stock symbol, referencing the stocks entity.\n",
    "    - **fiscal_year**: The fiscal year of the earnings data.\n",
    "    - **fiscal_period**: The fiscal quarter of the earnings data.\n",
    "    - **eps**: Earnings per share for the given period.\n",
    "    - **as_of_date**: The date as of which the earnings data is valid.\n",
    "\n",
    "5. **ticker_daily**:\n",
    "    - **daily_price_id (PK)**: A unique identifier for each daily price record.\n",
    "    - **ticker (FK)**: The stock symbol, referencing the stocks entity.\n",
    "    - **date**: The date of the price data.\n",
    "    - **open**: The opening price of the stock on the given date.\n",
    "    - **high**: The highest price of the stock on the given date.\n",
    "    - **low**: The lowest price of the stock on the given date.\n",
    "    - **close**: The closing price of the stock on the given date.\n",
    "    - **volume**: The number of shares traded on the given date.\n",
    "\n",
    "6. **ticker_intraday**:\n",
    "    - **intraday_price_id (PK)**: A unique identifier for each intraday price record.\n",
    "    - **ticker (FK)**: The stock symbol, referencing the stocks entity.\n",
    "    - **date_time**: The specific date and time of the intraday price data.\n",
    "    - **open**: The opening price in the given intraday interval.\n",
    "    - **high**: The highest price in the given intraday interval.\n",
    "    - **low**: The lowest price in the given intraday interval.\n",
    "    - **close**: The closing price at the end of the given intraday interval.\n",
    "    - **volume**: The number of shares traded during the intraday interval.\n",
    "\n",
    "#### **Relationships**\n",
    "\n",
    "The relationships between the entities are depicted as lines connecting them in the ER diagram, indicating how they are related and the nature of their relationships, which are described below:\n",
    "\n",
    "- **stocks ||--o{ portfolio**: A one-to-many relationship indicating that a stock can be included in multiple portfolio transactions.\n",
    "- **stocks ||--|| company_overview**: A one-to-one relationship indicating that each stock has a single company overview.\n",
    "- **stocks ||--o{ earnings**: A one-to-many relationship indicating that a stock can have multiple earnings reports.\n",
    "- **stocks ||--o{ ticker_daily**: A one-to-many relationship indicating that a stock can have multiple daily price records.\n",
    "- **ticker_daily ||--o{ ticker_intraday**: A one-to-many relationship indicating that each daily price record can have multiple intraday price records.\n",
    "\n",
    "#### **Optimization and Schema Type**\n",
    "\n",
    "The chosen schema is a Star Schema, optimized for querying large data sets, and is particularly useful in data warehouse environments. This schema type allows for efficient querying as it reduces the number of joins needed when querying related data, facilitating quick data retrieval. It is optimized for readability and ease of understanding, ensuring that users can construct queries with minimal complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc5f03",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "\n",
    "For the next steps, we will be using yfinance python package and polygon.io for fetching daily, and intraday prices for a set of stock tickers and its company info and earnings data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Prevent truncation of column width\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b6d29e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: psycopg2-binary in /opt/homebrew/lib/python3.9/site-packages (2.9.5)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b93ef9",
   "metadata": {},
   "source": [
    "<br>\n",
    "Lets start by building out the `stocks` table. We pick a list of 13 tickers and get the company name from `yfinance`<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489fb288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame for the STOCKS table\n",
    "stocks_df = pd.DataFrame(columns=['Ticker', 'Name'])\n",
    "\n",
    "# List of tickers to include in the STOCKS table\n",
    "tickers = [\"META\", \"AAPL\", \"NVDA\", \"TSLA\", \"NFLX\", \"TSM\", \"VOO\", \"VTI\", \"AMD\", \"INTC\", \"GE\", \"MSFT\", \"GOOG\"]\n",
    "\n",
    "# Populate the STOCKS DataFrame with Ticker symbols and Names using yfinance\n",
    "for ticker_symbol in tickers:\n",
    "    ticker = yf.Ticker(ticker_symbol)\n",
    "    info = ticker.info\n",
    "    stocks_df = stocks_df.append({'Ticker': ticker_symbol, 'Name': info.get('longName')}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535b30c",
   "metadata": {},
   "source": [
    "I have a portfolio file with below contents \n",
    "\n",
    "| Ticker | Date   | DateTime    | Action | Volume |\n",
    "|--------|--------|-------------|--------|--------|\n",
    "| META   | 9/4/23 | 2:07:20 PM  | Buy    | 741    |\n",
    "| AAPL   | 4/6/23 | 12:17:07 AM | Buy    | 44     |\n",
    "| NVDA   | 6/16/23| 8:28:40 AM  | Buy    | 959    |\n",
    "| TSLA   | 7/22/23| 5:50:18 PM  | Buy    | 903    |\n",
    "| NFLX   | 3/15/23| 12:43:02 PM | Buy    | 586    |\n",
    "| TSM    | 7/15/23| 5:25:05 AM  | Buy    | 659    |\n",
    "| VOO    | 1/10/24| 1:25:54 AM  | Buy    | 892    |\n",
    "| VTI    | 4/28/23| 2:50:55 AM  | Buy    | 4      |\n",
    "| AMD    | 6/20/23| 5:58:31 PM  | Buy    | 717    |\n",
    "| INTC   | 5/31/23| 11:20:40 PM | Buy    | 835    |\n",
    "| GE     | 11/2/23| 1:43:34 AM  | Buy    | 784    |\n",
    "| MSFT   | 7/6/23 | 11:14:25 PM | Buy    | 12     |\n",
    "| GOOG   | 1/20/24| 11:31:05 PM | Buy    | 767    |\n",
    "| META   | 6/12/23| 10:26:24 AM | Buy    | 493    |\n",
    "| AAPL   | 12/1/23| 12:41:20 PM | Buy    | 965    |\n",
    "| NVDA   | 3/10/23| 1:08:57 AM  | Buy    | 434    |\n",
    "| TSLA   | 10/9/23| 4:29:12 AM  | Buy    | 715    |\n",
    "| NFLX   | 1/21/24| 5:01:43 PM  | Buy    | 90     |\n",
    "| TSM    | 5/1/23 | 10:30:17 AM | Buy    | 607    |\n",
    "| VOO    | 9/9/23 | 11:39:50 PM | Buy    | 566    |\n",
    "\n",
    "\n",
    "I'll be querying yfinance to get the corresponding price and multiply with volume to get total transaction amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "# Function to adjust date for weekends/holidays by finding the next available trading day\n",
    "def adjust_date_for_market(date):\n",
    "    # Check if the date is a weekend and adjust to next Monday if it is\n",
    "    if date.weekday() > 4:  # 5 = Saturday, 6 = Sunday\n",
    "        date += BDay(1)\n",
    "    return date\n",
    "\n",
    "# Function to fetch the closest available price to the given datetime (considering market hours)\n",
    "def fetch_price(ticker, date, time):\n",
    "    # Fetch daily data for the date\n",
    "    df_daily = yf.download(ticker, start=date, end=date + timedelta(days=1), progress=False)\n",
    "    if not df_daily.empty:\n",
    "        return df_daily.iloc[0]['Open']  # Use opening price of the day\n",
    "    else:\n",
    "        # If data is empty (weekend or holiday), find the next available trading day\n",
    "        next_date = adjust_date_for_market(date + timedelta(days=1))\n",
    "        df_next = yf.download(ticker, start=next_date, end=next_date + timedelta(days=1), progress=False)\n",
    "        if not df_next.empty:\n",
    "            return df_next.iloc[0]['Open']  # Use opening price of the next trading day\n",
    "    return None\n",
    "\n",
    "# Load the portfolio CSV file\n",
    "portfolio_df = pd.read_csv('portfolio.csv')\n",
    "\n",
    "# Convert 'Date' and 'DateTime' columns\n",
    "portfolio_df['Date'] = pd.to_datetime(portfolio_df['Date'])\n",
    "portfolio_df['DateTime'] = pd.to_datetime(portfolio_df['DateTime'])\n",
    "portfolio_df['Time'] = portfolio_df['DateTime'].dt.time\n",
    "\n",
    "# Iterate over each row in the portfolio to fetch prices\n",
    "for index, row in portfolio_df.iterrows():\n",
    "    adjusted_date = adjust_date_for_market(row['Date'])\n",
    "    price = fetch_price(row['Ticker'], adjusted_date, row['Time'])\n",
    "    portfolio_df.at[index, 'Close'] = round(price, 2) if price else None\n",
    "\n",
    "# Calculate the total transaction amount\n",
    "portfolio_df['Total_Transaction_Amount'] = round(portfolio_df['Volume'] * portfolio_df['Close'], 2)\n",
    "\n",
    "# Add an ID field as a unique identifier\n",
    "portfolio_df.reset_index(inplace=True)\n",
    "portfolio_df.rename(columns={'index': 'ID'}, inplace=True)\n",
    "portfolio_df.rename(columns={'DateTime': 'TransactionDateTime', 'Date': 'TransactionDate'}, inplace=True)\n",
    "\n",
    "# Adjust the 'ID' field to start from 1 for readability\n",
    "portfolio_df['ID'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35375f",
   "metadata": {},
   "source": [
    "Next we query polygon API for the same ticker list to get the compnay overview info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c254c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Your Polygon API key\n",
    "API_KEY = 'YOUR_POLYGON_API_KEY'\n",
    "\n",
    "# List of tickers\n",
    "tickers = [\"META\", \"AAPL\", \"NVDA\", \"TSLA\", \"NFLX\", \"TSM\", \"VOO\", \"VTI\", \"AMD\", \"INTC\", \"GE\", \"MSFT\", \"GOOG\"]\n",
    "\n",
    "def format_market_cap(market_cap):\n",
    "    \"\"\"Format the market cap value to millions (M) or billions (B) with 2 decimal places.\"\"\"\n",
    "    if market_cap >= 1e12:  # Trillion\n",
    "        return f\"{market_cap / 1e12:.2f}T\"\n",
    "    elif market_cap >= 1e9:  # Billion\n",
    "        return f\"{market_cap / 1e9:.2f}B\"\n",
    "    elif market_cap >= 1e6:  # Million\n",
    "        return f\"{market_cap / 1e6:.2f}M\"\n",
    "    else:\n",
    "        return f\"{market_cap:.2f}\"\n",
    "\n",
    "def fetch_yfinance_overview(symbol):\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    info = ticker.info\n",
    "    market_cap = info.get(\"marketCap\")\n",
    "    formatted_market_cap = format_market_cap(market_cap) if market_cap else \"N/A\"\n",
    "    as_of_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    return {\n",
    "        \"Ticker\": symbol,\n",
    "        \"Name\": info.get(\"longName\"),\n",
    "        \"Sector\": info.get(\"sector\"),\n",
    "        \"Industry\": info.get(\"industry\"),\n",
    "        \"MarketCap\": formatted_market_cap,\n",
    "        \"Description\": info.get(\"longBusinessSummary\"),\n",
    "        \"As_of_Date\": as_of_date\n",
    "    }\n",
    "\n",
    "# Fetch company overview for each ticker and store in a list\n",
    "company_overviews = [fetch_yfinance_overview(ticker) for ticker in tickers]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "company_overviews_df = pd.DataFrame(company_overviews)\n",
    "\n",
    "company_overviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8c835",
   "metadata": {},
   "source": [
    "<br>\n",
    "Next we will continue querying the earnings endpoint of polygon api to get quarterly earnings for a year for the above ticker symbols.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad294240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time  # For adding sleep\n",
    "\n",
    "def fetch_quarterly_financials_2023(symbol):\n",
    "    # Set the date range for filings in 2023\n",
    "    start_date = \"2023-01-01\"\n",
    "    end_date = \"2024-01-01\"\n",
    "    \n",
    "    url = f\"https://api.polygon.io/vX/reference/financials?ticker={symbol}&timeframe=quarterly&filing_date.gte={start_date}&filing_date.lt={end_date}&apiKey={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        earnings_list = [{\n",
    "            \"Ticker\": symbol,\n",
    "            \"FiscalYear\": result.get(\"fiscal_year\"),\n",
    "            \"FiscalPeriod\": result.get(\"fiscal_period\"),\n",
    "            \"EPS\": result.get(\"financials\", {}).get(\"income_statement\", {}).get(\"basic_earnings_per_share\", {}).get(\"value\", 0),\n",
    "            \"AsOfDate\": datetime.now().strftime('%Y-%m-%d')  # Use current date as AsOfDate\n",
    "        } for result in data.get(\"results\", [])]\n",
    "        return earnings_list\n",
    "    else:\n",
    "        print(f\"Failed to fetch quarterly financials for {symbol}: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Initialize a list to store all earnings data for 2023\n",
    "all_earnings_data_2023 = []\n",
    "\n",
    "# Process tickers in batches to adhere to the rate limit\n",
    "batch_size = 5\n",
    "for i in range(0, len(tickers), batch_size):\n",
    "    batch = tickers[i:i+batch_size]\n",
    "    for ticker in batch:\n",
    "        earnings_data = fetch_quarterly_financials_2023(ticker)\n",
    "        all_earnings_data_2023.extend(earnings_data)\n",
    "    # Wait for 60 seconds after each batch except the last one\n",
    "    if i + batch_size < len(tickers):\n",
    "        print(\"Waiting for 60 seconds to respect the API rate limit...\")\n",
    "        time.sleep(60)\n",
    "\n",
    "# Convert the list of earnings data to a DataFrame\n",
    "earnings_df_2023 = pd.DataFrame(all_earnings_data_2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7d79a",
   "metadata": {},
   "source": [
    "<br>Next, we will use `yfinance` to get daily and latest intraday prices for the stock symbols\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e93ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize an empty DataFrame for daily stock prices\n",
    "daily_stock_prices_df = pd.DataFrame()\n",
    "\n",
    "# Fetch daily stock prices for the past year for each ticker\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching data for {ticker}...\")\n",
    "    data = yf.download(ticker, period=\"1y\", interval=\"1d\")\n",
    "    \n",
    "    # Check if data was fetched successfully\n",
    "    if not data.empty:\n",
    "        # Add ticker symbol to the DataFrame\n",
    "        data['Ticker'] = ticker\n",
    "        # Reset the index to make 'Date' a column, not an index\n",
    "        data.reset_index(inplace=True)\n",
    "        # Append the data to the daily_stock_prices_df DataFrame\n",
    "        daily_stock_prices_df = pd.concat([daily_stock_prices_df, data[['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']]], ignore_index=True)\n",
    "\n",
    "# Add an ID field as a unique identifier for each record\n",
    "daily_stock_prices_df.reset_index(inplace=True)\n",
    "daily_stock_prices_df.rename(columns={'index': 'ID'}, inplace=True)\n",
    "daily_stock_prices_df['ID'] += 1  # Start IDs from 1 for readability\n",
    "\n",
    "# Convert 'Date' to date format (YYYY-MM-DD) if it's not already\n",
    "daily_stock_prices_df['Date'] = pd.to_datetime(daily_stock_prices_df['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "# Define the list of tickers\n",
    "tickers = [\"META\", \"AAPL\", \"NVDA\", \"TSLA\", \"NFLX\", \"TSM\", \"VOO\", \"VTI\", \"AMD\", \"INTC\", \"GE\", \"MSFT\", \"GOOG\"]\n",
    "\n",
    "# Determine the latest business day\n",
    "latest_business_day = pd.datetime.now() - BDay(1)\n",
    "\n",
    "# Initialize an empty DataFrame for intraday stock prices\n",
    "intraday_prices_df = pd.DataFrame()\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching intraday data for {ticker}...\")\n",
    "    # Fetch intraday data with 1-minute interval for the latest business day\n",
    "    data = yf.download(ticker, start=latest_business_day, interval=\"1m\", progress=False)\n",
    "    \n",
    "    # Check if data was fetched successfully\n",
    "    if not data.empty:\n",
    "        # Add ticker symbol to the DataFrame\n",
    "        data['Ticker'] = ticker\n",
    "        # Reset the index to make 'Datetime' a column, not an index\n",
    "        data.reset_index(inplace=True)\n",
    "        # Append the data to the intraday_prices_df DataFrame\n",
    "        intraday_prices_df = pd.concat([intraday_prices_df, data[['Datetime', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']]], ignore_index=True)\n",
    "\n",
    "# Add an ID field as a unique identifier for each record\n",
    "intraday_prices_df.reset_index(inplace=True)\n",
    "intraday_prices_df.rename(columns={'index': 'ID'}, inplace=True)\n",
    "intraday_prices_df['ID'] += 1  # Start IDs from 1 for readability\n",
    "\n",
    "# Ensure 'Datetime' is in the correct datetime format\n",
    "intraday_prices_df['Datetime'] = pd.to_datetime(intraday_prices_df['Datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055db0aa",
   "metadata": {},
   "source": [
    "### Loading Tables\n",
    "\n",
    "Now that we have a clear understanding of our data model and the referential integrity constraints in place, the next step is loading data into these tables. Given the relationships between the tables, it's vital to load the data in a specific sequence to maintain referential integrity. \n",
    "\n",
    "Based on the dependencies depicted in our ER diagram, here is the suggested sequence for loading data into the tables:\n",
    "\n",
    "1. **stocks**: This table serves as the cornerstone of our database, holding unique identifiers for each stock along with the company name. It's the primary reference for other tables, hence should be loaded first.\n",
    "\n",
    "2. **company_overview**: This table contains detailed information about each company and is directly linked to the `stocks` table through the `ticker` attribute. Once we have the stock identifiers in place, we can proceed to populate this table.\n",
    "\n",
    "3. **earnings** and **ticker_daily**: These tables also reference the `stocks` table through the `ticker` attribute. They can be populated in parallel after the `stocks` table is loaded as they contain information on earnings and daily stock prices, respectively.\n",
    "\n",
    "4. **ticker_intraday**: This table is dependent on the `ticker_daily` table as it contains intraday price data linked to the daily price records. Therefore, it should be loaded after the `ticker_daily` table.\n",
    "\n",
    "5. **portfolio**: Lastly, we populate the `portfolio` table, which records individual transactions and references the `stocks` table through the `ticker` attribute.\n",
    "\n",
    "To facilitate the data loading process, we will use DataFrames for each table. The DataFrames will be populated with data, and we can then use the `to_sql` method from the pandas library to load data into the PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d593c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ticker', 'name'], dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_df.rename(columns={'Ticker':'ticker', 'Name':'name'}, inplace=True)\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "stocks_df.to_sql('stocks', engine, if_exists='append', index=False)\n",
    "\n",
    "company_overviews_df = company_overviews_df[['Ticker', 'Sector', 'Industry', 'MarketCap', 'Description',\n",
    "       'As_of_Date']]\n",
    "company_overviews_df.rename(columns={'Ticker':'ticker', 'Sector':'sector', 'Industry':'industry', 'MarketCap':'market_cap', 'Description':'description',\n",
    "       'As_of_Date':'as_of_date'}, inplace=True)\n",
    "\n",
    "company_overviews_df.to_sql('company_overview', engine, if_exists='append', index=False)\n",
    "\n",
    "earnings_df_2023.reset_index(inplace=True)\n",
    "earnings_df_2023.rename(columns={'index': 'earnings_id'}, inplace=True)\n",
    "earnings_df_2023['earnings_id'] += 1  # Start IDs from 1 for readability\n",
    "earnings_df_2023.rename(columns={'Ticker':'ticker', 'FiscalYear':'fiscal_year', 'FiscalPeriod':'fiscal_period', \n",
    "                                  'EPS': 'eps', 'AsOfDate':'as_of_date'}, inplace=True)\n",
    "\n",
    "earnings_df_2023.to_sql('earnings', engine, if_exists='append', index=False)\n",
    "\n",
    "daily_stock_prices_df = daily_stock_prices_df[['ID', 'Ticker', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "daily_stock_prices_df.rename(columns={'ID': 'daily_price_id', 'Ticker':'ticker', 'Date': 'date', 'Open':'open', \n",
    "                                      'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "daily_stock_prices_df.rename(columns={'Date': 'date'}, inplace=True)\n",
    "\n",
    "daily_stock_prices_df.to_sql('ticker_daily', engine, if_exists='append', index=False)\n",
    "\n",
    "intraday_prices_df = intraday_prices_df[['ID', 'Ticker', 'Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "intraday_prices_df.rename(columns={'ID':'intraday_price_id', 'Ticker':'ticker', 'Datetime':'date', \n",
    "                                   'Open':'open', 'High':'high', 'Low':'low', 'Close':'close', 'Volume':'volume'}, inplace=True)\n",
    "\n",
    "intraday_prices_df.rename(columns={'date': 'date_time'}, inplace=True)\n",
    "intraday_prices_df.to_sql('ticker_intraday', engine, if_exists='append', index=False)\n",
    "\n",
    "portfolio_df.rename(columns={'ID': 'portfolio_id', 'Ticker': 'ticker','TransactionDate': 'transaction_date', 'TransactionDateTime': 'transaction_time','Action': 'action', 'Volume': 'volume','Time': 'time', 'Close': 'close', 'Total_Transaction_Amount': 'total_transaction_amount'}, inplace=True)\n",
    "portfolio_df = portfolio_df[['id', 'ticker', 'transaction_date', 'action',\n",
    "       'volume', 'time', 'close', 'total_transaction_amount']]\n",
    "\n",
    "portfolio_df.to_sql('portfolio', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5838d9",
   "metadata": {},
   "source": [
    "# Introduction to PostGraphile\n",
    "\n",
    "PostGraphile is a powerful tool that serves as a bridge between PostgreSQL and GraphQL, enabling rapid development cycles. It automatically generates a highly-performant and extensible GraphQL API from a PostgreSQL database schema. Here are some key features and benefits of using PostGraphile:\n",
    "\n",
    "1. **Automatic Schema Generation:** PostGraphile inspects your PostgreSQL schema to automatically generate a GraphQL schema. This means you can have a GraphQL API up and running in no time, without having to manually define types, queries, and mutations.\n",
    "\n",
    "2. **Database-Driven Development:** It encourages a database-driven development approach, where the database schema serves as the single source of truth. This promotes a clean database design and allows you to leverage the full power of SQL and PostgreSQL's features in your GraphQL API.\n",
    "\n",
    "3. **Performance Optimizations:** PostGraphile includes several performance optimizations to ensure that your GraphQL API is fast and efficient. It supports batch querying to reduce database query overhead and employs smart query planning to fetch data in the most efficient way possible.\n",
    "\n",
    "4. **Real-Time Features:** It offers real-time features with live queries, allowing clients to receive real-time updates as the data changes in the database, fostering interactive and dynamic client applications.\n",
    "\n",
    "5. **Fine-Grained Access Control:** PostGraphile integrates seamlessly with PostgreSQL's role-based access control and row-level security features, allowing you to implement fine-grained access control policies directly in the database.\n",
    "\n",
    "6. **Extensibility:** It provides a powerful plugin system that allows you to customize and extend your GraphQL schema with additional types, queries, mutations, and more.\n",
    "\n",
    "<br>\n",
    "<b>Comparing Native GraphQL Setup to PostGraphile</b>\n",
    "<br>\n",
    "\n",
    "| Feature / Aspect           | Native GraphQL Setup                                      | PostGraphile                                                |\n",
    "|----------------------------|-----------------------------------------------------------|-------------------------------------------------------------|\n",
    "| Schema Generation          | Manually defined based on business requirements           | Automatically generated based on PostgreSQL schema          |\n",
    "| Learning Curve             | Steeper, requires understanding of GraphQL principles     | Easier for those familiar with PostgreSQL and SQL           |\n",
    "| Development Speed          | Slower due to manual schema definition                    | Faster due to automatic schema generation                   |\n",
    "| Control and Flexibility    | High, complete control over schema and resolvers          | Slightly less, though extensible through plugins            |\n",
    "| Performance Optimizations  | Need to be implemented manually                           | Includes several built-in optimizations                     |\n",
    "| Real-Time Features         | Can be implemented but requires additional effort         | Built-in support for live queries                           |\n",
    "| Security                  | Need to be implemented at the application layer           | Integrates with PostgreSQL's role-based access control      |\n",
    "| Community and Ecosystem    | Supported by a large community, many libraries available  | Growing community, benefitting from PostgreSQL ecosystem   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eafbbf",
   "metadata": {},
   "source": [
    "### Integrating PostGraphile with Docker\n",
    "\n",
    "After successfully setting up our PostgreSQL service in a Docker container, we further enhanced our setup by integrating PostGraphile, a tool that automatically generates a GraphQL API from a PostgreSQL schema. Here's how we achieved this integration:\n",
    "\n",
    "#### Step 1: Updating the Docker Compose File\n",
    "\n",
    "We began by adding a new service definition for the PostGraphile service in our `docker-compose.yml` file. This service uses the official PostGraphile Docker image and is configured to communicate with our PostgreSQL service. The significant elements of the service definition include:\n",
    "\n",
    "- **Image**: Specifies the official PostGraphile Docker image.\n",
    "- **Ports**: Maps port 5000 inside the container to port 5001 on the host machine, allowing us to access the GraphQL API at `http://localhost:5001/graphql`.\n",
    "- **Environment Variables**: Defines necessary environment variables including the database URL and the PostgreSQL user credentials.\n",
    "- **Depends On**: Ensures the PostGraphile service only starts after the PostgreSQL service is healthy, as indicated by the health check defined in the PostgreSQL service.\n",
    "- **Command**: Specifies the command to start the PostGraphile service with the appropriate options.\n",
    "\n",
    "#### Step 2: Health Check and Service Dependency\n",
    "\n",
    "To ensure a smooth startup process, we defined a health check for the PostgreSQL service using the `pg_isready` utility, which verifies if the PostgreSQL service is ready to accept connections. We then configured the PostGraphile service to start only after the PostgreSQL service reported a healthy status, effectively preventing any connection errors during startup.\n",
    "\n",
    "Here is the updated segment of the `docker-compose.yml` file, showcasing the PostgreSQL service health check and the PostGraphile service definition:\n",
    "\n",
    "\n",
    "<pre><code>version: '3.1'\n",
    "\n",
    "services:\n",
    "  postgres:\n",
    "    build: \n",
    "      context: ./postgres\n",
    "      dockerfile: Dockerfile\n",
    "    ports:\n",
    "      - \"5437:5432\"\n",
    "    environment:\n",
    "      POSTGRES_USER: godzilla\n",
    "      POSTGRES_PASSWORD: Mrawww\n",
    "      POSTGRES_DB: monsterverse\n",
    "      DATABASE_URL: postgres://godzilla:Mrawww@postgres/monsterverse\n",
    "      SHADOW_DATABASE_URL: postgres://godzilla:Mrawww@postgres/monsterverse_shadow\n",
    "      ROOT_DATABASE_URL: postgres://godzilla:Mrawww@postgres/postgres\n",
    "    volumes:\n",
    "      - ./postgres/data:/var/lib/postgresql/data\n",
    "      - ./postgres/init:/docker-entrypoint-initdb.d/\n",
    "    networks:\n",
    "      - my-network\n",
    "    restart: on-failure:10\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"pg_isready -U godzilla\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 5\n",
    "\n",
    "  postgraphile:\n",
    "    image: graphile/postgraphile\n",
    "    ports:\n",
    "      - \"5001:5000\"\n",
    "    environment: \n",
    "      - DATABASE_URL=postgres://godzilla:Mrawww@postgres:5432/monsterverse\n",
    "      - POSTGRES_USER=godzilla\n",
    "      - POSTGRES_PASSWORD=Mrawww\n",
    "      - SCHEMA=public\n",
    "    depends_on:\n",
    "      postgres:\n",
    "        condition: service_healthy\n",
    "    networks:\n",
    "      - my-network\n",
    "    command: [\"-n\", \"0.0.0.0\", \"--enhance-graphiql\", \"--watch\", \"-c\", \"postgres://godzilla:Mrawww@postgres:5432/monsterverse\", \"-s\", \"public\"]\n",
    "\n",
    "networks:\n",
    "  my-network:\n",
    "    driver: bridge\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "Then you can run `docker-compose up --build` to update the docker with postgraphile container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87dd977",
   "metadata": {},
   "source": [
    "#### Querying with a GraphQL Client\n",
    "\n",
    "In the preceding sections, we successfully set up a PostgreSQL database within a Docker container and initialized a GraphQL server using PostGraphile. \n",
    "\n",
    "In this section, we will explore the powerful querying capabilities of GraphQL, which allows for flexible and efficient data retrieval.\n",
    "\n",
    "1. **Understanding GraphQL Queries**: GraphQL queries, a cornerstone of GraphQL operations, offer a flexible approach to data retrieval. Unlike REST APIs, GraphQL allows clients to dictate the structure of the response, enabling precise data fetching without over-fetching or under-fetching information. Let's delve deeper into the anatomy of GraphQL queries, focusing on aspects such as:\n",
    "\n",
    "- Fields: The keys in the query structure that map to the data you want to retrieve.\n",
    "- Arguments: Parameters that you can pass to fields to filter or modify the data response.\n",
    "- Aliases: Allow renaming fields in a query to avoid conflicts or to request the same field with different arguments.\n",
    "- Fragments: Reusable pieces of queries that let you create complex queries more easily.\n",
    "\n",
    "A glimpse into a basic GraphQL query structure:\n",
    "\n",
    "```graphql\n",
    "{\n",
    "  stock(ticker: \"AAPL\") {\n",
    "    name\n",
    "    sector\n",
    "    industry\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6861434b",
   "metadata": {},
   "source": [
    "2. **Setting Up a GraphQL Client:** To streamline our querying process, we will first set up a GraphQL client. This client acts as an intermediary between us and the GraphQL server, providing a user-friendly platform to construct and execute queries. In this subsection, we will detail the steps to set up a popular GraphQL client and configure it to interface seamlessly with our GraphQL server.\n",
    "\n",
    "3. **Crafting Complex Queries:** GraphQL shines in its ability to craft complex queries with ease. Here, we will demonstrate constructing multi-level queries, retrieving nested data structures, and utilizing variables for more dynamic querying. This subsection will serve as a playground to experiment with crafting complex queries that harness the full potential of GraphQL.\n",
    "\n",
    "4. **Mutations and Data Manipulations:** GraphQL also supports data manipulations through mutations, allowing for data insertions, updates, and deletions. In this part, we will explore crafting mutations to perform data manipulations, further expanding our GraphQL proficiency.\n",
    "\n",
    "5. **Real-Time Data with Subscriptions:** Delving into the real-time capabilities of GraphQL, we will explore GraphQL subscriptions in this subsection. Subscriptions provide a way to receive real-time updates from the server, fostering a more interactive and dynamic data experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260a72c",
   "metadata": {},
   "source": [
    "#### Setting up GraphQL client\n",
    "\n",
    "Setting up a GraphQL client is a crucial step in building a robust GraphQL API interface. A GraphQL client facilitates the construction, organization, and execution of GraphQL queries and mutations, offering a streamlined approach to interact with GraphQL APIs. Various clients come with different features and capabilities. Here's a list of popular GraphQL clients along with their notable features:\n",
    "\n",
    "1. **GraphiQL**\n",
    "\n",
    "- In-browser IDE for exploring GraphQL APIs.\n",
    "- Syntax highlighting and validation.\n",
    "- Real-time error highlighting.\n",
    "- Built-in documentation explorer for the GraphQL schema.\n",
    "- Available at the /graphiql endpoint when using PostGraphile.\n",
    "\n",
    "2. **Insomnia**\n",
    "\n",
    "- Supports both REST and GraphQL APIs.\n",
    "- Provides a user-friendly interface for constructing complex queries.\n",
    "- Allows for the organization of queries into workspaces.\n",
    "- Supports variable environments, making it easier to switch between different API environments.\n",
    "- Offers features like response filtering and data import/export.\n",
    "\n",
    "3. **Apollo Client**\n",
    "\n",
    "- Comprehensive state management library for JavaScript.\n",
    "- Facilitates local and remote data management with GraphQL.\n",
    "- Supports features like caching, error handling, and subscriptions.\n",
    "- Can be integrated into web applications built with frameworks like React, Angular, or Vue.js.\n",
    "- Offers devtools for debugging and performance insights.\n",
    "\n",
    "4. **Altair GraphQL Client**\n",
    "\n",
    "- Cross-platform GraphQL client.\n",
    "- Supports multiple languages.\n",
    "- Allows for setting up and using variables in queries.\n",
    "- Supports subscriptions for real-time data updates.\n",
    "- Offers features like query linting, response formatting, and schema documentation.\n",
    "\n",
    "5. **Postman**\n",
    "\n",
    "- Supports REST, GraphQL, and other API formats.\n",
    "- Allows for creating collections of queries with different configurations.\n",
    "- Offers features like automated testing, monitoring, and mock servers.\n",
    "- Provides collaboration features for team-based development.\n",
    "- GraphQL Playground\n",
    "\n",
    "6. **Interactive in-browser GraphQL IDE**\n",
    "- Supports exploring GraphQL APIs with features like schema introspection, query execution, and documentation browsing.\n",
    "- Offers features like query history and automatic schema reloading.\n",
    "- When choosing a GraphQL client, consider aspects like the client's feature set, your project requirements, and your personal preference in terms of user interface and workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8535f",
   "metadata": {},
   "source": [
    "For this chapter we will use `GraphiQL`\n",
    "\n",
    "To get started with GraphiQL, you can navigate to `http://localhost:5001/graphiql`\n",
    "\n",
    "\n",
    "<b> GraphiQL: </b><br><br>\n",
    "<img src=\"https://i.postimg.cc/PJygd3Rp/graphiql.png\" height = \"800\" width = \"1000\"><br><br>\n",
    "\n",
    "At the root level, the explorer tab lists the primary operation types supported by your GraphQL schema - queries, mutations, and subscriptions. You can click on these to expand and see the available operations of each type.\n",
    "\n",
    "When you expand an operation, it shows a list of available fields that you can query. It also allows you to explore nested fields further, providing a deep dive into the data structure and helping you construct complex queries with ease.\n",
    "<br><br>\n",
    "\n",
    "<img src=\"https://i.postimg.cc/dVgVM3dM/all-Stocks.png\" height = \"400\" width = \"700\"><br><br>\n",
    "\n",
    "\n",
    "## Schema Transformation to GraphQL\n",
    "\n",
    "In the process of generating a GraphQL API from a PostgreSQL schema using PostGraphile, several transformations occur to adapt the SQL schema to GraphQL conventions. One prominent transformation is the conversion of naming conventions from snake_case (common in PostgreSQL) to camelCase (common in GraphQL).\n",
    "\n",
    "For instance, a table named stocks in your PostgreSQL database would be represented as allStocks in the GraphQL schema for querying multiple records, following the GraphQL convention of using camelCase for field names. Similarly, fields with names like created_at in PostgreSQL would be transformed to createdAt in GraphQL.\n",
    "\n",
    "<b>Query Fields and Pagination</b>\n",
    "When you query allStocks, it provides various fields and arguments to facilitate querying and pagination:\n",
    "\n",
    "1. **Arguments for Pagination and Filtering**\n",
    "\n",
    "- first: Limits the results to a specified number of first records.\n",
    "- last: Limits the results to a specified number of last records.\n",
    "- offset: Skips a specified number of records before returning the results.\n",
    "- before: Used for cursor-based pagination to get records before a specific cursor.\n",
    "- after: Used for cursor-based pagination to get records after a specific cursor.\n",
    "- orderBy: Allows you to specify the order of results based on one or more fields.\n",
    "- condition: Allows you to specify conditions to filter the results based on field values.\n",
    "\n",
    "2. **Fields in the Response**\n",
    "\n",
    "- nodes: This field returns a list of records directly, where each record is represented as an object with fields corresponding to the columns in the stocks table. You can select the fields you want to retrieve within the nodes field.\n",
    "- edges: This field returns a list of edges, where each edge contains a node (representing a record) and potentially additional information such as a cursor for pagination. The node within each edge has the same structure as the nodes in the nodes field.\n",
    "- pageInfo: Contains information about the current page of results, including details like whether there are more results available, and cursors for the first and last records on the page. This information is useful for implementing cursor-based pagination.\n",
    "- totalCount: Returns the total count of records that match the query, irrespective of any pagination limits applied to the query.\n",
    "\n",
    "\n",
    "<b>Example query that uses some of these fields and arguments:</b>\n",
    "\n",
    "```graphql\n",
    "{\n",
    "  allStocks(first: 5, orderBy: TICKER_ASC) {\n",
    "    totalCount\n",
    "    pageInfo {\n",
    "      hasNextPage\n",
    "      hasPreviousPage\n",
    "      startCursor\n",
    "      endCursor\n",
    "    }\n",
    "    nodes {\n",
    "      ticker\n",
    "      name\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7e35b",
   "metadata": {},
   "source": [
    "<img src=\"https://i.postimg.cc/Y0rVYsYT/all-Stocks-Query.png\" height = \"800\" width = \"1000\"><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79883e37",
   "metadata": {},
   "source": [
    "### Modifying the data model to continue exploring postgres implementation\n",
    "\n",
    "1. We will introduce a portfolio summary table to distinguish portfolio transactions from portfolio holdings summary. \n",
    " - We would also rename portfolio to portfolio transactions\n",
    " - In our current implementation we missed to add PK and FK relationships to portfolio table, which we will implement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9df6e062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1411eabe0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rename portfolio Table to portfolio_transactions\n",
    "engine.execute(\"\"\"\n",
    "    ALTER TABLE portfolio RENAME TO portfolio_transactions;\n",
    "\"\"\")\n",
    "engine.execute(\"COMMIT;\")\n",
    "\n",
    "# We will add a primary key to the portfolio_id column in the portfolio_transactions table.\n",
    "engine.execute(\"\"\"\n",
    "    ALTER TABLE portfolio_transactions\n",
    "    ADD PRIMARY KEY (portfolio_id);\n",
    "\"\"\")\n",
    "engine.execute(\"COMMIT;\")\n",
    "\n",
    "#Next, we will add a foreign key to the ticker column in the portfolio_transactions table, \n",
    "#referencing the ticker column in the stocks table.\n",
    "engine.execute(\"\"\"\n",
    "    ALTER TABLE portfolio_transactions\n",
    "    ADD CONSTRAINT fk_ticker\n",
    "    FOREIGN KEY (ticker)\n",
    "    REFERENCES stocks(ticker);\n",
    "\"\"\")\n",
    "engine.execute(\"COMMIT;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a79b4",
   "metadata": {},
   "source": [
    "2. Let's create a new table called portfolio_summary with columns ticker, total_shares, and total_asset_value. We'll populate this table with the calculated summary values for each ticker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "18668328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1411c8af0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS portfolio_summary;\n",
    "\"\"\")\n",
    "engine.execute(\"\"\"\n",
    "    CREATE TABLE portfolio_summary (\n",
    "        sha_key VARCHAR(50) PRIMARY KEY,\n",
    "        ticker VARCHAR(50) REFERENCES stocks(ticker),\n",
    "        total_shares INT,\n",
    "        total_asset_value DECIMAL,\n",
    "        as_of_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "\"\"\")\n",
    "engine.execute(\"COMMIT;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc8e7f2",
   "metadata": {},
   "source": [
    "3. Next, we will create a PostgreSQL function that automatically updates the portfolio_summary table whenever there is a change to the portfolio_transactions table. This function will be triggered by INSERT, UPDATE, or DELETE operations on the portfolio_transactions table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791bc6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1068775b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "\n",
    "engine.execute(\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION update_portfolio_summary() \n",
    "    RETURNS TRIGGER AS $$\n",
    "    DECLARE \n",
    "        short_sha_key VARCHAR(50);\n",
    "    BEGIN\n",
    "        INSERT INTO portfolio_summary (sha_key, ticker, total_shares, total_asset_value, as_of_date)\n",
    "        SELECT \n",
    "            LEFT(MD5(CONCAT(inner_query.ticker, inner_query.total_shares::TEXT, inner_query.total_asset_value::TEXT, CURRENT_DATE::TEXT)), 7) AS sha_key,\n",
    "            inner_query.ticker,\n",
    "            inner_query.total_shares,\n",
    "            inner_query.total_asset_value,\n",
    "            CURRENT_TIMESTAMP AS as_of_date\n",
    "        FROM\n",
    "            (SELECT \n",
    "                ticker,\n",
    "                COALESCE(SUM(volume) FILTER (WHERE action = 'Buy'), 0) - COALESCE(SUM(volume) FILTER (WHERE action = 'Sell'), 0) AS total_shares,\n",
    "                COALESCE(SUM(total_transaction_amount) FILTER (WHERE action = 'Buy'), 0) - COALESCE(SUM(total_transaction_amount) FILTER (WHERE action = 'Sell'), 0) AS total_asset_value\n",
    "            FROM\n",
    "                portfolio_transactions\n",
    "            GROUP BY\n",
    "                ticker) AS inner_query\n",
    "        ON CONFLICT (ticker, date_trunc('day', as_of_date))\n",
    "        DO UPDATE SET \n",
    "            total_shares = excluded.total_shares,\n",
    "            total_asset_value = excluded.total_asset_value,\n",
    "            as_of_date = excluded.as_of_date;\n",
    "\n",
    "        RETURN NEW;\n",
    "    END;\n",
    "    $$ LANGUAGE plpgsql;\n",
    "\"\"\")\n",
    "engine.execute(\"COMMIT;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05159312",
   "metadata": {},
   "source": [
    "4. Next, we create a trigger to call this function whenever there is a change in the portfolio_transactions table\n",
    "\n",
    "This trigger will automatically call update_portfolio_summary function whenever rows in portfolio_transactions table are inserted, updated, or deleted, keeping the portfolio_summary table up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "353d9906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1411d3820>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"\"\"\n",
    "    DROP TRIGGER IF EXISTS portfolio_transactions_change ON portfolio_transactions;\n",
    "\"\"\")\n",
    "engine.execute(\"\"\"\n",
    "    CREATE TRIGGER portfolio_transactions_change \n",
    "    AFTER INSERT OR UPDATE OR DELETE ON portfolio_transactions\n",
    "    FOR EACH ROW EXECUTE FUNCTION update_portfolio_summary();\n",
    "\"\"\")\n",
    "engine.execute(\"COMMIT;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "191185c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x141032790>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One time execution to populate the summary table\n",
    "\n",
    "engine.execute(\"\"\"\n",
    "    INSERT INTO portfolio_summary (sha_key, ticker, total_shares, total_asset_value, as_of_date)\n",
    "    SELECT \n",
    "        LEFT(MD5(CONCAT(inner_query.ticker, inner_query.total_shares::TEXT, inner_query.total_asset_value::TEXT, CURRENT_DATE::TEXT)), 7) AS sha_key,\n",
    "        inner_query.ticker,\n",
    "        inner_query.total_shares,\n",
    "        inner_query.total_asset_value,\n",
    "        CURRENT_TIMESTAMP AS as_of_date\n",
    "    FROM\n",
    "        (SELECT \n",
    "            ticker,\n",
    "            COALESCE(SUM(volume) FILTER (WHERE action = 'Buy'), 0) - COALESCE(SUM(volume) FILTER (WHERE action = 'Sell'), 0) AS total_shares,\n",
    "            COALESCE(SUM(total_transaction_amount) FILTER (WHERE action = 'Buy'), 0) - COALESCE(SUM(total_transaction_amount) FILTER (WHERE action = 'Sell'), 0) AS total_asset_value\n",
    "        FROM\n",
    "            portfolio_transactions WHERE ticker = 'JBLU'\n",
    "        GROUP BY\n",
    "            ticker) AS inner_query\n",
    "    ON CONFLICT (sha_key)\n",
    "    DO UPDATE SET \n",
    "        total_shares = excluded.total_shares,\n",
    "        total_asset_value = excluded.total_asset_value,\n",
    "        as_of_date = excluded.as_of_date;\n",
    "\"\"\")\n",
    "engine.execute(\"COMMIT;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc60af4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Lets build a workflow to accept `Ticker`, `Volume`, and `Action (Buy/Sell)` from the user and update the underlying tables based on this output.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "47f860d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter action (buy/sell): Buy\n",
      "Enter ticker: JBLU\n",
      "Enter volume: 100\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "def execute_stock_transaction():\n",
    "    # Step 1: Get user inputs\n",
    "    action_input = input(\"Enter action (buy/sell): \").strip().lower()\n",
    "    action = \"Buy\" if action_input == \"buy\" else \"Sell\" if action_input == \"sell\" else \"Invalid\"\n",
    "    ticker_symbol = input(\"Enter ticker: \").strip().upper()\n",
    "    volume = int(input(\"Enter volume: \"))\n",
    "\n",
    "    # Step 2: Validate the ticker using yfinance\n",
    "    ticker = yf.Ticker(ticker_symbol)\n",
    "    if ticker.info.get('symbol') != ticker_symbol:\n",
    "        print(f\"Invalid ticker: {ticker_symbol}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # Step 3: Check for sufficient shares in case of sell action\n",
    "    if action == 'sell':\n",
    "        result = engine.execute(f\"SELECT total_shares FROM portfolio_summary WHERE ticker = '{ticker_symbol}'\")\n",
    "        total_shares = result.fetchone()\n",
    "        if total_shares is None or total_shares[0] < volume:\n",
    "            print(\"Insufficient amount of shares to sell\")\n",
    "            return None, None, None, None, None\n",
    "\n",
    "    # Get the current transaction date and time in EST\n",
    "    est = timezone('US/Eastern')\n",
    "    transaction_datetime = datetime.now(est)\n",
    "    transaction_date = transaction_datetime.date()\n",
    "    transaction_time = transaction_datetime.time()\n",
    "\n",
    "    return action, ticker_symbol, volume, transaction_date, transaction_time\n",
    "\n",
    "# Call the function to execute a stock transaction\n",
    "action, ticker_symbol, volume, transaction_date, transaction_time = execute_stock_transaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ab65dce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/1r4fv_1j0dvb38w891pxz3qw0000gn/T/ipykernel_37290/3149746732.py:21: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead.\n",
      "  closest_time_index = intraday_data.index.get_loc(transaction_datetime, method='nearest')\n",
      "/var/folders/4k/1r4fv_1j0dvb38w891pxz3qw0000gn/T/ipykernel_37290/3149746732.py:21: FutureWarning: Passing method to DatetimeIndex.get_loc is deprecated and will raise in a future version. Use index.get_indexer([item], method=...) instead.\n",
      "  closest_time_index = intraday_data.index.get_loc(transaction_datetime, method='nearest')\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_stock_price(ticker_symbol, transaction_date, transaction_time):\n",
    "    # Define the business hours (9:30 AM to 4:00 PM EST)\n",
    "    business_hours_start = datetime.strptime('9:30:00', '%H:%M:%S').time()\n",
    "    business_hours_end = datetime.strptime('16:00:00', '%H:%M:%S').time()\n",
    "\n",
    "    # Combine transaction date and time into a single datetime object\n",
    "    transaction_datetime = datetime.combine(transaction_date, transaction_time)\n",
    "\n",
    "    # Check if the transaction date is a business day\n",
    "    if transaction_date.weekday() >= 5:\n",
    "        print(f\"Transactions cannot occur on weekends. Please choose a business day.\")\n",
    "        return None\n",
    "\n",
    "    # Check if the transaction time is within business hours\n",
    "    if business_hours_start <= transaction_time <= business_hours_end:\n",
    "        # Fetch intraday data and get the price at the transaction time or the closest available time\n",
    "        intraday_data = yf.download(ticker_symbol, start=transaction_date.strftime('%Y-%m-%d'), end=(transaction_date + timedelta(days=1)).strftime('%Y-%m-%d'), interval='1m', progress=False)\n",
    "        if not intraday_data.empty:\n",
    "            closest_time_index = intraday_data.index.get_loc(transaction_datetime, method='nearest')\n",
    "            return intraday_data.iloc[closest_time_index]['Close']\n",
    "        else:\n",
    "            print(f\"Could not fetch intraday data for {ticker_symbol} on {transaction_date}.\")\n",
    "            return None\n",
    "    else:\n",
    "        # Fetch daily data and get the closing price\n",
    "        daily_data = yf.download(ticker_symbol, start=transaction_date.strftime('%Y-%m-%d'), end=(transaction_date + timedelta(days=1)).strftime('%Y-%m-%d'), progress=False)\n",
    "        if not daily_data.empty:\n",
    "            return daily_data['Close'].iloc[-1]\n",
    "        else:\n",
    "            print(f\"Could not fetch daily data for {ticker_symbol} on {transaction_date}.\")\n",
    "            return None\n",
    "\n",
    "# Now you can call this function with the transaction_date and transaction_time variables you got from execute_stock_transaction() function\n",
    "stock_price = get_stock_price(ticker_symbol, transaction_date, transaction_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "aefae69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks table updated with JBLU.\n"
     ]
    }
   ],
   "source": [
    "def update_stocks_table(ticker_symbol):\n",
    "    try:\n",
    "        # Check if the ticker is already present in the stocks table\n",
    "        result = engine.execute(f\"SELECT * FROM stocks WHERE ticker = '{ticker_symbol}'\")\n",
    "        if result.fetchone() is None:\n",
    "            # The stock is new, fetch information using yfinance\n",
    "            ticker = yf.Ticker(ticker_symbol)\n",
    "            info = ticker.info\n",
    "\n",
    "            # Insert a new record into the stocks table\n",
    "            engine.execute(f\"\"\"\n",
    "                INSERT INTO stocks (ticker, name) \n",
    "                VALUES ('{ticker_symbol}', '{info.get('longName')}')\n",
    "                ON CONFLICT (ticker) DO NOTHING\n",
    "            \"\"\")\n",
    "            print(f\"Stocks table updated with {ticker_symbol}.\")\n",
    "        else:\n",
    "            print(f\"The ticker {ticker_symbol} already exists in the stocks table.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating stocks table: {e}\")\n",
    "\n",
    "# Call this function within your main function where you are handling the transaction\n",
    "update_stocks_table(ticker_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "37717749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "\n",
    "engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "\n",
    "def format_market_cap(market_cap):\n",
    "    \"\"\"Format the market cap value to millions (M) or billions (B) with 2 decimal places.\"\"\"\n",
    "    if market_cap >= 1e12:  # Trillion\n",
    "        return f\"{market_cap / 1e12:.2f}T\"\n",
    "    elif market_cap >= 1e9:  # Billion\n",
    "        return f\"{market_cap / 1e9:.2f}B\"\n",
    "    elif market_cap >= 1e6:  # Million\n",
    "        return f\"{market_cap / 1e6:.2f}M\"\n",
    "    else:\n",
    "        return f\"{market_cap:.2f}\"\n",
    "\n",
    "def fetch_yfinance_overview(symbol):\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    info = ticker.info\n",
    "    market_cap = info.get(\"marketCap\")\n",
    "    formatted_market_cap = format_market_cap(market_cap) if market_cap else \"N/A\"\n",
    "    as_of_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    return {\n",
    "        \"ticker\": symbol,\n",
    "        \"sector\": info.get(\"sector\"),\n",
    "        \"industry\": info.get(\"industry\"),\n",
    "        \"market_cap\": formatted_market_cap,\n",
    "        \"description\": info.get(\"longBusinessSummary\"),\n",
    "        \"as_of_date\": as_of_date\n",
    "    }\n",
    "\n",
    "def update_company_overview_table(ticker_symbol):\n",
    "    # Fetch the company overview data\n",
    "    data = fetch_yfinance_overview(ticker_symbol)\n",
    "    \n",
    "    # Check if the data already exists in the company_overview table\n",
    "    result = engine.execute(f\"SELECT * FROM company_overview WHERE ticker = '{ticker_symbol}'\")\n",
    "    if result.fetchone() is None:\n",
    "        # Insert the data into the company_overview table\n",
    "        query = \"\"\"INSERT INTO company_overview \n",
    "                   (ticker, sector, industry, market_cap, description, as_of_date) \n",
    "                   VALUES \n",
    "                   (:ticker, :sector, :industry, :market_cap, :description, :as_of_date)\"\"\"\n",
    "        engine.execute(text(query), **data)\n",
    "    else:\n",
    "        print(f\"The data for ticker {ticker_symbol} already exists in the company_overview table\")\n",
    "\n",
    "# Call the function to update the company_overview table\n",
    "update_company_overview_table(ticker_symbol)  # Replace \"JBLU\" with your desired ticker symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e29ffb",
   "metadata": {},
   "source": [
    "<br><br><font size=\"4\">We need to redefine our fetch quarterly earnings function to retrieve quarterly earnings from 2023 to current quarter</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "923fd513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_quarterly_financials(symbol):\n",
    "    # Set the date range for filings starting from 2023\n",
    "    start_date = \"2023-01-01\"\n",
    "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://api.polygon.io/vX/reference/financials?ticker={symbol}&timeframe=quarterly&filing_date.gte={start_date}&filing_date.lt={end_date}&apiKey={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        earnings_list = [{\n",
    "            \"Ticker\": symbol,\n",
    "            \"FiscalYear\": result.get(\"fiscal_year\"),\n",
    "            \"FiscalPeriod\": result.get(\"fiscal_period\"),\n",
    "            \"EPS\": result.get(\"financials\", {}).get(\"income_statement\", {}).get(\"basic_earnings_per_share\", {}).get(\"value\", 0),\n",
    "            \"AsOfDate\": datetime.now().strftime('%Y-%m-%d')  # Use current date as AsOfDate\n",
    "        } for result in data.get(\"results\", [])]\n",
    "        return earnings_list\n",
    "    else:\n",
    "        print(f\"Failed to fetch quarterly financials for {symbol}: {response.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148f367",
   "metadata": {},
   "source": [
    "We need to further modify the earnings table as below\n",
    "\n",
    "```sql\n",
    "ALTER TABLE earnings DROP CONSTRAINT earnings_pkey;\n",
    "\n",
    "ALTER TABLE earnings DROP COLUMN earnings_id;\n",
    "\n",
    "ALTER TABLE earnings ADD COLUMN earnings_id VARCHAR(32);\n",
    "\n",
    "UPDATE earnings \n",
    "SET earnings_id = LEFT(MD5(CONCAT(ticker, fiscal_year, fiscal_period)), 7);\n",
    "\n",
    "ALTER TABLE earnings ADD PRIMARY KEY (earnings_id);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a8c7ce78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/1r4fv_1j0dvb38w891pxz3qw0000gn/T/ipykernel_37290/3128305128.py:18: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  data = earnings_df.to_dict(orient='records')\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def update_earnings_table(ticker_symbol):\n",
    "    # Fetch the earnings data using a similar approach to your existing function\n",
    "    earnings_data = fetch_quarterly_financials(ticker_symbol)\n",
    "\n",
    "    # If earnings data is successfully fetched, insert it into the earnings table\n",
    "    if earnings_data:\n",
    "        earnings_df = pd.DataFrame(earnings_data)\n",
    "        earnings_df['as_of_date'] = datetime.now().strftime('%Y-%m-%d')  # Use the current date as AsOfDate\n",
    "        earnings_df['ticker'] = ticker_symbol  # Add ticker column\n",
    "        earnings_df.rename(columns={'Ticker': 'ticker', 'FiscalYear': 'fiscal_year', 'FiscalPeriod': 'fiscal_period', 'EPS': 'eps', 'AsOfDate': 'as_of_date'}, inplace=True)\n",
    "\n",
    "        # Add earnings_id as an MD5 hash of (ticker, fiscal_year, fiscal_period)\n",
    "        earnings_df['earnings_id'] = earnings_df.apply(lambda row: hashlib.md5(f\"{row['ticker']}{row['fiscal_year']}{row['fiscal_period']}\".encode('utf-8')).hexdigest()[:7], axis=1)\n",
    "\n",
    "        # Convert the DataFrame to a dictionary to insert into the SQL table\n",
    "        data = earnings_df.to_dict(orient='records')\n",
    "\n",
    "        # Create a connection to the database\n",
    "        engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "\n",
    "        # Insert data into the earnings table\n",
    "        with engine.connect() as conn:\n",
    "            for record in data:\n",
    "                conn.execute(text(\"\"\"\n",
    "                    INSERT INTO earnings (earnings_id, ticker, fiscal_year, fiscal_period, eps, as_of_date) \n",
    "                    VALUES (:earnings_id, :ticker, :fiscal_year, :fiscal_period, :eps, :as_of_date)\n",
    "                    ON CONFLICT (earnings_id) \n",
    "                    DO UPDATE SET eps = :eps, as_of_date = :as_of_date;\n",
    "                \"\"\"), record)\n",
    "    else:\n",
    "        print(f\"Failed to fetch earnings data for {ticker_symbol}.\")\n",
    "\n",
    "update_earnings_table(ticker_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bedea",
   "metadata": {},
   "source": [
    "We need to further modify the earnings table as below\n",
    "\n",
    "```sql\n",
    "ALTER TABLE ticker_daily DROP CONSTRAINT ticker_daily_pkey;\n",
    "\n",
    "ALTER TABLE ticker_daily DROP COLUMN daily_price_id;\n",
    "\n",
    "ALTER TABLE ticker_daily ADD COLUMN daily_price_id VARCHAR(32);\n",
    "\n",
    "UPDATE ticker_daily \n",
    "SET daily_price_id = LEFT(MD5(CONCAT(ticker, date)), 7);\n",
    "\n",
    "ALTER TABLE ticker_daily ADD PRIMARY KEY (daily_price_id);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0125d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "def update_ticker_daily_table(ticker_symbol):\n",
    "    # Fetch daily stock prices for the last year (or since 2023) for the new ticker\n",
    "    daily_data = yf.download(ticker_symbol, start='2023-01-01', end=datetime.now().strftime('%Y-%m-%d'), interval='1d')\n",
    "    \n",
    "    if daily_data.empty:\n",
    "        print(f\"Failed to fetch daily data for {ticker_symbol}\")\n",
    "        return\n",
    "    \n",
    "    # Format the data to match the ticker_daily table structure\n",
    "    daily_data.reset_index(inplace=True)\n",
    "    daily_data['ticker'] = ticker_symbol\n",
    "    daily_data.rename(columns={'Date': 'date', 'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}, inplace=True)\n",
    "    daily_data['daily_price_id'] = daily_data.apply(lambda row: hashlib.md5(f\"{row['ticker']}{row['date']}\".encode('utf-8')).hexdigest()[:7], axis=1)\n",
    "    \n",
    "    # Convert the DataFrame to a dictionary to insert into the SQL table\n",
    "    data = daily_data.to_dict(orient='records')\n",
    "    \n",
    "    # Create a connection to the database\n",
    "    engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "    \n",
    "    # Insert data into the ticker_daily table\n",
    "    with engine.connect() as conn:\n",
    "        for record in data:\n",
    "            conn.execute(text(\"\"\"\n",
    "                INSERT INTO ticker_daily (daily_price_id, ticker, date, open, high, low, close, volume)\n",
    "                VALUES (:daily_price_id, :ticker, :date, :open, :high, :low, :close, :volume)\n",
    "                ON CONFLICT (daily_price_id) \n",
    "                DO UPDATE SET open = :open, high = :high, low = :low, close = :close, volume = :volume;\n",
    "            \"\"\"), record)\n",
    "\n",
    "# Usage\n",
    "update_ticker_daily_table(ticker_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e4c2efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/1r4fv_1j0dvb38w891pxz3qw0000gn/T/ipykernel_37290/211969391.py:3: FutureWarning: The parsing of 'now' in pd.to_datetime without `utc=True` is deprecated. In a future version, this will match Timestamp('now') and Timestamp.now()\n",
      "  latest_business_day = pd.to_datetime('now') - BDay(1)\n"
     ]
    }
   ],
   "source": [
    "def update_ticker_intraday_table(ticker_symbol):\n",
    "    # Determine the latest business day\n",
    "    latest_business_day = pd.to_datetime('now') - BDay(1)\n",
    "    \n",
    "    # Fetch intraday data with 1-minute interval for the latest business day\n",
    "    intraday_data = yf.download(ticker_symbol, start=latest_business_day, interval='1m', progress=False)\n",
    "    \n",
    "    if intraday_data.empty:\n",
    "        print(f\"Failed to fetch intraday data for {ticker_symbol}\")\n",
    "        return\n",
    "    \n",
    "    # Format the data to match the ticker_intraday table structure\n",
    "    intraday_data.reset_index(inplace=True)\n",
    "    intraday_data['ticker'] = ticker_symbol\n",
    "    intraday_data.rename(columns={'Datetime': 'date_time', 'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}, inplace=True)\n",
    "    \n",
    "    # Convert the DataFrame to a dictionary to insert into the SQL table\n",
    "    data = intraday_data.to_dict(orient='records')\n",
    "    \n",
    "    # Create a connection to the database\n",
    "    engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "    \n",
    "    # Find the maximum existing intraday_price_id in the table\n",
    "    result = engine.execute(\"SELECT max(intraday_price_id) FROM ticker_intraday\")\n",
    "    max_id = result.fetchone()[0]\n",
    "    if max_id is None:\n",
    "        max_id = 0\n",
    "\n",
    "    # Insert data into the ticker_intraday table\n",
    "    with engine.connect() as conn:\n",
    "        for record in data:\n",
    "            max_id += 1\n",
    "            record['intraday_price_id'] = max_id\n",
    "            conn.execute(text(\"\"\"\n",
    "                INSERT INTO ticker_intraday (intraday_price_id, ticker, date_time, open, high, low, close, volume)\n",
    "                VALUES (:intraday_price_id, :ticker, :date_time, :open, :high, :low, :close, :volume);\n",
    "            \"\"\"), record)\n",
    "\n",
    "# Usage\n",
    "update_ticker_intraday_table(ticker_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6ed216b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_portfolio_transactions_table(ticker_symbol, action, volume, stock_price, transaction_date, transaction_time):\n",
    "    # Create a connection to the database\n",
    "    engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "\n",
    "    # Prepare the data for insertion\n",
    "    data = {\n",
    "        'ticker': ticker_symbol,\n",
    "        'transaction_date': transaction_date.strftime('%Y-%m-%d'),\n",
    "        'transaction_time': transaction_time.strftime('%H:%M:%S'),\n",
    "        'action': action,\n",
    "        'volume': volume,\n",
    "        'price': round(stock_price, 2),\n",
    "        'total_transaction_amount': round(volume * stock_price, 2) \n",
    "    }\n",
    "\n",
    "    # Insert data into the portfolio_transactions table\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO portfolio_transactions (ticker, transaction_date, time, action, volume, close, total_transaction_amount)\n",
    "            VALUES (:ticker, :transaction_date, :transaction_time, :action, :volume, :price, :total_transaction_amount)\n",
    "        \"\"\"), data)\n",
    "\n",
    "# Usage example:\n",
    "update_portfolio_transactions_table(ticker_symbol, action, volume, stock_price, transaction_date, transaction_time)\n",
    "#action, ticker_symbol, volume, transaction_date, transaction_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e485854",
   "metadata": {},
   "source": [
    "You can query the `portfolio_summary` table to see the trigger auto updating the table based on the JBLU transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63eacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
