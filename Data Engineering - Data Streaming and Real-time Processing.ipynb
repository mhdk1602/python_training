{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be77d1cc",
   "metadata": {},
   "source": [
    "# Data Streaming and Real-time Processing\n",
    "\n",
    "\n",
    "## Introduction to Data Streaming and Real-time Processing\n",
    "\n",
    "In modern data architectures, data streaming and real-time processing hold a pivotal role. Unlike batch processing, which handles large, finite sets of data, data streaming processes data in real-time, allowing for instantaneous analysis and action. This is particularly beneficial in scenarios such as fraud detection, monitoring systems, and real-time analytics.\n",
    "\n",
    "Data streaming is seamlessly integrated with cloud platforms like AWS and Azure, enhancing scalability and offering robust solutions for real-time data handling. Let's delve deeper into the tools and libraries that facilitate real-time data processing in Python.\n",
    "\n",
    "\n",
    "## Tools and Libraries for Real-time Data Processing in Python\n",
    "\n",
    "\n",
    "### Apache Kafka\n",
    "\n",
    "##### 1. Overview\n",
    "\n",
    "Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and streaming applications. It is horizontally scalable, fault-tolerant, and incredibly fast, facilitating real-time analytics and monitoring. Kafka's real power comes from its ability to manage streams of data from various sources, making it a popular choice for organizations looking to analyze and process large streams of data in real time.\n",
    "\n",
    "##### 2. Features and Advantages\n",
    "\n",
    "<ul>\n",
    "    <li><b>High Throughput:</b> Capable of handling millions of messages per second, making it suitable for high-speed data analytics.</li>\n",
    "    <li><b>Scalability:</b> Can easily scale horizontally to accommodate growing data streams.</li>\n",
    "    <li><b>Fault Tolerance:</b> Provides built-in fault tolerance by replicating data across multiple brokers.</li>\n",
    "    <li><b>Durability:</b> Ensures data persistence on disk, safeguarding against data loss.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "##### 3. Installation\n",
    "\n",
    "To start working with Apache Kafka in Python, the first step is to install the Kafka-Python library, a Python client for Apache Kafka. You can install it using the following command:\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">!pip install</font> kafka-python\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "#### 4. Configuring Kafka Producers and Consumers in Python\n",
    "\n",
    "In this section, we will delve into how you can set up and configure Kafka producers and consumers using Python. Below are Python snippets that demonstrate how to initialize Kafka producers and consumers and how to send and receive messages.\n",
    "\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">from</font> kafka <font color=\"blue\">import</font> KafkaProducer, KafkaConsumer\n",
    "\n",
    "# Initializing Kafka producer\n",
    "producer = KafkaProducer(bootstrap_servers=<font color=\"green\">'localhost:9092'</font>)\n",
    "\n",
    "# Sending a message to the Kafka topic\n",
    "producer.send(<font color=\"green\">'sample_topic'</font>, value=<font color=\"green\">b'Hello, Kafka'</font>)\n",
    "producer.flush()\n",
    "\n",
    "# Initializing Kafka consumer\n",
    "consumer = KafkaConsumer(<font color=\"green\">'sample_topic'</font>, bootstrap_servers=<font color=\"green\">'localhost:9092'</font>, auto_offset_reset=<font color=\"green\">'earliest'</font>, enable_auto_commit=<font color=\"blue\">True</font>)\n",
    "\n",
    "# Reading and printing messages from the Kafka topic\n",
    "<font color=\"blue\">for</font> message <font color=\"blue\">in</font> consumer:\n",
    "    print (message)\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "\n",
    "### Apache Flink\n",
    "\n",
    "\n",
    "##### 1. Overview\n",
    "\n",
    "Apache Flink is a powerful, open-source, distributed processing engine for stateful computations over unbounded and bounded data streams. Flink provides highly flexible streaming applications as well as real-time analytics, making it an exceptional choice for working with real-time data pipelines.\n",
    "\n",
    "##### 2. Features and Advantages\n",
    "\n",
    "- **Event Time Processing**: It supports event time semantics allowing for accurate results even when working with out-of-order or late-arriving data.\n",
    "- **Stateful Computations**: Provides robust stateful computation support which is vital for many real-time processing use cases.\n",
    "- **Exactly-once Processing Semantics**: Guarantees exactly-once processing semantics which ensure no data is lost or duplicated.\n",
    "- **High Performance**: Optimized for both high throughput and low latency, ensuring performance isn’t compromised.\n",
    "\n",
    "##### 3. Installation\n",
    "\n",
    "To begin with Apache Flink in Python, you need to install the PyFlink package. This can be done with the following command:\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">!pip install</font> apache-flink\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "###### 4. Real-Time Data Processing with Apache Flink in Python\n",
    "\n",
    "Here’s a simple example of how to use Apache Flink in Python for real-time data processing:\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">from</font> pyflink.common.serialization <font color=\"blue\">import</font> SimpleStringSchema\n",
    "<font color=\"blue\">from</font> pyflink.common.typeinfo <font color=\"blue\">import</font> Types\n",
    "<font color=\"blue\">from</font> pyflink.datastream <font color=\"blue\">import</font> StreamExecutionEnvironment\n",
    "<font color=\"blue\">from</font> pyflink.datastream.connectors <font color=\"blue\">import</font> FlinkKafkaConsumer, FlinkKafkaProducer\n",
    "\n",
    "# Set up the execution environment\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "# Configure Kafka consumer\n",
    "kafka_consumer = FlinkKafkaConsumer(\n",
    "    topics=<font color=\"green\">'input_topic'</font>,\n",
    "    schema=SimpleStringSchema(),\n",
    "    properties={<font color=\"green\">'bootstrap.servers'</font>: <font color=\"green\">'localhost:9092'</font>, <font color=\"green\">'group.id'</font>: <font color=\"green\">'my_group'</font>}\n",
    ")\n",
    "\n",
    "# Configure Kafka producer\n",
    "kafka_producer = FlinkKafkaProducer(\n",
    "    topic=<font color=\"green\">'output_topic'</font>,\n",
    "    schema=SimpleStringSchema(),\n",
    "    properties={<font color=\"green\">'bootstrap.servers'</font>: <font color=\"green\">'localhost:9092'</font>}\n",
    ")\n",
    "\n",
    "# Define the data processing pipeline\n",
    "data_stream = env.add_source(kafka_consumer).map(<font color=\"blue\">lambda</font> x: f<font color=\"green\">\"Processed: {x}\"</font>)\n",
    "\n",
    "# Send the processed data to the output topic\n",
    "data_stream.add_sink(kafka_producer)\n",
    "\n",
    "# Execute the data pipeline\n",
    "env.execute(<font color=\"green\">\"Real-Time Data Processing with Apache Flink in Python\"</font>)\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "### Faust\n",
    "\n",
    "\n",
    "##### 1. Overview\n",
    "\n",
    "Faust is a Python stream processing library, porting the ideas of Apache Kafka Streams to Python. It provides a way to build stream processing applications that are both highly efficient and straightforward to understand.\n",
    "\n",
    "##### 2. Features and Advantages\n",
    "\n",
    "- **Functional Style**: Utilizes functional programming style for defining and connecting processes in a pipeline.\n",
    "- **Native to Python**: Faust is native to Python, making it easier to use for teams familiar with Python.\n",
    "- **Windowed Aggregations**: Supports windowed aggregations, joins, and sessions.\n",
    "\n",
    "##### 3. Installation\n",
    "\n",
    "You can install Faust using pip with the following command:\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">!pip install</font> faust\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "##### 4. Real-Time Data Processing with Faust in Python\n",
    "\n",
    "Here’s an example of how you could use Faust for processing real-time data:\n",
    "\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">import</font> faust\n",
    "\n",
    "app = faust.App(<font color=\"green\">'my_app'</font>, broker=<font color=\"green\">'kafka://localhost:9092'</font>)\n",
    "\n",
    "<font color=\"blue\">class</font> Greeting(faust.Record):\n",
    "    from_name: <font color=\"blue\">str</font>\n",
    "    to_name: <font color=\"blue\">str</font>\n",
    "\n",
    "greetings_topic = app.topic(<font color=\"green\">'greetings'</font>, value_type=Greeting)\n",
    "\n",
    "<font color=\"blue\">@app.agent</font>(greetings_topic)\n",
    "<font color=\"blue\">async def</font> process_greetings(greetings):\n",
    "    <font color=\"blue\">async for</font> greeting <font color=\"blue\">in</font> greetings:\n",
    "        print(f<font color=\"green\">\"Hello {greeting.to_name} from {greeting.from_name}\"</font>)\n",
    "\n",
    "<font color=\"blue\">if</font> __name__ == <font color=\"green\">'__main__'</font>:\n",
    "    app.main()\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "<hr style=\"background: linear-gradient(to right, #f00, #00f); height: 5px; border: none;\" />\n",
    "\n",
    "\n",
    "### 1. Real-Time Streaming of ATM Transactions using Apache Kafka in AWS:\n",
    "\n",
    "Imagine a banking organization that wants to monitor ATM transactions in real-time to detect and prevent fraudulent activities. The transactions from various ATM machines can be streamed to a centralized system for real-time analysis and monitoring.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Data Collection**:\n",
    "    - ATM machines generate transaction data which includes transaction amount, timestamp, ATM location, account number, etc.\n",
    "    - This data is published to a Kafka topic on a Kafka cluster hosted in AWS.\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">from</font> kafka <font color=\"blue\">import</font> KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=<font color=\"green\">'your-aws-kafka-cluster:9092'</font>)\n",
    "\n",
    "<font color=\"blue\">def</font> send_transaction_to_kafka(transaction_data):\n",
    "    producer.send(<font color=\"green\">'atm-transactions'</font>, value=transaction_data)\n",
    "    producer.flush()\n",
    "</code></pre>\n",
    "\n",
    "2. **Real-Time Processing**:\n",
    "   - A real-time processing application consumes the transaction data from the Kafka topic.\n",
    "   - It applies various real-time analytics like checking for unusual withdrawal patterns, verifying account balances, or comparing transaction locations to identify possible fraudulent activities.\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">from</font> kafka <font color=\"blue\">import</font> KafkaConsumer\n",
    "<font color=\"blue\">import</font> json\n",
    "\n",
    "consumer = KafkaConsumer(<font color=\"green\">'atm-transactions'</font>, bootstrap_servers=<font color=\"green\">'your-aws-kafka-cluster:9092'</font>)\n",
    "\n",
    "<font color=\"blue\">for</font> message <font color=\"blue\">in</font> consumer:\n",
    "    transaction_data = json.loads(message.value)\n",
    "    <font color=\"comment\"># Real-time analytics to detect possible fraud</font>\n",
    "    detect_fraud(transaction_data)\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "### 2. Pub/Sub Messaging on Azure:\n",
    "\n",
    "In a retail scenario, a company wants to notify customers in real-time when the price of a watched item drops. They decide to use Azure Event Hubs for Pub/Sub messaging to implement this functionality.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Publishing Price Updates**:\n",
    "   - Whenever there's a price update for any item, the new price data is published to an Azure Event Hub.\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">from</font> azure.eventhub <font color=\"blue\">import</font> EventHubProducerClient, EventData\n",
    "\n",
    "producer = EventHubProducerClient(\n",
    "    fully_qualified_namespace=<font color=\"green\">\"your-namespace\"</font>,\n",
    "    eventhub_name=<font color=\"green\">\"your-eventhub\"</font>,\n",
    "    credential=YourCredential\n",
    ")\n",
    "\n",
    "<font color=\"blue\">def</font> send_price_update(item_id, new_price):\n",
    "    event_data = EventData(f<font color=\"green\">'{{\"item_id\": \"{item_id}\", \"new_price\": {new_price}}}'</font>)\n",
    "    producer.send(event_data)\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "2. **Subscribing to Price Updates**:\n",
    "   - A consumer application subscribes to the Event Hub to receive price updates.\n",
    "   - When a price update for a watched item is received, the application sends a notification to the respective customers.\n",
    "\n",
    "<pre><code class=\"language-python\">\n",
    "<font color=\"blue\">from</font> azure.eventhub <font color=\"blue\">import</font> EventHubConsumerClient\n",
    "<font color=\"blue\">import</font> json\n",
    "\n",
    "consumer = EventHubConsumerClient(\n",
    "    fully_qualified_namespace=<font color=\"green\">\"your-namespace\"</font>,\n",
    "    eventhub_name=<font color=\"green\">\"your-eventhub\"</font>,\n",
    "    consumer_group=<font color=\"green\">\"&dollar;Default\"</font>,\n",
    "    credential=YourCredential\n",
    ")\n",
    "\n",
    "<font color=\"blue\">def</font> on_event(partition_context, event):\n",
    "    price_data = json.loads(event.body_as_str())\n",
    "    notify_customers(price_data)\n",
    "\n",
    "consumer.receive(on_event)\n",
    "</code></pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865dcda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
