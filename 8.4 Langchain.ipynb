{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2e5a847",
   "metadata": {},
   "source": [
    "# Unveiling Langchain\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the rapidly evolving world of Generative AI (GenAI), Langchain offers unique capabilities to enhance the development of intelligent applications. Let's delve into the world of Langchain, exploring its genesis, capabilities, and benefits, particularly for data engineers.\n",
    "\n",
    "## Section 1: Understanding Langchain\n",
    "\n",
    "### What is Langchain?\n",
    "\n",
    "Langchain is a powerful tool that facilitates the development of GenAI applications. It serves as a bridge, integrating cutting-edge language models with data retrieval and processing systems. By providing a seamless interface between these elements, Langchain enhances the ability to generate insightful and contextually aware responses in AI applications.\n",
    "\n",
    "### Origin and Development\n",
    "\n",
    "Langchain was developed with a vision to enhance the interaction between AI models and data resources. It employs advanced techniques to analyze and understand natural language queries, enabling the retrieval of relevant data and the generation of insightful responses. The development process focused on creating a tool that can work harmoniously with existing AI models, augmenting their capabilities.\n",
    "\n",
    "## Section 2: The Significance of Langchain\n",
    "\n",
    "### Why is Langchain Needed?\n",
    "\n",
    "In the contemporary AI landscape, the ability to generate contextually aware responses is pivotal. Langchain addresses this by offering a mechanism to integrate rich data resources with AI models. It enables the models to have a deeper understanding of queries, thereby generating responses that are not only accurate but also enriched with context and insights.\n",
    "\n",
    "### Enhancing GenAI Applications\n",
    "\n",
    "Langchain serves as a vital tool in the development of GenAI applications. Its ability to facilitate the seamless interaction between AI models and data repositories allows for the creation of applications that can provide deeply insightful and contextually aware responses. Whether it's answering complex questions or generating content, Langchain empowers GenAI applications to perform at an elevated level of intelligence.\n",
    "\n",
    "## Section 3: Benefits for Data Engineers\n",
    "\n",
    "### Leveraging Langchain\n",
    "\n",
    "For data engineers, Langchain presents a plethora of benefits. It allows for the streamlined integration of AI models with data resources, reducing the complexity traditionally associated with such endeavors. Moreover, it provides a platform for data engineers to leverage the capabilities of advanced AI models, enhancing the value and insights that can be derived from data.\n",
    "\n",
    "---\n",
    "\n",
    "Langchain stands as a revolutionary tool in the GenAI landscape, offering unique capabilities that enhance the development and functionality of intelligent applications. Its emphasis on facilitating context-aware responses makes it a vital asset for data engineers, paving the way for a new era of insightful and intelligent data interactions.\n",
    "\n",
    "---\n",
    "\n",
    "# Context-Aware Responses in GenAI\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This write-up explores the mechanism that enables Langchain to generate such responses and delineates how it stands apart from conventional RAG models.\n",
    "\n",
    "## Section 1: The Mechanics of Context-Aware Responses in Langchain\n",
    "\n",
    "Langchain leverages state-of-the-art AI models and data retrieval systems to generate responses that are not only accurate but also deeply rooted in the context of the query. Let's delve into the specifics of how Langchain accomplishes this:\n",
    "\n",
    "### Context-Aware Response Generation\n",
    "\n",
    "Langchain employs a sophisticated approach to understanding and responding to queries. It integrates cutting-edge language models with data retrieval systems, allowing for a deep analysis of the query. This analysis, coupled with the retrieval of relevant data, enables the generation of responses that are both insightful and contextually aware.\n",
    "\n",
    "<br>\n",
    "<div class=\"alert alert-success\" role=\"alert\"><b>Langchain incorporates a series of advanced techniques to analyze queries at a granular level. Initially, it employs natural language processing to dissect the query and understand the underlying context. This deep analysis forms the basis for generating responses that are finely tuned to the nuances of the query.</b></div>\n",
    "\n",
    "\n",
    "### Integration with Data Resources\n",
    "\n",
    "A standout feature of Langchain is its ability to integrate seamlessly with various data resources. This integration facilitates the retrieval of data that is pertinent to the query, allowing for responses that are enriched with relevant information and insights.\n",
    "\n",
    "## Section 2: Differentiating Langchain from Conventional RAG Models\n",
    "\n",
    "Langchain not only enhances the capabilities of traditional RAG models but also brings unique features to the table. Below, we present a comparative analysis between Langchain and conventional RAG models, highlighting the distinctive benefits of Langchain:\n",
    "\n",
    "| Feature | Langchain | Conventional RAG Models |\n",
    "|---------|-----------|-------------------------|\n",
    "| **Contextual Understanding** | Deep analysis of queries to understand context at a granular level. | Limited to the understanding of the query based on predefined data patterns. |\n",
    "| **Data Integration** | Seamless integration with various data resources, allowing for enriched responses. | Generally restricted to specific data sets, limiting the depth of responses. |\n",
    "| **Response Generation** | Generates responses that are deeply insightful and contextually aligned with the query. | Generates responses based on matching patterns in the data, which may lack depth and context. |\n",
    "| **Customization and Flexibility** | Offers higher customization and flexibility in integrating diverse data resources and adjusting response generation. | Offers limited customization options, generally confined to the capabilities of the underlying model. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa0877",
   "metadata": {},
   "source": [
    "# Multiple LLM Models and Providers with Langchain\n",
    "\n",
    "Langchain stands out in the Generative AI landscape for its ability to integrate multiple LLM (Language and Logic Models) and providers into a unified system. This integration amplifies the capabilities of AI applications by harnessing the collective strengths of various models and providers. Let's delve into the details of how Langchain manages to seamlessly blend these elements to provide enriched services.\n",
    "\n",
    "\n",
    "## Flexible Integration of LLM Models and Providers\n",
    "\n",
    "Langchain offers a platform where users can seamlessly integrate various LLM models and providers to enhance the intelligence and contextual awareness of AI applications.\n",
    "\n",
    "### User-Driven and Automated Model Selection\n",
    "\n",
    "Langchain allows users the flexibility to specify the LLM models and providers they wish to integrate, offering a tailored approach to suit specific project needs. Moreover, it is equipped with the capability to automatically select the most apt models and providers based on the nuances of the query, ensuring an optimized response generation process.\n",
    "\n",
    "### Parallel Query Processing\n",
    "\n",
    "By facilitating parallel query processing across different LLMs, Langchain enhances the speed of response generation, providing a richer and more comprehensive set of insights. This parallel processing is coupled with an intelligent aggregation system that combines the insights derived from various models into a unified, well-rounded response.\n",
    "\n",
    "\n",
    "<br>\n",
    "<div class=\"alert alert-info\" role=\"alert\"><b>Langchain is designed to work harmoniously with multiple LLMs, integrating them into a single cohesive system. This integration means that Langchain can leverage the unique strengths and capabilities of different LLMs, combining their powers to generate even more insightful and contextually rich responses.</b></div>\n",
    "\n",
    "### How Langchain Achieves Multi-LLM Integration\n",
    "\n",
    "Langchain incorporates multiple LLMs through a sophisticated system that facilitates seamless interaction between different models. The key steps in this process are:\n",
    "\n",
    "1. **Query Routing**: Based on the nature of the query, Langchain determines which LLMs are best suited to process it. This is achieved through an intelligent routing system that can analyze the query and route it to the appropriate LLMs.\n",
    "\n",
    "2. **Parallel Processing**: Langchain is capable of processing queries in parallel across different LLMs. This parallel processing not only speeds up the response time but also allows for a richer and more diverse set of responses.\n",
    "\n",
    "3. **Response Aggregation**: After processing the query, Langchain aggregates the responses from different LLMs. It employs advanced algorithms to combine these responses into a single, cohesive, and comprehensive response.\n",
    "\n",
    "## Advantages of Multi-LLM Integration\n",
    "\n",
    "### Enhanced Contextual Understanding\n",
    "\n",
    "By incorporating multiple LLMs, Langchain can analyze queries from different perspectives, leading to a more nuanced and comprehensive understanding of the context.\n",
    "\n",
    "### Richer Responses\n",
    "\n",
    "Multi-LLM integration enables Langchain to generate responses that are richer and more insightful, as it can draw upon the combined knowledge and capabilities of various LLMs.\n",
    "\n",
    "### Customization and Scalability\n",
    "\n",
    "The ability to integrate multiple LLMs allows for greater customization and scalability. Depending on the specific requirements, Langchain can be configured to incorporate different combinations of LLMs, offering a highly adaptable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b05c529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet langchain langchain-community langchain-openai faiss-cpu tiktoken> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb8ca5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datetime import date\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm.notebook import tqdm\n",
    "import timeit\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29cb1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = ChatOpenAI(temperature=0, max_tokens=1024, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f568463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fc192a037b432abff7da084ddf704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading PDFs:   0%|          | 0/2301 [00:00<?, ?page/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 3.33 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "path = \"gitignore-files/\"\n",
    "pdf_texts = {}\n",
    "\n",
    "# Get total number of pages across all files\n",
    "total_pages = sum([len(fitz.open(os.path.join(path, f))) for f in os.listdir(path) if f.endswith(\".pdf\")])\n",
    "\n",
    "progress_bar = tqdm(total=total_pages, desc=\"Reading PDFs\", unit=\"page\")\n",
    "\n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        doc = fitz.open(os.path.join(path, file_name))\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "            progress_bar.update(1)\n",
    "        pdf_texts[file_name] = text\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(f'Total time taken: {elapsed_time:.2f} seconds')\n",
    "\n",
    "processed_texts = {file_name: text.lower().replace('\\n', ' ') for file_name, text in pdf_texts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5842c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\"Based on the novels, {question}\")\n",
    "\n",
    "# Set up an output parser to convert raw output to string\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Initialize the OpenAI model with GPT-4\n",
    "model = ChatOpenAI(model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ef8c8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly, I'd be delighted to tell you an original story inspired by the essence of novel storytelling.\n",
      "\n",
      "---\n",
      "\n",
      "**The Last Page of Summer**\n",
      "\n",
      "In the quaint village of Eldoria, nestled between whispering forests and rolling hills, there lived a young boy named Finn. Finn was an avid reader, with an insatiable hunger for stories. He dreamt of adventures beyond the horizon, of heroes and heroines, of magic that whispered in the wind. Yet, his reality was firmly rooted in the cobblestone streets of Eldoria, where the most significant event was the annual Harvest Festival.\n",
      "\n",
      "One balmy summer afternoon, as Finn meandered through the village's modest library, his fingers brushed against a spine that seemed to hum with a peculiar energy. It was an old, leather-bound book, its title faded with time. Curiosity piqued, Finn checked out the book and hurried home, unaware that his life was about to change.\n",
      "\n",
      "As he turned the pages, the world around him began to shift. The walls of his room stretched and blurred until they were no more, replaced by an emerald forest that shimmered under a sky painted with twilight. Finn stood at the threshold of an adventure he had only dared to dream of.\n",
      "\n",
      "He learned that this realm was known as Thalion, a land suspended in perpetual summer, where the sun kissed the earth in an endless embrace. However, Thalion was in peril. An ancient darkness, long forgotten, had begun to stir in the depths of the Shadowmere, threatening to engulf the land in eternal night.\n",
      "\n",
      "The key to saving Thalion, as the book revealed, rested with the Last Summer Bloom, a mythical flower said to possess the power of the sun itself. But the bloom had not been seen for centuries, hidden away by the winds of fate.\n",
      "\n",
      "Guided by the wisdom of the book, Finn embarked on a journey across Thalion. He crossed paths with a cast of characters who would become the unlikeliest of allies: Aria, a fierce warrior with a melody for every mood; Tolen, a mage whose spells often misfired with humorous results; and Lirra, a rogue with a heart of gold, hidden beneath layers of cunning and stealth.\n",
      "\n",
      "Together, they braved the trials of the Whispering Woods, outwitted the cunning River Sirens, and scaled the peaks of the Everfrost Mountains. Their bond, forged in the fires of adversity, became Finn's greatest strength.\n",
      "\n",
      "As the final leaves of summer began to fall, Finn and his companions reached the heart of the Shadowmere. There, amidst the darkness, they found the Last Summer Bloom, its light a beacon of hope. But the ancient darkness would not yield so easily. A battle ensued, where courage and friendship were tested against the might of shadows.\n",
      "\n",
      "In the end, it was Finn's belief in the power of stories, of endings that lead to new beginnings, that turned the tide. He realized that the Last Summer Bloom needed not just sunlight but the light within, the light of hearts unbroken by despair.\n",
      "\n",
      "With the darkness vanquished, Thalion was bathed in a light brighter than the first dawn. The land healed, and the perpetual summer gave way to the natural cycle of seasons, each one a promise of rebirth and renewal.\n",
      "\n",
      "Finn awoke in his room, the book closed beside him. The adventure had ended, or perhaps, it was just beginning. For in his hand, he held a single petal from the Last Summer Bloom, a reminder that even in the most ordinary of places, magic awaits, ready to be discovered.\n",
      "\n",
      "And so, Finn's summer came to a close, not with the turning of a page, but with the promise of countless stories yet to be written, in the book of his life.\n",
      "\n",
      "---\n",
      "\n",
      "I hope you enjoyed this journey through the realms of imagination, a testament to the power of stories to transport us beyond the confines of our world.\n"
     ]
    }
   ],
   "source": [
    "def get_response(question, processed_texts):\n",
    "    # Create a basic chain with the prompt template, model, and output parser\n",
    "    chain = prompt_template | model | output_parser\n",
    "    \n",
    "    # Formulate the input for the chain\n",
    "    chain_input = {\"question\": question}\n",
    "    \n",
    "    # Invoke the chain with the user's question\n",
    "    response = chain.invoke(chain_input)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the get_response function with a sample question\n",
    "print(get_response(\"Can you tell me a story?\", processed_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c838b",
   "metadata": {},
   "source": [
    "## Langchain Code Walkthrough\n",
    "\n",
    "In this section, we analyze a script that utilizes Langchain to establish a response generation system. We'll delve into each component to understand its role in the process.\n",
    "\n",
    "### Initializing the LLM (Language and Logic Model)\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI(temperature=0, max_tokens=1024, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "```\n",
    "\n",
    "Here, we initialize an instance of the `ChatOpenAI` class to serve as the Language and Logic Model (LLM). The parameters are defined as follows:\n",
    "- `temperature=0`: Controls the randomness in the output generation, with a lower value producing more deterministic output.\n",
    "- `max_tokens=1024`: Defines the maximum number of tokens (words/characters) in the generated response.\n",
    "- `openai_api_key=os.getenv(\"OPENAI_API_KEY\")`: Securely retrieves the OpenAI API key from the environment variables.\n",
    "\n",
    "### Configuring the Chat Prompt Template\n",
    "\n",
    "```python\n",
    "prompt_template = ChatPromptTemplate.from_template(\"Based on the novels, {question}\")\n",
    "```\n",
    "\n",
    "In this step, a chat prompt template is created to structure the queries that will be sent to the model. The `{question}` acts as a placeholder that will be replaced by the actual user query during execution.\n",
    "\n",
    "### Setting Up the Output Parser\n",
    "\n",
    "```python\n",
    "output_parser = StrOutputParser()\n",
    "```\n",
    "\n",
    "Here, we instantiate the `StrOutputParser`, which will convert the raw output from the LLM into a string, making it presentable as a response.\n",
    "\n",
    "### Instantiating the OpenAI Model\n",
    "\n",
    "```python\n",
    "model = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "```\n",
    "\n",
    "This line initializes another `ChatOpenAI` instance, specifying the `gpt-4-turbo-preview` as the model to handle the response generation.\n",
    "\n",
    "### Function Definition: get_response\n",
    "\n",
    "```python\n",
    "def get_response(question, processed_texts):\n",
    "    # Create a basic chain with the prompt template, model, and output parser\n",
    "    chain = prompt_template | model | output_parser\n",
    "    \n",
    "    # Formulate the input for the chain\n",
    "    chain_input = {\"question\": question}\n",
    "    \n",
    "    # Invoke the chain with the user's question\n",
    "    response = chain.invoke(chain_input)\n",
    "    \n",
    "    return response\n",
    "```\n",
    "\n",
    "Here, we define a function `get_response` that:\n",
    "1. Creates a chain linking the `prompt_template`, `model`, and `output_parser`, outlining the flow from the input question to the final output.\n",
    "2. Constructs an input dictionary to pass the user's question to the chain.\n",
    "3. Invokes the chain with the user's question, generating and returning a response.\n",
    "\n",
    "### Function Test\n",
    "\n",
    "```python\n",
    "# Test the get_response function with a sample question\n",
    "print(get_response(\"Can you tell me a story?\", processed_texts))\n",
    "```\n",
    "\n",
    "Finally, the `get_response` function is tested using a sample question, demonstrating the functionality of the setup.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87097a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc9f4b49b5646d4b0122e7b1c68576f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing sentences:   0%|          | 0/3 [00:00<?, ?text/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 139.42 seconds\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize text into sentences\n",
    "def tokenize_text(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# Start the timer\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Create a thread pool and tokenize the texts in parallel\n",
    "sentences = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(tokenize_text, processed_texts.values()), total=len(processed_texts), desc=\"Tokenizing sentences\", unit=\"text\"))\n",
    "\n",
    "# Combine the results from all threads into the sentences list\n",
    "for result in results:\n",
    "    sentences.extend(result)\n",
    "\n",
    "# Initialize a vector store with sentence-wise context from the novels\n",
    "vector_store = FAISS.from_texts(sentences, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Set up a retriever that can find the most relevant context based on the user's question\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Stop the timer and print the total time taken\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(f\"Total time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6de84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this segment of the script, we are essentially preparing our system to be able to retrieve contextually relevant sentences in response to a user query. The script is achieving this through several steps - tokenizing the text data into sentences, initializing a vector store with sentence-wise context from the novels, and setting up a retriever. Let's dive deeper into each component and understand the role of `ThreadPoolExecutor` in optimizing this process.\n",
    "\n",
    "### Sentence Tokenization\n",
    "\n",
    "```python\n",
    "# Function to tokenize text into sentences\n",
    "def tokenize_text(text):\n",
    "    return sent_tokenize(text)\n",
    "```\n",
    "\n",
    "Here, a function named `tokenize_text` is defined to tokenize the input text into individual sentences using the `sent_tokenize` function. This step is crucial to break down the texts into manageable and analyzable units, which will later be used to retrieve relevant context.\n",
    "\n",
    "### Parallel Processing Using ThreadPoolExecutor\n",
    "\n",
    "```python\n",
    "# Start the timer\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Create a thread pool and tokenize the texts in parallel\n",
    "sentences = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(tokenize_text, processed_texts.values()), total=len(processed_texts), desc=\"Tokenizing sentences\", unit=\"text\"))\n",
    "```\n",
    "\n",
    "In this section, the `ThreadPoolExecutor` is introduced to optimize the tokenization process. \n",
    "\n",
    "<div class=\"alert alert-info\"><b>\n",
    "ThreadPoolExecutor is a class in the concurrent.futures module that provides a high-level interface for asynchronously executing callables. It creates a pool of threads, each of which can run a task. In this script, it is utilized to parallelize the tokenization of texts from multiple documents, significantly speeding up the process compared to sequential execution. The `executor.map` function is used to map the `tokenize_text` function to each text in `processed_texts.values()`, allowing for concurrent tokenization. Utilizing ThreadPoolExecutor is particularly beneficial when dealing with a large number of documents, as it can help to vastly reduce the overall processing time.\n",
    "</b></div>\n",
    "\n",
    "### Aggregating Results and Initializing Vector Store\n",
    "\n",
    "```python\n",
    "# Combine the results from all threads into the sentences list\n",
    "for result in results:\n",
    "    sentences.extend(result)\n",
    "\n",
    "# Initialize a vector store with sentence-wise context from the novels\n",
    "vector_store = FAISS.from_texts(sentences, embedding=OpenAIEmbeddings())\n",
    "```\n",
    "\n",
    "Post parallel processing, the results from all threads are aggregated into a single list of sentences. Following this, a vector store is initialized using the FAISS (Facebook AI Similarity Search) library, creating embeddings for each sentence to facilitate quick and efficient similarity searches later on.\n",
    "\n",
    "### Setting up the Retriever\n",
    "\n",
    "```python\n",
    "# Set up a retriever that can find the most relevant context based on the user's question\n",
    "retriever = vector_store.as_retriever()\n",
    "```\n",
    "\n",
    "Here, a retriever is set up using the vector store, which will be used to find the most relevant context based on the user's question during the response generation phase.\n",
    "\n",
    "### Timing the Operation\n",
    "\n",
    "```python\n",
    "# Stop the timer and print the total time taken\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(f\"Total time taken: {elapsed_time:.2f} seconds\")\n",
    "```\n",
    "\n",
    "Finally, the total time taken for the entire operation (from tokenization to retriever setup) is calculated and printed, allowing for performance monitoring and optimization.\n",
    "\n",
    "This segment of the script is crucial in setting up the backend system that enables the intelligent and context-aware responses that Langchain is known for. It ensures that the system is equipped to analyze and respond to user queries with contextually relevant and insightful responses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e54e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory component to remember previous interactions\n",
    "memory = ConversationBufferMemory(return_messages=True, output_key=\"answer\", input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b1d9eaf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears that you're invoking a scenario reminiscent of dialogues from novels, specifically those that might involve characters engaged in a mystery or an exchange of pivotal information. Even though the exact source of your quotes isn't provided, the names and the context suggest a nod towards the Harry Potter series by J.K. Rowling, where exchanges of riddles, stories, and crucial questions play a significant part in the narrative. Let's explore a hypothetical scenario inspired by the essence of your quotes:\n",
      "\n",
      "---\n",
      "\n",
      "**\"So - what's the story, Harry?\"**\n",
      "\n",
      "This question could be imagined as coming from one of Harry Potter's closest friends, perhaps Ron Weasley or Hermione Granger, in a moment of calm within the storm of their adventures. They're in the Gryffindor common room, the embers of the fireplace casting flickering shadows over their faces, as they lean in, eager for Harry to share a piece of crucial information that he's discovered about Voldemort's past, a hidden Horcrux, or a secret about Hogwarts itself.\n",
      "\n",
      "**\"Can I hear the riddle?\"**\n",
      "\n",
      "This line might follow in a scenario where the trio is grappling with one of the many magical challenges that come their way. It could be a Sphinx's riddle in the maze of the Triwizard Tournament, a cryptic clue left by Dumbledore in the hunt for the Horcruxes, or even a puzzle set by a protective enchantment. Here, Hermione's intellect shines, her mind racing through possibilities, logic, and lore to decipher the meaning hidden within the words.\n",
      "\n",
      "**\"Can I ask you something?\"** repeated for emphasis or perhaps by two different characters, underscores the gravity of the situation. The first could be Harry, hesitating yet needing to ask Dumbledore (or perhaps even Snape in a rare moment of truce) about something deeply personal and vital to his questâ€”maybe about his connection with Voldemort, or the true allegiance of Severus Snape. The second, could be a moment of vulnerability where Harry turns to Sirius or Lupin, seeking advice not as The Boy Who Lived, but as just Harry, a boy trying to find his way in a world that's both wonderful and perilous.\n",
      "\n",
      "---\n",
      "\n",
      "In crafting a narrative around these lines, we'd delve deep into the themes of friendship, courage, and the quest for knowledge that are so central to the Harry Potter series. Each question and riddle serves as a catalyst for action, pushing the characters towards growth, discovery, and ultimately, their destinies. The story weaves together the magical with the mundane, reminding us that it's not just the magic that makes us strong, but the bonds we form with others, the questions we dare to ask, and the challenges we choose to face.\n"
     ]
    }
   ],
   "source": [
    "# Define a more complex chat prompt template\n",
    "# Update the chat prompt template to be more directive\n",
    "template = \"\"\"You are an expert on the novels stored in your database. Considering the information from the novels, here is a question: {question}. Please provide a detailed response based on the context: {context}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create a function to extract context from the retriever\n",
    "def get_context(question):\n",
    "    # Ensure the question is a string before passing it to the retriever\n",
    "    top_matches = retriever.get_relevant_documents(str(question), top_n=5)\n",
    "    \n",
    "    # Extract the page_content from each Document object and combine them to form a richer context\n",
    "    combined_context = \" \".join([match.page_content for match in top_matches])\n",
    "    \n",
    "    return {\"context\": combined_context}\n",
    "\n",
    "# Create a function to package both context and question into a dictionary\n",
    "def package_input(input_dict):\n",
    "    return {\"context\": input_dict.get(\"context\"), \"question\": input_dict.get(\"question\")}\n",
    "\n",
    "# Update the chain to use the new package_input function\n",
    "chain = RunnableLambda(get_context) | RunnableLambda(package_input) | prompt | model | StrOutputParser()\n",
    "\n",
    "\n",
    "def get_response(question):\n",
    "    # Invoke the chain with the user's question to get the response\n",
    "    chain_response = chain.invoke({\"question\": question})\n",
    "    \n",
    "    # The chain_response is already a string, so we can directly assign it to the answer variable\n",
    "    answer = chain_response if chain_response else \"Sorry, I couldn't find an answer to that question.\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the get_response function with a sample question\n",
    "print(get_response(\"Can you tell me a story?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc52e904",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: does ahab kill the whale?\n",
      "Bot: The passage you've referenced, while paraphrased, evokes the central themes and character dynamics of Herman Melville's \"Moby-Dick.\" Captain Ahab's obsession with the white whale, Moby Dick, is not just a personal vendetta but becomes emblematic of a deeper, more existential quest that both isolates and, paradoxically, connects him to his crew and humanity at large. \n",
      "\n",
      "When considering the question \"Is Ahab, Ahab?\" in the context of the novel, it invites a reflection on the nature of identity, obsession, and the human condition. Ahab, as the captain of the Pequod, is defined by his single-minded pursuit of the white whale, Moby Dick, which has come to consume his entire being. This obsession is so intense that it transforms his identity, making him synonymous with his quest. Ahab is no longer just a man; he embodies the relentless, destructive pursuit of an insurmountable goal. \n",
      "\n",
      "The line \"Are they not one and all with Ahab, in this matter of the whale?\" suggests a unity between Ahab and his crew, born out of the shared experience of the hunt. While the crew may not share Ahab's personal vendetta against the whale, they are inexorably tied to his quest. Their fates are intertwined with his, for better or worse. This connection highlights the theme of collective human endeavor and the ways in which individual obsessions can engulf and define a community.\n",
      "\n",
      "Ahab's response, \"Hast seen the white whale?\" to the question about his identity further underscores the extent to which his quest has consumed him. His identity is so entangled with the pursuit of Moby Dick that he can only see himself through this lens. This moment encapsulates the tragedy of Ahab: his inability to disentangle his self from his obsession leads to his downfall and that of his crew.\n",
      "\n",
      "In \"Moby-Dick,\" Melville explores the destructive nature of obsession, the elusiveness of truth and knowledge, and the complexities of human nature. Ahab's character serves as a cautionary tale about the dangers of allowing a single goal or vendetta to define one's existence. His pursuit of Moby Dick is not just a hunt for a whale but a metaphor for the human condition, reflecting the eternal struggle between man and nature, knowledge and the unknown, and ultimately, between man and his own demons. \n",
      "\n",
      "So, in the context of the novel, Ahab is both singularly himself and a representation of universal human themes. His identity is inseparable from his quest, making him a compelling and tragic figure whose story resonates with the timeless inquiry into the nature of obsession and identity.\n",
      "You: quit\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = get_response(user_input)\n",
    "        print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e61296",
   "metadata": {},
   "source": [
    "## Script Analysis: Memory Initialization and Context-Aware Response Generation\n",
    "\n",
    "In this part of the script, we are enhancing the Langchain setup to incorporate memory components and generate context-aware responses. Here's a detailed breakdown of each section in this code block:\n",
    "\n",
    "### Initializing Conversation Memory\n",
    "\n",
    "```python\n",
    "# Initialize memory component to remember previous interactions\n",
    "memory = ConversationBufferMemory(return_messages=True, output_key=\"answer\", input_key=\"question\")\n",
    "```\n",
    "\n",
    "Here, the `ConversationBufferMemory` component is initialized to help the system remember previous interactions. This memory component can track the conversation history, which can be utilized to provide more contextually aware and coherent responses as the conversation progresses.\n",
    "\n",
    "### Setting Up an Advanced Chat Prompt Template\n",
    "\n",
    "```python\n",
    "# Define a more complex chat prompt template\n",
    "# Update the chat prompt template to be more directive\n",
    "template = \"\"\"You are an expert on the novels stored in your database. Considering the information from the novels, here is a question: {question}. Please provide a detailed response based on the context: {context}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "```\n",
    "\n",
    "In this section, a more complex and directive chat prompt template is defined. This template guides the model to consider itself an expert on the novels stored in its database and to provide detailed responses based on the context derived from these novels.\n",
    "\n",
    "### Functions for Context Retrieval and Input Packaging\n",
    "\n",
    "```python\n",
    "# Create a function to extract context from the retriever\n",
    "def get_context(question):\n",
    "    # Ensure the question is a string before passing it to the retriever\n",
    "    top_matches = retriever.get_relevant_documents(str(question), top_n=5)\n",
    "    \n",
    "    # Extract the page_content from each Document object and combine them to form a richer context\n",
    "    combined_context = \" \".join([match.page_content for match in top_matches])\n",
    "    \n",
    "    return {\"context\": combined_context}\n",
    "\n",
    "# Create a function to package both context and question into a dictionary\n",
    "def package_input(input_dict):\n",
    "    return {\"context\": input_dict.get(\"context\"), \"question\": input_dict.get(\"question\")}\n",
    "```\n",
    "\n",
    "Here, two functions are defined:\n",
    "1. `get_context`: This function takes a question as input, retrieves the top 5 relevant documents from the retriever, and combines their content to form a rich context for response generation.\n",
    "2. `package_input`: This function takes a dictionary with \"context\" and \"question\" keys and packages them into a new dictionary to be used as input for the chain.\n",
    "\n",
    "### Updating the Chain with New Functions\n",
    "\n",
    "```python\n",
    "# Update the chain to use the new package_input function\n",
    "chain = RunnableLambda(get_context) | RunnableLambda(package_input) | prompt | model | StrOutputParser()\n",
    "```\n",
    "\n",
    "In this section, the chain is updated to include the `get_context` and `package_input` functions, ensuring that the chain can now generate responses that are more context-aware and detailed.\n",
    "\n",
    "### Defining and Testing the get_response Function\n",
    "\n",
    "```python\n",
    "def get_response(question):\n",
    "    # Invoke the chain with the user's question to get the response\n",
    "    chain_response = chain.invoke({\"question\": question})\n",
    "    \n",
    "    # The chain_response is already a string, so we can directly assign it to the answer variable\n",
    "    answer = chain_response if chain_response else \"Sorry, I couldn't find an answer to that question.\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the get_response function with a sample question\n",
    "print(get_response(\"Can you tell me a story?\"))\n",
    "```\n",
    "\n",
    "Here, the `get_response` function is defined to invoke the chain with the user's question and return the generated response. It also includes a fallback response in case no answer is generated. The function is then tested with a sample question.\n",
    "\n",
    "### Implementing a Continuous Interaction Loop\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = get_response(user_input)\n",
    "        print(\"Bot:\", response)\n",
    "```\n",
    "\n",
    "Finally, a continuous loop is implemented below to allow users to interact with the bot in a conversational manner. The loop continues until the user inputs 'quit', upon which it terminates, and the bot bids goodbye.\n",
    "\n",
    "This section of the script enhances the Langchain setup by introducing a memory component and refining the chat prompt template, thereby facilitating more context-aware and detailed response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb797467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: does ahab kill the whale?\n",
      "Invoking the chain with the question...\n",
      "Retrieving relevant context...\n",
      "Formulating the answer...\n",
      "Bot: The passage you've provided resonates with the thematic core and the complex character study found in Herman Melville's magnum opus, \"Moby-Dick; or, The Whale\" (1851). Captain Ahab, the monomaniacal commander of the whaling ship Pequod, is indeed a multifaceted character whose obsession with the eponymous white whale, Moby Dick, drives not only his fate but also shapes the destiny of his crew. The line \"None. Please provide a detailed response based on the context: is Ahab, Ahab? Are they not one and all with Ahab, in this matter of the whale?\" can be seen as an exploration of Ahab's identity and how it becomes inseparable from his obsession.\n",
      "\n",
      "Ahab's quest for Moby Dick is not just a hunt for a whale but a profound, existential battle that questions the nature of fate, free will, and humanity's place in the universe. The dialogue \"Hast seen the white whale?\" \"Gritted Ahab in reply. So Ahab.\" succinctly captures Ahab's singular focus and determination. His identity is so intertwined with his quest that interactions with others often circle back to his obsession. When Ahab inquires about the white whale, it is a reflection of how his entire being is consumed by his vendetta against Moby Dick. This obsession is so intense that it defines his interactions, his command, and ultimately, his very essence.\n",
      "\n",
      "The question \"Are they not one and all with Ahab, in this matter of the whale?\" reflects on the crew's entanglement in Ahab's obsession. While not all share his monomania, the crew of the Pequod is drawn into Ahab's vortex of vengeance. This shared fate raises questions about collective responsibility, leadership, and the human capacity for obsession. Ahab's charisma and authoritative command compel his crew to follow him, despite the perilous nature of his quest.\n",
      "\n",
      "In a broader sense, Ahab's struggle with the white whale can be interpreted as humanity's struggle against the incomprehensible and uncontrollable forces of nature and fate. Ahab personifies the human desire to conquer and understand the unknown, regardless of the cost. His relentless pursuit of Moby Dick, despite the warnings and omens, illustrates the dangerous allure of obsession and the potential for it to consume one's identity and reason.\n",
      "\n",
      "In summary, Ahab's identity is inseparably linked to his quest for Moby Dick. His obsession not only defines him but also draws his crew into a shared doom. The dialogue you've mentioned captures the essence of Ahab's character and the central themes of Melville's novel, offering a poignant commentary on obsession, identity, and the human condition.\n",
      "You: quit\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Update the chat prompt template to focus solely on answering the question\n",
    "template = \"\"\"Please provide a direct and concise answer to the following question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def get_response(question):\n",
    "    print(\"Invoking the chain with the question...\")\n",
    "    # Invoke the chain with the user's question to get the initial response\n",
    "    chain_response = chain.invoke({\"question\": question})\n",
    "    \n",
    "    print(\"Retrieving relevant context...\")\n",
    "    # Retrieve relevant context based on the user's question\n",
    "    relevant_context = retriever.get_relevant_documents(str(question), top_n=5)\n",
    "    combined_context = \" \".join([match.page_content for match in relevant_context])\n",
    "    \n",
    "    print(\"Formulating the answer...\")\n",
    "    # Combine the initial response with information from the relevant context (if necessary)\n",
    "    answer = chain_response  # We can add logic here to supplement the answer with information from the relevant context if needed\n",
    "    \n",
    "    return answer\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = get_response(user_input)\n",
    "        print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd406f01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Breaking down the output generation\n",
    "\n",
    "1. **User Input**: \n",
    "   The user initiates the conversation with a query: \"does Ahab kill the whale?\".\n",
    "\n",
    "2. **Invoking the Chain with the Question**:\n",
    "   The `get_response` function is activated with the user's query, starting the chain of processes which is indicated by the message \"Invoking the chain with the question...\".\n",
    "\n",
    "3. **Retrieving Relevant Context**:\n",
    "   At this stage, the `get_context` function is invoked, denoted by the message \"Retrieving relevant context...\". This function utilizes the retriever (initialized earlier in the script) to find the top 5 documents or text matches that are pertinent to the question from the stored data (the tokenized sentences from the novels). These matches are determined based on the similarity between the content in the datastore and the user's query.\n",
    "\n",
    "4. **Formulating the Answer**:\n",
    "   Following the retrieval of the relevant context, the script transitions to the answer formulation phase, represented by the message \"Formulating the answer...\". Here, the `package_input` function takes charge, packaging the retrieved context and the initial question into a dictionary format. This input is then processed through a chat prompt template, instructing the LLM to consider the information from the novels and formulate a detailed response based on the retrieved context and the question.\n",
    "\n",
    "5. **Generating the Response**:\n",
    "   The packaged input is then supplied to the LLM (specified as \"gpt-4-turbo-preview\" in your script) to generate a response. The LLM analyzes the provided context and the question, crafting a detailed response in return. The `StrOutputParser` comes into play at the end of the chain to parse the raw LLM output into a string format, which is then ready to be presented as the response.\n",
    "\n",
    "6. **Output**:\n",
    "   The response generated by the LLM is displayed on the console as the Bot's reply. In this instance, the response is a comprehensive analysis of Ahab's character and his obsession with Moby Dick, grounded in the context extracted from the novels stored in the database.\n",
    "\n",
    "7. **Termination**:\n",
    "   If the user inputs 'quit', the script breaks the loop, concluding the conversation with a farewell message, \"Goodbye!\".\n",
    "\n",
    "In essence, the script you shared functions to find relevant text matches based on the user's query, and then employs the LLM to scrutinize these matches to construct a detailed, context-rich response. The generated response offers a profound analysis of the query, taking into account the context obtained from the novels in the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa79c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a template for question analysis\n",
    "question_analysis_template = \"Analyze the following question deeply to understand its essence: {question}\"\n",
    "question_analysis_prompt = ChatPromptTemplate.from_template(question_analysis_template)\n",
    "\n",
    "# Define a chain for question analysis\n",
    "question_analysis_chain = question_analysis_prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4463a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_texts(analyzed_question):\n",
    "    # Retrieve the most relevant sections from the novels based on the analyzed question\n",
    "    relevant_texts = retriever.get_relevant_documents(analyzed_question, top_n=3)\n",
    "    \n",
    "    # Combine the relevant texts to form a context for the response\n",
    "    combined_context = \" \".join([text.page_content for text in relevant_texts])\n",
    "    \n",
    "    return combined_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15fd7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a template for generating a direct answer\n",
    "answer_generation_template = \"Based on the novels in your database, please provide a direct and concise answer to this question: {question}. Use the following context from the novels to formulate your answer: {context}\"\n",
    "answer_generation_prompt = ChatPromptTemplate.from_template(answer_generation_template)\n",
    "\n",
    "# Define a chain for answer generation\n",
    "answer_generation_chain = answer_generation_prompt | model\n",
    "\n",
    "def generate_direct_answer(question, context):\n",
    "    # Generate a direct and concise answer using the answer generation chain\n",
    "    answer = answer_generation_chain.invoke({\"question\": question, \"context\": context})\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97a1a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question):\n",
    "    with tqdm(total=3, desc=\"Generating Response\", unit=\"step\") as pbar:\n",
    "        # Step 1: Analyze the question deeply\n",
    "        print(\"Invoking the chain with the question...\")\n",
    "        analyzed_question_output = question_analysis_chain.invoke({\"question\": question})\n",
    "        analyzed_question_text = analyzed_question_output.content  # Extract the text from the output object\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Step 2: Retrieve relevant texts based on the analyzed question\n",
    "        print(\"Retrieving relevant context...\")\n",
    "        retrieved_context = retrieve_relevant_texts(analyzed_question_text)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Step 3: Generate a direct and concise answer based on the retrieved context\n",
    "        print(\"Formulating the answer...\")\n",
    "        response = generate_direct_answer(question, retrieved_context)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a060809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: does ahab kill the whale?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae6c88e04824b9cac61885644fb2989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Response:   0%|          | 0/3 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking the chain with the question...\n",
      "Retrieving relevant context...\n",
      "Formulating the answer...\n",
      "Bot: content='No, Ahab does not kill the whale. In Herman Melville\\'s \"Moby-Dick,\" Captain Ahab is ultimately killed by Moby Dick, the white whale, in their final encounter.' response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 524, 'total_tokens': 565}, 'model_name': 'gpt-4-turbo-preview', 'system_fingerprint': 'fp_f38f4d6482', 'finish_reason': 'stop', 'logprobs': None} id='run-ecd1cb7c-d0e3-4091-9eaf-f17c7ad9178c-0'\n",
      "You: quit\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = get_response(user_input)\n",
    "        print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96deef1",
   "metadata": {},
   "source": [
    "## Refined Context-Aware Response Generation with Langchain\n",
    "\n",
    "In this advanced version of the script, the focus shifts towards a more analytical approach where the user's question is deeply analyzed first, followed by the extraction of relevant texts based on this analysis. This refined data then guides the model to formulate a direct and concise answer. Here's a detailed breakdown of the different sections of the updated script:\n",
    "\n",
    "### Setting Up Templates for Question Analysis and Answer Generation\n",
    "\n",
    "```python\n",
    "# Define a template for question analysis\n",
    "question_analysis_template = \"Analyze the following question deeply to understand its essence: {question}\"\n",
    "question_analysis_prompt = ChatPromptTemplate.from_template(question_analysis_template)\n",
    "\n",
    "# Define a template for generating a direct answer\n",
    "answer_generation_template = \"Based on the novels in your database, please provide a direct and concise answer to this question: {question}. Use the following context from the novels to formulate your answer: {context}\"\n",
    "answer_generation_prompt = ChatPromptTemplate.from_template(answer_generation_template)\n",
    "```\n",
    "\n",
    "In this section, two templates are set up:\n",
    "1. **Question Analysis Template**: Guides the model to analyze the question deeply to grasp its essence.\n",
    "2. **Answer Generation Template**: Directs the model to provide a direct and concise answer based on the context retrieved from the novels.\n",
    "\n",
    "### Creating Chains for Question Analysis and Answer Generation\n",
    "\n",
    "```python\n",
    "# Define a chain for question analysis\n",
    "question_analysis_chain = question_analysis_prompt | model\n",
    "\n",
    "# Define a chain for answer generation\n",
    "answer_generation_chain = answer_generation_prompt | model\n",
    "```\n",
    "\n",
    "Here, two separate chains are defined:\n",
    "1. **Question Analysis Chain**: Utilizes the question analysis prompt and the model to analyze the question deeply.\n",
    "2. **Answer Generation Chain**: Utilizes the answer generation prompt and the model to generate a direct and concise answer based on the retrieved context.\n",
    "\n",
    "### Function Definitions for Context Retrieval and Direct Answer Generation\n",
    "\n",
    "```python\n",
    "def retrieve_relevant_texts(analyzed_question):\n",
    "    # Retrieve the most relevant sections from the novels based on the analyzed question\n",
    "    relevant_texts = retriever.get_relevant_documents(analyzed_question, top_n=3)\n",
    "    \n",
    "    # Combine the relevant texts to form a context for the response\n",
    "    combined_context = \" \".join([text.page_content for text in relevant_texts])\n",
    "    \n",
    "    return combined_context\n",
    "\n",
    "def generate_direct_answer(question, context):\n",
    "    # Generate a direct and concise answer using the answer generation chain\n",
    "    answer = answer_generation_chain.invoke({\"question\": question, \"context\": context})\n",
    "    \n",
    "    return answer\n",
    "```\n",
    "\n",
    "These functions are defined to:\n",
    "1. **retrieve_relevant_texts**: Extract relevant texts based on the analyzed question. It combines these texts to form a richer context for the response.\n",
    "2. **generate_direct_answer**: Generate a direct and concise answer based on the question and the retrieved context.\n",
    "\n",
    "### Implementing the Main Get Response Function\n",
    "\n",
    "```python\n",
    "def get_response(question):\n",
    "    with tqdm(total=3, desc=\"Generating Response\", unit=\"step\") as pbar:\n",
    "        # Step 1: Analyze the question deeply\n",
    "        print(\"Invoking the chain with the question...\")\n",
    "        analyzed_question_output = question_analysis_chain.invoke({\"question\": question})\n",
    "        analyzed_question_text = analyzed_question_output.content  # Extract the text from the output object\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Step 2: Retrieve relevant texts based on the analyzed question\n",
    "        print(\"Retrieving relevant context...\")\n",
    "        retrieved_context = retrieve_relevant_texts(analyzed_question_text)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Step 3: Generate a direct and concise answer based on the retrieved context\n",
    "        print(\"Formulating the answer...\")\n",
    "        response = generate_direct_answer(question, retrieved_context)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    return response\n",
    "```\n",
    "\n",
    "This function orchestrates the entire process:\n",
    "1. **Analyzing the Question**: It starts with deeply analyzing the question to understand its essence.\n",
    "2. **Retrieving Relevant Context**: Next, it retrieves the most relevant texts based on the analyzed question.\n",
    "3. **Generating the Response**: Finally, it generates a direct and concise answer using the relevant context.\n",
    "\n",
    "### Continuous User Interaction Loop\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = get_response(user_input)\n",
    "        print(\"Bot:\", response)\n",
    "```\n",
    "\n",
    "This section implements a loop that allows for continuous interaction with the user, where they can keep asking questions until they decide to quit by typing 'quit'.\n",
    "\n",
    "In summary, this refined script shifts the focus towards a more analytical approach, where the user's question is deeply analyzed first to extract the essence of the query. This essence then guides the retrieval of relevant texts from the novels, which serve as the basis for generating a direct and concise answer, resulting in a more focused and contextually rich response generation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bbeb787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: is tom sawyer a good person?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e90d3a7b394db19bbb10813f0a5a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Response:   0%|          | 0/3 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking the chain with the question...\n",
      "Retrieving relevant context...\n",
      "Formulating the answer...\n",
      "Bot: Tom Sawyer, as depicted in Mark Twain's novel \"The Adventures of Tom Sawyer,\" is generally considered to be a good person, albeit complex and mischievous. His actions and personality traits include both commendable and questionable behaviors. Tom is adventurous, imaginative, and often goes out of his way to help others, which are positive traits. However, he also engages in activities that cause trouble for himself and those around him, such as playing hooky and concocting elaborate schemes. Despite his flaws, Tom's heart is in the right place, and he often demonstrates a strong sense of morality and justice by the end of the novel. Therefore, while Tom Sawyer may not be perfect, his overall character leans more towards being good.\n",
      "You: what is harry potter's weakness?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d634c483ba4bcf80032d90935987f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Response:   0%|          | 0/3 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking the chain with the question...\n",
      "Retrieving relevant context...\n",
      "Formulating the answer...\n",
      "Bot: Harry Potter's weakness lies in his emotional vulnerability, particularly his deep fear of losing his loved ones and the guilt he feels over those who have suffered or died because of him. His connection to Voldemort through his lightning scar also exposes him to manipulation and pain.\n",
      "You: quit\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = get_response(user_input)\n",
    "        \n",
    "        response_content = response.content\n",
    "        response_metadata = response.response_metadata  # Extract the response metadata\n",
    "\n",
    "        # Create a JSON object for the interaction\n",
    "        interaction_json = {\n",
    "            \"question\": user_input,\n",
    "            \"response\": {\n",
    "                \"content\": response_content,\n",
    "                \"response_metadata\": response_metadata,\n",
    "                \"id\": response.id\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Append the interaction JSON object to a list in the file\n",
    "        with open(f'chatbot-conversations/conversation_{date.today()}.json', 'a') as outfile:\n",
    "            json.dump(interaction_json, outfile, indent=4)\n",
    "            outfile.write(',\\n')  # Add a comma and newline to separate JSON objects\n",
    "        \n",
    "        print(\"Bot:\", response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224ceef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
