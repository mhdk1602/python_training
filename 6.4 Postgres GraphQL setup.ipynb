{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf28b4e",
   "metadata": {},
   "source": [
    "# Harnessing GraphQL: Interacting with PostgreSQL in Docker\n",
    "\n",
    "Having explored the advantages of GraphQL over REST APIs, we are now ready to delve deeper and leverage GraphQL to interact with relational databases, particularly PostgreSQL. Our journey will unfold as follows:\n",
    "\n",
    "1. **Setting Up a Docker Container**: We will kickstart our project by setting up a Docker container equipped with a PostgreSQL service, paving the way for a seamless and manageable development environment.\n",
    "\n",
    "2. **Understanding Idempotency**: Before we dive into database operations, it's crucial to grasp the concept of idempotency and its significance in database initialization scripts, ensuring that our setup scripts can be run multiple times without adverse effects.\n",
    "\n",
    "3. **Loading Tables**: Next, we will focus on crafting and executing Data Definition Language (DDL) scripts to initialize our PostgreSQL database with the necessary tables, all the while ensuring idempotent operations.\n",
    "\n",
    "4. **Initializing GraphQL**: With our database ready, we will turn our attention to setting up a GraphQL server. This step involves configuring GraphQL schemas that mirror our database structure, thereby setting the stage for seamless querying capabilities.\n",
    "\n",
    "5. **Querying with a GraphQL Client**: To bring it all together, we will employ a GraphQL client to interact with our PostgreSQL database. We will walk through the process of crafting and executing queries, allowing for flexible and efficient data retrieval from our database.\n",
    "\n",
    "By the end of this guide, you will be equipped with the knowledge and skills to set up and interact with a PostgreSQL database using GraphQL endpoints, marking a significant milestone in your GraphQL journey.\n",
    "\n",
    "\n",
    "### Setting Up a Docker Container\n",
    "\n",
    "In the initial phase of our project, we will be setting up a Docker container to host our PostgreSQL service. But why choose Docker in the first place? Here are a few compelling reasons:\n",
    "\n",
    "1. **Environment Consistency**: Docker ensures that your application runs the same regardless of where Docker is running. This eliminates the classic problem of \"it works on my machine\" scenarios, fostering consistency across development, testing, and production environments.\n",
    "   \n",
    "2. **Isolation and Security**: Docker containers are isolated from each other, which means that they have their own environments and file systems. This isolation enhances security by containing any potential application breaches to the individual container.\n",
    "\n",
    "3. **Ease of Setup and Use**: Setting up databases can sometimes be a complex task involving many steps. Docker containers can encapsulate all these complexities, allowing you to set up services like PostgreSQL with just a few commands. This ease of use accelerates the development cycle significantly.\n",
    "\n",
    "4. **Resource Efficiency**: Docker containers share the host system's kernel, rather than including their own operating system. This makes them lightweight and efficient in terms of system resources, which allows running many containers on a host machine without straining system resources.\n",
    "\n",
    "5. **Community and Ecosystem**: Docker has a vibrant community and a rich ecosystem of pre-built images available on Docker Hub. This means you can leverage the work of thousands of others to quickly and easily set up and deploy services.\n",
    "\n",
    "With these advantages in mind, our first step is to set up a Docker container running a PostgreSQL service. This process involves creating a `docker-compose.yml` file to define the service configurations and using Docker Compose commands to manage the lifecycle of our containers. \n",
    "\n",
    "Our PostgreSQL service configuration in the `docker-compose.yml` file will look something like this:\n",
    "\n",
    "```yaml\n",
    "version: '3.1'\n",
    "\n",
    "services:\n",
    "  postgres:\n",
    "    build: \n",
    "      context: ./postgres\n",
    "      dockerfile: Dockerfile\n",
    "    ports:\n",
    "      - \"5437:5432\"\n",
    "    environment:\n",
    "      POSTGRES_USER: godzilla\n",
    "      POSTGRES_PASSWORD: Mrawww\n",
    "      POSTGRES_DB: monsterverse\n",
    "      DATABASE_URL: postgres://godzilla:Mrawww@postgres/monsterverse\n",
    "      SHADOW_DATABASE_URL: postgres://godzilla:Mrawww@postgres/monsterverse_shadow\n",
    "      ROOT_DATABASE_URL: postgres://godzilla:Mrawww@postgres/postgres\n",
    "    volumes:\n",
    "      - ./postgres/data:/var/lib/postgresql/data\n",
    "      - ./postgres/init:/docker-entrypoint-initdb.d/\n",
    "    networks:\n",
    "      - my-network\n",
    "    restart: on-failure:10\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"pg_isready -U godzilla -d monsterverse -q && psql -U godzilla -d monsterverse -c 'SELECT 1' | grep 1\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 5\n",
    "\n",
    "networks:\n",
    "  my-network:\n",
    "    driver: bridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d6964",
   "metadata": {},
   "source": [
    "<br>\n",
    "In this configuration file:\n",
    "\n",
    "- The `services` block defines the PostgreSQL service, including build context, Dockerfile location, port mapping, and environment variables.\n",
    "- The `volumes` directive maps local folders to folders inside the container, facilitating data persistence and initialization script execution.\n",
    "- The `networks` block defines a custom network for facilitating communication between different services in Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0223a2",
   "metadata": {},
   "source": [
    "### Structuring Your Docker Project\n",
    "\n",
    "Before we delve into idempotency, it's important to understand the structure of our Docker project and the purpose behind each component. Let's break down the elements of the project:\n",
    "\n",
    "#### 1. **Postgres Folder**\n",
    "\n",
    "This folder serves as the central location where all PostgreSQL related files are stored. It helps in organizing your project by segregating the database files from other components of your application. Here’s a closer look at the important files and folders within the `postgres` folder:\n",
    "\n",
    "##### a. **Dockerfile**\n",
    "\n",
    "The `Dockerfile` is a blueprint that contains instructions for building a Docker image, which in turn is used to create containers. In our case, the `Dockerfile` includes instructions for setting up a PostgreSQL service. It specifies the base image to use (PostgreSQL in our case), environment variables, and other configurations necessary to run the PostgreSQL service.\n",
    "\n",
    "\n",
    "<pre><code>\n",
    "# Use the official image as a parent image\n",
    "FROM postgres:latest\n",
    "\n",
    "# Set the working directory in the container to /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY . /app\n",
    "\n",
    "# Make port 5432 available to the world outside this container\n",
    "EXPOSE 5432</code></pre>\n",
    "\n",
    "##### b. **init Folder**\n",
    "\n",
    "The `init` folder contains SQL scripts that are executed when the PostgreSQL container is initialized. These scripts are used to set up the database schema, create tables, and seed initial data. The scripts in this folder are executed in alphabetical order, hence naming them as `01_init.sql`, `02_init.sql`, etc., helps in controlling the execution order.\n",
    "\n",
    "##### c. **data Folder**\n",
    "\n",
    "This folder is mapped to the data directory of the PostgreSQL service inside the container. By mapping this folder as a volume, the data stored in the database persists even when the container is removed, ensuring that you don't lose your data when you bring down the Docker container.\n",
    "\n",
    "##### d. **init.sql Files**\n",
    "\n",
    "Files like `01_init.sql` and `02_init.sql` in the `init` folder contain SQL scripts to initialize the database. These scripts can include commands to create databases, tables, and populate them with initial data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc42d09",
   "metadata": {},
   "source": [
    "### Understanding Idempotency\n",
    "\n",
    "As we venture deeper into setting up our Dockerized PostgreSQL service, it's critical to grasp the concept of idempotency, a fundamental property that ensures the stability and reliability of our database initialization scripts.\n",
    "\n",
    "#### **What is Idempotency?**\n",
    "\n",
    "In the context of database operations, idempotency refers to the property of certain operations where they can be applied multiple times without changing the result beyond the initial application. In simpler terms, an idempotent operation, when executed multiple times, has the same effect as if it were executed just once.\n",
    "\n",
    "#### **Why is Idempotency Important?**\n",
    "\n",
    "1. **Reliable Initialization**: Ensuring that our initialization scripts are idempotent means that we can run them multiple times without worrying about adverse effects or inconsistencies in our database. This is particularly useful during the development phase where the database setup might change frequently.\n",
    "\n",
    "2. **Error Recovery**: In case of errors or interruptions during the initialization process, idempotent scripts allow for safe retries without the risk of duplicating data or corrupting the database state.\n",
    "\n",
    "3. **Simplified Maintenance**: Idempotent scripts simplify maintenance and updates, as they can be rerun safely whenever changes are made, without requiring complex checks or conditional logic.\n",
    "\n",
    "#### **Implementing Idempotency in SQL Scripts**\n",
    "\n",
    "To implement idempotency in SQL scripts, we can use conditional statements to check the existence of objects (like tables or databases) before attempting to create them. For instance, before creating a table, we can check if it already exists, and only create it if it doesn't. Here’s how you can do this in PostgreSQL:\n",
    "\n",
    "```sql\n",
    "DO\n",
    "$$\n",
    "BEGIN\n",
    "    IF NOT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'mytable' AND table_schema = 'public') THEN\n",
    "        CREATE TABLE public.mytable (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            name VARCHAR(50)\n",
    "        );\n",
    "    END IF;\n",
    "END\n",
    "$$;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3f0e1",
   "metadata": {},
   "source": [
    "In this script, the IF NOT EXISTS clause checks if the table 'mytable' already exists in the 'public' schema, and if not, it proceeds to create the table. This ensures that the script is idempotent and can be run multiple times without causing errors or duplications.\n",
    "\n",
    "As we move forward, we will be applying the principle of idempotency to our database initialization scripts, ensuring a robust and reliable setup process.\n",
    "\n",
    "Now before we move to the next step of loading data into the postgres instance. Let's walk through designing the data model and exploring some code for data prep.\n",
    "\n",
    "<b> Data Model: </b><br><br>\n",
    "<img src=\"https://i.postimg.cc/05DLhr1N/star-schema.png\" height = \"800\" width = \"1000\"><br><br>\n",
    "\n",
    "### Exploring the Data Model\n",
    "\n",
    "Our data model, depicted in the ER diagram above, is a meticulous representation of a financial database capturing detailed information about stock portfolios, company overviews, earnings, and stock prices at daily and intraday levels. Let's delve deeper into each entity in the model and understand their roles and relationships:\n",
    "\n",
    "#### **Entities and Attributes**\n",
    "\n",
    "1. **stocks**: \n",
    "    - **ticker (PK)**: A unique identifier representing the stock symbol of a company.\n",
    "    - **name**: The name of the company corresponding to the stock symbol.\n",
    "\n",
    "2. **portfolio**:\n",
    "    - **portfolio_id (PK)**: A unique identifier for each transaction in the portfolio.\n",
    "    - **ticker (FK)**: The stock symbol involved in the transaction, referencing the stocks entity.\n",
    "    - **transaction_date**: The date of the transaction.\n",
    "    - **action**: The type of transaction - either \"Buy\" or \"Sell\".\n",
    "    - **volume**: The number of shares involved in the transaction.\n",
    "    - **time**: The time at which the transaction took place.\n",
    "    - **close (optional)**: The per-share price at the time of the transaction.\n",
    "    - **total_transaction_amount (optional)**: The total value of the transaction.\n",
    "\n",
    "3. **company_overview**:\n",
    "    - **ticker (PK, FK)**: The stock symbol, serving as a foreign key referencing the stocks entity.\n",
    "    - **sector**: The business sector of the company.\n",
    "    - **industry**: The specific industry category within the sector.\n",
    "    - **market_cap**: The market capitalization value, formatted as a string.\n",
    "    - **description**: A description of the company.\n",
    "    - **as_of_date**: The date as of which the data is valid.\n",
    "\n",
    "4. **earnings**:\n",
    "    - **earnings_id (PK)**: A unique identifier for each earnings record.\n",
    "    - **ticker (FK)**: The stock symbol, referencing the stocks entity.\n",
    "    - **fiscal_year**: The fiscal year of the earnings data.\n",
    "    - **fiscal_period**: The fiscal quarter of the earnings data.\n",
    "    - **eps**: Earnings per share for the given period.\n",
    "    - **as_of_date**: The date as of which the earnings data is valid.\n",
    "\n",
    "5. **ticker_daily**:\n",
    "    - **daily_price_id (PK)**: A unique identifier for each daily price record.\n",
    "    - **ticker (FK)**: The stock symbol, referencing the stocks entity.\n",
    "    - **date**: The date of the price data.\n",
    "    - **open**: The opening price of the stock on the given date.\n",
    "    - **high**: The highest price of the stock on the given date.\n",
    "    - **low**: The lowest price of the stock on the given date.\n",
    "    - **close**: The closing price of the stock on the given date.\n",
    "    - **volume**: The number of shares traded on the given date.\n",
    "\n",
    "6. **ticker_intraday**:\n",
    "    - **intraday_price_id (PK)**: A unique identifier for each intraday price record.\n",
    "    - **daily_price_id (FK)**: A foreign key linking to the daily price record for the given date.\n",
    "    - **date_time**: The specific date and time of the intraday price data.\n",
    "    - **open**: The opening price in the given intraday interval.\n",
    "    - **high**: The highest price in the given intraday interval.\n",
    "    - **low**: The lowest price in the given intraday interval.\n",
    "    - **close**: The closing price at the end of the given intraday interval.\n",
    "    - **volume**: The number of shares traded during the intraday interval.\n",
    "\n",
    "#### **Relationships**\n",
    "\n",
    "The relationships between the entities are depicted as lines connecting them in the ER diagram, indicating how they are related and the nature of their relationships, which are described below:\n",
    "\n",
    "- **stocks ||--o{ portfolio**: A one-to-many relationship indicating that a stock can be included in multiple portfolio transactions.\n",
    "- **stocks ||--|| company_overview**: A one-to-one relationship indicating that each stock has a single company overview.\n",
    "- **stocks ||--o{ earnings**: A one-to-many relationship indicating that a stock can have multiple earnings reports.\n",
    "- **stocks ||--o{ ticker_daily**: A one-to-many relationship indicating that a stock can have multiple daily price records.\n",
    "- **ticker_daily ||--o{ ticker_intraday**: A one-to-many relationship indicating that each daily price record can have multiple intraday price records.\n",
    "\n",
    "#### **Optimization and Schema Type**\n",
    "\n",
    "The chosen schema is a Star Schema, optimized for querying large data sets, and is particularly useful in data warehouse environments. This schema type allows for efficient querying as it reduces the number of joins needed when querying related data, facilitating quick data retrieval. It is optimized for readability and ease of understanding, ensuring that users can construct queries with minimal complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f32b92",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "\n",
    "For the next steps, we will be using yfinance python package and polygon.io for fetching daily, and intraday prices for a set of stock tickers and its company info and earnings data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43483156",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Prevent truncation of column width\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f534c2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: psycopg2-binary in /opt/homebrew/lib/python3.9/site-packages (2.9.5)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d6235",
   "metadata": {},
   "source": [
    "<br>\n",
    "Lets start by building out the `stocks` table. We pick a list of 13 tickers and get the company name from `yfinance`<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7992f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame for the STOCKS table\n",
    "stocks_df = pd.DataFrame(columns=['Ticker', 'Name'])\n",
    "\n",
    "# List of tickers to include in the STOCKS table\n",
    "tickers = [\"META\", \"AAPL\", \"NVDA\", \"TSLA\", \"NFLX\", \"TSM\", \"VOO\", \"VTI\", \"AMD\", \"INTC\", \"GE\", \"MSFT\", \"GOOG\"]\n",
    "\n",
    "# Populate the STOCKS DataFrame with Ticker symbols and Names using yfinance\n",
    "for ticker_symbol in tickers:\n",
    "    ticker = yf.Ticker(ticker_symbol)\n",
    "    info = ticker.info\n",
    "    stocks_df = stocks_df.append({'Ticker': ticker_symbol, 'Name': info.get('longName')}, ignore_index=True)\n",
    "\n",
    "stocks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405b03a",
   "metadata": {},
   "source": [
    "I have a portfolio file with below contents \n",
    "\n",
    "| Ticker | Date   | DateTime    | Action | Volume |\n",
    "|--------|--------|-------------|--------|--------|\n",
    "| META   | 9/4/23 | 2:07:20 PM  | Buy    | 741    |\n",
    "| AAPL   | 4/6/23 | 12:17:07 AM | Buy    | 44     |\n",
    "| NVDA   | 6/16/23| 8:28:40 AM  | Buy    | 959    |\n",
    "| TSLA   | 7/22/23| 5:50:18 PM  | Buy    | 903    |\n",
    "| NFLX   | 3/15/23| 12:43:02 PM | Buy    | 586    |\n",
    "| TSM    | 7/15/23| 5:25:05 AM  | Buy    | 659    |\n",
    "| VOO    | 1/10/24| 1:25:54 AM  | Buy    | 892    |\n",
    "| VTI    | 4/28/23| 2:50:55 AM  | Buy    | 4      |\n",
    "| AMD    | 6/20/23| 5:58:31 PM  | Buy    | 717    |\n",
    "| INTC   | 5/31/23| 11:20:40 PM | Buy    | 835    |\n",
    "| GE     | 11/2/23| 1:43:34 AM  | Buy    | 784    |\n",
    "| MSFT   | 7/6/23 | 11:14:25 PM | Buy    | 12     |\n",
    "| GOOG   | 1/20/24| 11:31:05 PM | Buy    | 767    |\n",
    "| META   | 6/12/23| 10:26:24 AM | Buy    | 493    |\n",
    "| AAPL   | 12/1/23| 12:41:20 PM | Buy    | 965    |\n",
    "| NVDA   | 3/10/23| 1:08:57 AM  | Buy    | 434    |\n",
    "| TSLA   | 10/9/23| 4:29:12 AM  | Buy    | 715    |\n",
    "| NFLX   | 1/21/24| 5:01:43 PM  | Buy    | 90     |\n",
    "| TSM    | 5/1/23 | 10:30:17 AM | Buy    | 607    |\n",
    "| VOO    | 9/9/23 | 11:39:50 PM | Buy    | 566    |\n",
    "\n",
    "\n",
    "I'll be querying yfinance to get the corresponding price and multiply with volume to get total transaction amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1495f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "# Function to adjust date for weekends/holidays by finding the next available trading day\n",
    "def adjust_date_for_market(date):\n",
    "    # Check if the date is a weekend and adjust to next Monday if it is\n",
    "    if date.weekday() > 4:  # 5 = Saturday, 6 = Sunday\n",
    "        date += BDay(1)\n",
    "    return date\n",
    "\n",
    "# Function to fetch the closest available price to the given datetime (considering market hours)\n",
    "def fetch_price(ticker, date, time):\n",
    "    # Fetch daily data for the date\n",
    "    df_daily = yf.download(ticker, start=date, end=date + timedelta(days=1), progress=False)\n",
    "    if not df_daily.empty:\n",
    "        return df_daily.iloc[0]['Open']  # Use opening price of the day\n",
    "    else:\n",
    "        # If data is empty (weekend or holiday), find the next available trading day\n",
    "        next_date = adjust_date_for_market(date + timedelta(days=1))\n",
    "        df_next = yf.download(ticker, start=next_date, end=next_date + timedelta(days=1), progress=False)\n",
    "        if not df_next.empty:\n",
    "            return df_next.iloc[0]['Open']  # Use opening price of the next trading day\n",
    "    return None\n",
    "\n",
    "# Load the portfolio CSV file\n",
    "portfolio_df = pd.read_csv('portfolio.csv')\n",
    "\n",
    "# Convert 'Date' and 'DateTime' columns\n",
    "portfolio_df['Date'] = pd.to_datetime(portfolio_df['Date'])\n",
    "portfolio_df['DateTime'] = pd.to_datetime(portfolio_df['DateTime'])\n",
    "portfolio_df['Time'] = portfolio_df['DateTime'].dt.time\n",
    "\n",
    "# Iterate over each row in the portfolio to fetch prices\n",
    "for index, row in portfolio_df.iterrows():\n",
    "    adjusted_date = adjust_date_for_market(row['Date'])\n",
    "    price = fetch_price(row['Ticker'], adjusted_date, row['Time'])\n",
    "    portfolio_df.at[index, 'Close'] = round(price, 2) if price else None\n",
    "\n",
    "# Calculate the total transaction amount\n",
    "portfolio_df['Total_Transaction_Amount'] = round(portfolio_df['Volume'] * portfolio_df['Close'], 2)\n",
    "\n",
    "# Add an ID field as a unique identifier\n",
    "portfolio_df.reset_index(inplace=True)\n",
    "portfolio_df.rename(columns={'index': 'ID'}, inplace=True)\n",
    "portfolio_df.rename(columns={'DateTime': 'TransactionDateTime', 'Date': 'TransactionDate'}, inplace=True)\n",
    "\n",
    "# Adjust the 'ID' field to start from 1 for readability\n",
    "portfolio_df['ID'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dba996",
   "metadata": {},
   "source": [
    "Next we query polygon API for the same ticker list to get the compnay overview info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9abb7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Your Polygon API key\n",
    "API_KEY = 'YOUR_POLYGON_API_KEY'\n",
    "\n",
    "# List of tickers\n",
    "tickers = [\"META\", \"AAPL\", \"NVDA\", \"TSLA\", \"NFLX\", \"TSM\", \"VOO\", \"VTI\", \"AMD\", \"INTC\", \"GE\", \"MSFT\", \"GOOG\"]\n",
    "\n",
    "def format_market_cap(market_cap):\n",
    "    \"\"\"Format the market cap value to millions (M) or billions (B) with 2 decimal places.\"\"\"\n",
    "    if market_cap >= 1e12:  # Trillion\n",
    "        return f\"{market_cap / 1e12:.2f}T\"\n",
    "    elif market_cap >= 1e9:  # Billion\n",
    "        return f\"{market_cap / 1e9:.2f}B\"\n",
    "    elif market_cap >= 1e6:  # Million\n",
    "        return f\"{market_cap / 1e6:.2f}M\"\n",
    "    else:\n",
    "        return f\"{market_cap:.2f}\"\n",
    "\n",
    "def fetch_yfinance_overview(symbol):\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    info = ticker.info\n",
    "    market_cap = info.get(\"marketCap\")\n",
    "    formatted_market_cap = format_market_cap(market_cap) if market_cap else \"N/A\"\n",
    "    as_of_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    return {\n",
    "        \"Ticker\": symbol,\n",
    "        \"Name\": info.get(\"longName\"),\n",
    "        \"Sector\": info.get(\"sector\"),\n",
    "        \"Industry\": info.get(\"industry\"),\n",
    "        \"MarketCap\": formatted_market_cap,\n",
    "        \"Description\": info.get(\"longBusinessSummary\"),\n",
    "        \"As_of_Date\": as_of_date\n",
    "    }\n",
    "\n",
    "# Fetch company overview for each ticker and store in a list\n",
    "company_overviews = [fetch_yfinance_overview(ticker) for ticker in tickers]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "company_overviews_df = pd.DataFrame(company_overviews)\n",
    "\n",
    "company_overviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9e340",
   "metadata": {},
   "source": [
    "<br>\n",
    "Next we will continue querying the earnings endpoint of polygon api to get quarterly earnings for a year for the above ticker symbols.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eea82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time  # For adding sleep\n",
    "\n",
    "def fetch_quarterly_financials_2023(symbol):\n",
    "    # Set the date range for filings in 2023\n",
    "    start_date = \"2023-01-01\"\n",
    "    end_date = \"2024-01-01\"\n",
    "    \n",
    "    url = f\"https://api.polygon.io/vX/reference/financials?ticker={symbol}&timeframe=quarterly&filing_date.gte={start_date}&filing_date.lt={end_date}&apiKey={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        earnings_list = [{\n",
    "            \"Ticker\": symbol,\n",
    "            \"FiscalYear\": result.get(\"fiscal_year\"),\n",
    "            \"FiscalPeriod\": result.get(\"fiscal_period\"),\n",
    "            \"EPS\": result.get(\"financials\", {}).get(\"income_statement\", {}).get(\"basic_earnings_per_share\", {}).get(\"value\", 0),\n",
    "            \"AsOfDate\": datetime.now().strftime('%Y-%m-%d')  # Use current date as AsOfDate\n",
    "        } for result in data.get(\"results\", [])]\n",
    "        return earnings_list\n",
    "    else:\n",
    "        print(f\"Failed to fetch quarterly financials for {symbol}: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Initialize a list to store all earnings data for 2023\n",
    "all_earnings_data_2023 = []\n",
    "\n",
    "# Process tickers in batches to adhere to the rate limit\n",
    "batch_size = 5\n",
    "for i in range(0, len(tickers), batch_size):\n",
    "    batch = tickers[i:i+batch_size]\n",
    "    for ticker in batch:\n",
    "        earnings_data = fetch_quarterly_financials_2023(ticker)\n",
    "        all_earnings_data_2023.extend(earnings_data)\n",
    "    # Wait for 60 seconds after each batch except the last one\n",
    "    if i + batch_size < len(tickers):\n",
    "        print(\"Waiting for 60 seconds to respect the API rate limit...\")\n",
    "        time.sleep(60)\n",
    "\n",
    "# Convert the list of earnings data to a DataFrame\n",
    "earnings_df_2023 = pd.DataFrame(all_earnings_data_2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1f149",
   "metadata": {},
   "source": [
    "<br>Next, we will use `yfinance` to get daily and latest intraday prices for the stock symbols\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22255c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize an empty DataFrame for daily stock prices\n",
    "daily_stock_prices_df = pd.DataFrame()\n",
    "\n",
    "# Fetch daily stock prices for the past year for each ticker\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching data for {ticker}...\")\n",
    "    data = yf.download(ticker, period=\"1y\", interval=\"1d\")\n",
    "    \n",
    "    # Check if data was fetched successfully\n",
    "    if not data.empty:\n",
    "        # Add ticker symbol to the DataFrame\n",
    "        data['Ticker'] = ticker\n",
    "        # Reset the index to make 'Date' a column, not an index\n",
    "        data.reset_index(inplace=True)\n",
    "        # Append the data to the daily_stock_prices_df DataFrame\n",
    "        daily_stock_prices_df = pd.concat([daily_stock_prices_df, data[['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']]], ignore_index=True)\n",
    "\n",
    "# Add an ID field as a unique identifier for each record\n",
    "daily_stock_prices_df.reset_index(inplace=True)\n",
    "daily_stock_prices_df.rename(columns={'index': 'ID'}, inplace=True)\n",
    "daily_stock_prices_df['ID'] += 1  # Start IDs from 1 for readability\n",
    "\n",
    "# Convert 'Date' to date format (YYYY-MM-DD) if it's not already\n",
    "daily_stock_prices_df['Date'] = pd.to_datetime(daily_stock_prices_df['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80778b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "# Define the list of tickers\n",
    "tickers = [\"META\", \"AAPL\", \"NVDA\", \"TSLA\", \"NFLX\", \"TSM\", \"VOO\", \"VTI\", \"AMD\", \"INTC\", \"GE\", \"MSFT\", \"GOOG\"]\n",
    "\n",
    "# Determine the latest business day\n",
    "latest_business_day = pd.datetime.now() - BDay(1)\n",
    "\n",
    "# Initialize an empty DataFrame for intraday stock prices\n",
    "intraday_prices_df = pd.DataFrame()\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching intraday data for {ticker}...\")\n",
    "    # Fetch intraday data with 1-minute interval for the latest business day\n",
    "    data = yf.download(ticker, start=latest_business_day, interval=\"1m\", progress=False)\n",
    "    \n",
    "    # Check if data was fetched successfully\n",
    "    if not data.empty:\n",
    "        # Add ticker symbol to the DataFrame\n",
    "        data['Ticker'] = ticker\n",
    "        # Reset the index to make 'Datetime' a column, not an index\n",
    "        data.reset_index(inplace=True)\n",
    "        # Append the data to the intraday_prices_df DataFrame\n",
    "        intraday_prices_df = pd.concat([intraday_prices_df, data[['Datetime', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']]], ignore_index=True)\n",
    "\n",
    "# Add an ID field as a unique identifier for each record\n",
    "intraday_prices_df.reset_index(inplace=True)\n",
    "intraday_prices_df.rename(columns={'index': 'ID'}, inplace=True)\n",
    "intraday_prices_df['ID'] += 1  # Start IDs from 1 for readability\n",
    "\n",
    "# Ensure 'Datetime' is in the correct datetime format\n",
    "intraday_prices_df['Datetime'] = pd.to_datetime(intraday_prices_df['Datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5dbf504d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'ticker', 'transaction_date', 'transaction_time', 'action',\n",
       "       'volume', 'time', 'close', 'total_transaction_amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_df.rename(columns={'ID': 'id', 'Ticker': 'ticker','TransactionDate': 'transaction_date', 'TransactionDateTime': 'transaction_time','Action': 'action', 'Volume': 'volume','Time': 'time', 'Close': 'close', 'Total_Transaction_Amount': 'total_transaction_amount'}, inplace=True)\n",
    "portfolio_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e722ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df = portfolio_df[['id', 'ticker', 'transaction_date', 'action',\n",
    "       'volume', 'time', 'close', 'total_transaction_amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "915e8847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "\n",
    "portfolio_df.to_sql('portfolio', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de50cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
