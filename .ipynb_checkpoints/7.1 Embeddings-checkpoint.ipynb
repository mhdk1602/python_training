{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76020c5",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "### Overview\n",
    "\n",
    "#### What are Embeddings?\n",
    "\n",
    "- Embeddings are a technique in machine learning where words or objects are represented as vectors in a multidimensional space. This representation is based on the context in which words or objects appear, and it captures the semantic relationships between them. For instance:\n",
    "  - Words with similar meanings would be mapped to a similar region in the space.\n",
    "  - The distance between vectors can indicate the level of similarity between the words or objects they represent.\n",
    "  \n",
    "  This technique allows machine learning models to understand and process data more effectively, enhancing their ability to make accurate predictions or classifications.\n",
    "\n",
    "#### Connection between Text Comparison and Embeddings\n",
    "\n",
    "- Text comparison is a technique often utilized in the initial stages of data analysis, where the goal is to find similarities or differences between texts based on certain criteria or metrics. Embeddings further this analysis by providing a more nuanced method of comparison. For instance:\n",
    "  - Consider the sentences: \"The cat is on the roof\" and \"The feline is on top of the house\". \n",
    "  - A simple text comparison might indicate that these sentences are quite different.\n",
    "  - However, when we use embeddings, we can represent words such as 'cat' and 'feline', 'roof' and 'house', 'on' and 'on top of' as vectors in a space where their proximity indicates similarity. This representation allows us to perceive the sentences as more similar than a simple text comparison would suggest.\n",
    "  \n",
    "  Embeddings, therefore, offer a way to capture the synonymous nature of words and phrases, translating them into a numerical format that can be used for deeper analyses and various machine learning tasks.\n",
    "\n",
    "#### Role of Data Engineering in Working with Embeddings\n",
    "\n",
    "- Data engineering plays a significant role in effectively managing the data pipelines associated with embeddings. This role encompasses several tasks including:\n",
    "  - **Data Cleaning**: Ensuring the data is cleaned and preprocessed to remove any noise or irrelevant information.\n",
    "  - **Data Transformation**: Transforming data into a format that is suitable for generating embeddings.\n",
    "  - **Optimized Storage Solutions**: Developing strategies for storing and retrieving the generated embeddings efficiently, which is crucial for facilitating smooth data processing and analysis.\n",
    "  \n",
    "  These aspects of data engineering are vital when working with large datasets typically encountered in embedding generation tasks, ensuring a streamlined workflow in machine learning projects that utilize embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32508b07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Types of Embeddings\n",
    "\n",
    "### Overview\n",
    "\n",
    "In the field of machine learning and natural language processing, various types of embeddings are used to convert words or items into numerical vectors. Understanding the different types of embeddings can help in choosing the right approach for a specific task. In this section, we will explore some of the popular types of embeddings:\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "#### Word2Vec\n",
    "- **Description**: Word2Vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.\n",
    "- **Example**: If the word 'king' is often found in the same context as 'queen', their vectors would be close in the vector space.\n",
    "- **Application in Data Engineering**: Data engineers may utilize Word2Vec for tasks such as semantic search, where they need to identify items that are semantically similar to a given item.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e47786",
   "metadata": {},
   "source": [
    "In the below script, we are utilizing the `gensim` library to create word embeddings using the Word2Vec model. Here's a step-by-step explanation of the script:\n",
    "\n",
    "1. **Importing the Word2Vec Class**: \n",
    "   ```python\n",
    "   from gensim.models import Word2Vec\n",
    "   ```\n",
    "   We are importing the `Word2Vec` class from the `gensim.models` module.\n",
    "\n",
    "2. **Creating a List of Sentences**:\n",
    "   ```python\n",
    "   sentences = [['I', 'love', 'coding'], ['Python', 'is', 'my', 'favorite', 'language'], ['I', 'love', 'data', 'engineering']]\n",
    "   ```\n",
    "   We are creating a list of sentences, where each sentence is represented as a list of words (tokens).\n",
    "\n",
    "3. **Initializing and Training the Word2Vec Model**:\n",
    "   ```python\n",
    "   model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "   ```\n",
    "   We are initializing and training the Word2Vec model with the following parameters:\n",
    "   - `sentences`: The data on which the model will be trained, a list of sentence tokens.\n",
    "   - `vector_size=100`: The number of dimensions of the embedding space, meaning each word will be represented as a 100-dimensional vector.\n",
    "   - `window=5`: The maximum distance between the current and predicted word within a sentence during training. In this case, it considers 5 words on both sides of the current word.\n",
    "   - `min_count=1`: Ignores all words with a total frequency lower than this. Here, it includes all words as the minimum count is 1.\n",
    "   - `workers=4`: The number of CPU cores to use to train the model; higher numbers will lead to faster training.\n",
    "\n",
    "4. **Getting the Vector Representation of a Word**:\n",
    "   ```python\n",
    "   vector = model.wv['coding']\n",
    "   ```\n",
    "   After training the model, we are getting the vector representation of the word 'coding'. This vector is a 100-dimensional array representing the 'coding' word in the vector space created by Word2Vec.\n",
    "\n",
    "5. **Output - Vector**:\n",
    "   The `vector` variable holds the 100-dimensional vector representation of the word 'coding'. \n",
    "\n",
    "6. **Usage of the Output Vector**:\n",
    "   This vector can be used in various natural language processing tasks such as:\n",
    "   - **Semantic Analysis**: To understand the semantic similarity between words by calculating the cosine similarity between their vectors.\n",
    "   - **Text Classification**: As features in machine learning models for tasks like sentiment analysis.\n",
    "   - **Information Retrieval**: To enhance search algorithms by finding semantically similar words or documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91dc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Training Word2Vec model on sample sentences\n",
    "sentences = [['I', 'love', 'coding'], ['Python', 'is', 'my', 'favorite', 'language'], ['I', 'love', 'data', 'engineering']]\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# Getting the vector representation for the word 'coding'\n",
    "vector = model.wv['coding']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1dbb4",
   "metadata": {},
   "source": [
    "#### GloVe (Global Vectors for Word Representation)\n",
    "- **Description**: GloVe, which stands for \"Global Vectors\", is a method to efficiently learn word vectors by leveraging both global statistical information and local context information from the corpus on which it is trained. The resulting vectors encapsulate semantic relationships between words.\n",
    "\n",
    "  In this script:\n",
    "  - We first import the `spaCy` library and load a pre-trained language model that includes GloVe embeddings.\n",
    "  - We then retrieve the vector representations for two words: 'computer' and 'laptop'.\n",
    "  - Next, we calculate the cosine similarity between the two vectors, which gives us a measure of the semantic similarity between the words.\n",
    "  - Finally, we print the calculated cosine similarity.\n",
    "\n",
    "- **Application in Data Engineering**: Data engineers can use GloVe embeddings to analyze text data more deeply. By using the vector representations, they can develop systems capable of understanding semantic relationships between words, which can be used in various applications such as semantic search, sentiment analysis, and recommendation systems.\n",
    "\n",
    "- **Output - Cosine Similarity**: The output is a cosine similarity score between the vectors representing the words 'computer' and 'laptop'. This score indicates the semantic similarity between the two words, with a higher score indicating greater similarity.\n",
    "\n",
    "- **Usage of the Output**: The calculated cosine similarity can be used in several ways:\n",
    "  - **Semantic Search**: To develop search algorithms that can find semantically similar words or documents.\n",
    "  - **Natural Language Processing**: To enhance NLP models by understanding the semantic relationships between words.\n",
    "  - **Data Analysis**: To perform data analysis where understanding the semantic relationships between words is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021006a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m spacy download en_core_web_md\n",
    "!{sys.executable} -m pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59a1e082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the words 'computer' and 'laptop' is: 0.6255755212776157\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model that includes GloVe embeddings\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Get the vector representations for the words 'computer' and 'laptop'\n",
    "vector_computer = nlp('computer').vector\n",
    "vector_laptop = nlp('laptop').vector\n",
    "\n",
    "# Calculate the cosine similarity between the two vectors to find the semantic similarity\n",
    "cosine_similarity = vector_computer.dot(vector_laptop) / (nlp('computer').vector_norm * nlp('laptop').vector_norm)\n",
    "\n",
    "print(f\"The cosine similarity between the words 'computer' and 'laptop' is: {cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ee943",
   "metadata": {},
   "source": [
    "### Advanced Embeddings\n",
    "\n",
    "#### BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "- **Description**: BERT, which stands for Bidirectional Encoder Representations from Transformers, is a transformer-based machine learning technique for natural language processing pre-training. BERT considers the context from both left and right (bidirectional context) of a word during training, thereby understanding the word in its context within the sentence, which is a departure from previous methods that considered words in isolation.\n",
    "\n",
    "<br>\n",
    "\n",
    "  In the below script:\n",
    "  - We first import the necessary classes from the `transformers` library.\n",
    "  - We initialize the BERT tokenizer and model using the `from_pretrained` method to load the pre-trained BERT model.\n",
    "  - We define two sentences that we want to analyze for semantic similarity.\n",
    "  - We tokenize these sentences and pass them through the BERT model to obtain embeddings for each sentence.\n",
    "  - We then calculate the cosine similarity between the embeddings of the two sentences to get a measure of their semantic similarity.\n",
    "  - Finally, we print the calculated cosine similarity score.\n",
    "\n",
    "<br><br>\n",
    "- **Application in Data Engineering**: Data engineers can utilize BERT embeddings in various NLP applications such as sentiment analysis, question-answering systems, and document classification. The embeddings provide a rich representation of the text data, capturing the contextual nuances and semantic relationships between words.\n",
    "\n",
    "- **Output - Cosine Similarity**: The script outputs the cosine similarity between the embeddings of the two sentences. This score is a measure of the semantic similarity between the sentences, with a higher score indicating a higher degree of similarity.\n",
    "\n",
    "- **Usage of the Output**: The cosine similarity score can be used in several applications such as:\n",
    "  - **Semantic Search**: Enhancing search algorithms by finding documents or content that are semantically similar to the query.\n",
    "  - **Content Recommendation**: Developing recommendation systems that suggest content based on semantic similarity to user preferences.\n",
    "  - **Text Analysis**: Performing text analysis where understanding the semantic relationships between pieces of text is vital.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9984dc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the sentences is: 0.8084198236465454\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define two sentences for similarity analysis\n",
    "sentence1 = \"I love programming.\"\n",
    "sentence2 = \"Coding is my passion.\"\n",
    "\n",
    "# Tokenize the sentences and obtain the outputs from the BERT model\n",
    "inputs1 = tokenizer(sentence1, return_tensors='pt')\n",
    "outputs1 = model(**inputs1)\n",
    "\n",
    "inputs2 = tokenizer(sentence2, return_tensors='pt')\n",
    "outputs2 = model(**inputs2)\n",
    "\n",
    "# Calculate the cosine similarity between the sentence embeddings\n",
    "cosine_similarity = F.cosine_similarity(outputs1.last_hidden_state.mean(dim=1), outputs2.last_hidden_state.mean(dim=1))\n",
    "\n",
    "print(f\"The cosine similarity between the sentences is: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b82d9",
   "metadata": {},
   "source": [
    "0.8 does indicate a high level of similarity, and it seems that the model has captured the similar thematic content of both sentences, which revolves around a positive sentiment towards coding/programming. However, the subtlety of the difference in context between \"loving programming\" and having \"a passion for coding\" might not be fully captured in this similarity score.\n",
    "\n",
    "BERT, being a contextual model, forms representations based on the words and their surroundings, which in this case, both sentences express a fondness or affinity for coding/programming, hence the high similarity score. It's essential to note that semantic similarity metrics might not always align perfectly with human interpretation, and different embeddings or methods might give different insights into the semantic relationships between sentences.\n",
    "\n",
    "To make the example more illustrative, we could potentially choose sentences with more distinct semantic content to showcase how BERT can differentiate between them based on the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0bd4f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the sentences is: 0.5463641285896301\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define two sentences for similarity analysis\n",
    "sentence1 = \"I love programming.\"\n",
    "sentence2 = \"The weather is sunny today.\"\n",
    "\n",
    "# Tokenize the sentences and obtain the outputs from the BERT model\n",
    "inputs1 = tokenizer(sentence1, return_tensors='pt')\n",
    "outputs1 = model(**inputs1)\n",
    "\n",
    "inputs2 = tokenizer(sentence2, return_tensors='pt')\n",
    "outputs2 = model(**inputs2)\n",
    "\n",
    "# Calculate the cosine similarity between the sentence embeddings\n",
    "cosine_similarity = F.cosine_similarity(outputs1.last_hidden_state.mean(dim=1), outputs2.last_hidden_state.mean(dim=1))\n",
    "\n",
    "print(f\"The cosine similarity between the sentences is: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc613876",
   "metadata": {},
   "source": [
    "#### ELMo (Embeddings from Language Models)\n",
    "\n",
    "- **Description**: ELMo, which stands for \"Embeddings from Language Models\", is a deep learning model developed to create word embeddings. Unlike traditional word embeddings, ELMo embeddings are contextual, meaning that the embedding for each word depends on the surrounding words in a sentence. This allows ELMo to capture complex semantics and understand word polysemy, where a single word can have multiple meanings based on its context.\n",
    "\n",
    "- **Python Usage and Example**:\n",
    "  ```python\n",
    "  import tensorflow_hub as hub\n",
    "  import tensorflow as tf\n",
    "  from sklearn.metrics.pairwise import cosine_similarity\n",
    "  \n",
    "  # Load the ELMo module from TensorFlow Hub\n",
    "  elmo = hub.load('https://tfhub.dev/google/elmo/3')\n",
    "  \n",
    "  # Define two sentences for similarity analysis\n",
    "  sentence1 = [\"I love programming.\"]\n",
    "  sentence2 = [\"Coding is my passion.\"]\n",
    "  \n",
    "  # Get the ELMo embeddings for the sentences\n",
    "  embeddings1 = elmo.signatures['default'](tf.constant(sentence1))['elmo']\n",
    "  embeddings2 = elmo.signatures['default'](tf.constant(sentence2))['elmo']\n",
    "  \n",
    "  # Calculate the cosine similarity between the sentence embeddings\n",
    "  cosine_sim = cosine_similarity(embeddings1[0].numpy().mean(axis=0).reshape(1, -1), embeddings2[0].numpy().mean(axis=0).reshape(1, -1))\n",
    "  \n",
    "  print(f\"The cosine similarity between the sentences is: {cosine_sim[0][0]}\")\n",
    "  ```\n",
    "  In this script:\n",
    "  - We first import the necessary modules: `tensorflow_hub` for loading the ELMo model, `tensorflow` to work with tensors, and `cosine_similarity` from scikit-learn to calculate the cosine similarity.\n",
    "  - We then load the pre-trained ELMo model from TensorFlow Hub.\n",
    "  - Next, we define two sentences that we wish to analyze for semantic similarity.\n",
    "  - We use the ELMo model to get the embeddings for each sentence. These embeddings are contextual, capturing the semantic nuances based on the context of each word in the sentences.\n",
    "  - We then calculate the cosine similarity between the mean embeddings of the two sentences to gauge their semantic similarity.\n",
    "  - Finally, we print the calculated cosine similarity score, which gives a measure of the semantic similarity between the sentences.\n",
    "\n",
    "- **Application in Data Engineering**: ELMo embeddings can be utilized in various data engineering tasks such as semantic search, sentiment analysis, and text summarization. By understanding the contextual nuances of words, data engineers can build more sophisticated NLP models that provide richer insights into text data.\n",
    "\n",
    "- **Output - Cosine Similarity**: The script outputs the cosine similarity score between the two sentences, indicating the degree of semantic similarity based on the context captured by the ELMo embeddings.\n",
    "\n",
    "- **Usage of the Output**: The cosine similarity score can be used in several applications including:\n",
    "  - **Semantic Search**: Enhancing search algorithms to find content or documents that are semantically similar to a given query.\n",
    "  - **Content Recommendation**: Building recommendation systems that suggest content with similar semantic contexts to users.\n",
    "  - **Text Analysis**: Conducting text analysis where understanding the semantic relationships between texts is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89d56c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the sentences is: 0.5848343968391418\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the ELMo module from TensorFlow Hub\n",
    "elmo = hub.load('https://tfhub.dev/google/elmo/3')\n",
    "\n",
    "# Define two sentences for similarity analysis\n",
    "sentence1 = [\"I love programming.\"]\n",
    "sentence2 = [\"Coding is my passion.\"]\n",
    "\n",
    "# Get the ELMo embeddings for the sentences\n",
    "embeddings1 = elmo.signatures['default'](tf.constant(sentence1))['elmo']\n",
    "embeddings2 = elmo.signatures['default'](tf.constant(sentence2))['elmo']\n",
    "\n",
    "# Calculate the cosine similarity between the sentence embeddings\n",
    "cosine_sim = cosine_similarity(embeddings1[0].numpy().mean(axis=0).reshape(1, -1), embeddings2[0].numpy().mean(axis=0).reshape(1, -1))\n",
    "\n",
    "print(f\"The cosine similarity between the sentences is: {cosine_sim[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ff91d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Embeddings can play a significant role in various data engineering concepts and tasks. Here's a detailed explanation:\n",
    "\n",
    "1. **Data Preprocessing and Feature Engineering**:\n",
    "   - **Text Normalization**: Embeddings can help in the normalization of text data by identifying synonyms and semantically similar words, which can then be unified to a standard term.\n",
    "   - **Dimensionality Reduction**: Embeddings can facilitate dimensionality reduction by representing high-dimensional data (like text) in a lower-dimensional space while preserving essential information.\n",
    "\n",
    "2. **Information Retrieval**:\n",
    "   - **Semantic Search**: Embeddings can enhance search algorithms by enabling the retrieval of information based on semantic similarity rather than exact keyword matches. This makes the search more intuitive and capable of understanding user intent.\n",
    "   - **Document Clustering**: Embeddings can be used to cluster documents based on semantic similarity, facilitating the organization and retrieval of information in large datasets.\n",
    "\n",
    "3. **Content Recommendation**:\n",
    "   - **Personalized Recommendations**: Embeddings can be used in recommendation systems to suggest content (articles, products, etc.) that is semantically similar to the content that the user has shown interest in, thereby personalizing the recommendations.\n",
    "   \n",
    "4. **Natural Language Processing (NLP)**:\n",
    "   - **Sentiment Analysis**: Embeddings can enhance sentiment analysis by capturing the semantic nuances in the text, helping in the accurate classification of sentiments.\n",
    "   - **Named Entity Recognition (NER)**: In NER tasks, embeddings can help in identifying and categorizing entities in the text based on semantic contexts.\n",
    "   \n",
    "5. **Data Integration**:\n",
    "   - **Entity Resolution**: Embeddings can assist in entity resolution tasks by identifying records that refer to the same entity across different data sources based on semantic similarity.\n",
    "   \n",
    "6. **Data Visualization**:\n",
    "   - **Semantic Data Visualization**: Embeddings can be used to visualize data in a semantic space, where the geometric distances between points represent semantic relationships, aiding in the exploration and analysis of complex data.\n",
    "\n",
    "7. **Anomaly Detection**:\n",
    "   - **Outlier Detection**: In anomaly detection tasks, embeddings can help identify outliers by analyzing the semantic relationships between data points and identifying those that deviate significantly from the norm.\n",
    "\n",
    "8. **Data Quality and Consistency**:\n",
    "   - **Data Cleaning**: Embeddings can assist in data cleaning by identifying inconsistencies and errors in the data based on semantic analysis, helping to improve the quality and reliability of the data.\n",
    "\n",
    "9. **Optimizing Data Storage**:\n",
    "   - **Data Compression**: Embeddings can facilitate data compression by representing data in a compact form without significant loss of information, optimizing data storage requirements.\n",
    "\n",
    "10. **Real-Time Analytics**:\n",
    "    - **Stream Analytics**: Embeddings can be used in stream analytics to analyze and interpret data in real-time, enabling applications like real-time sentiment analysis, trend detection, etc.\n",
    "\n",
    "By integrating embeddings into data engineering workflows, data engineers can build more sophisticated data pipelines and systems that can understand and process data in a semantically rich manner, enhancing the insights and value derived from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f05b5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data Preprocessing and Feature Engineering - Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bec64cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Create a Pandas DataFrame with some sample data\n",
    "data = {'Text': ['I love programming', 'I adore coding', 'I enjoy software development']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Using TfidfVectorizer to convert the text data to vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "# Finding synonyms (or semantically similar words) for the word 'love' based on the vector representations\n",
    "cosine_similarities = linear_kernel(tfidf_matrix[0:1], tfidf_matrix).flatten()\n",
    "related_docs_indices = cosine_similarities.argsort()[:-3:-1]\n",
    "related_docs_indices\n",
    "\n",
    "# Output: array([0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74174e4a",
   "metadata": {},
   "source": [
    "In this script, we are using the TF-IDF (Term Frequency-Inverse Document Frequency) technique to find sentences that are semantically similar to the first sentence (\"I love programming\") in the given dataset. Here's a detailed breakdown of the script and the meaning of its output:\n",
    "\n",
    "1. **Importing Necessary Libraries and Creating a DataFrame**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "   from sklearn.metrics.pairwise import linear_kernel\n",
    "   \n",
    "   # Create a Pandas DataFrame with some sample data\n",
    "   data = {'Text': ['I love programming', 'I adore coding', 'I enjoy software development']}\n",
    "   df = pd.DataFrame(data)\n",
    "   ```\n",
    "   In this step, we import the necessary libraries and create a Pandas DataFrame containing some sample sentences.\n",
    "\n",
    "2. **Vectorizing the Text Data**:\n",
    "   ```python\n",
    "   # Using TfidfVectorizer to convert the text data to vectors\n",
    "   vectorizer = TfidfVectorizer()\n",
    "   tfidf_matrix = vectorizer.fit_transform(df['Text'])\n",
    "   ```\n",
    "   We use the `TfidfVectorizer` class from the scikit-learn library to convert the sentences into TF-IDF vectors. TF-IDF is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents (corpus).\n",
    "\n",
    "3. **Calculating Cosine Similarities**:\n",
    "   ```python\n",
    "   # Finding synonyms (or semantically similar words) for the word 'love' based on the vector representation\n",
    "   cosine_similarities = linear_kernel(tfidf_matrix[0:1], tfidf_matrix).flatten()\n",
    "   ```\n",
    "   We calculate the cosine similarities between the TF-IDF vector of the first sentence and the TF-IDF vectors of all sentences in the DataFrame. The `linear_kernel` function is a computationally efficient method to calculate cosine similarities when dealing with TF-IDF vectors.\n",
    "\n",
    "4. **Finding the Most Similar Sentences**:\n",
    "   ```python\n",
    "   related_docs_indices = cosine_similarities.argsort()[:-3:-1]\n",
    "   related_docs_indices\n",
    "   # Output: array([0, 2])\n",
    "   ```\n",
    "   We identify the most similar sentences to the first sentence by sorting the cosine similarity scores in descending order and selecting the top scores (top 2 in this case).\n",
    "\n",
    "**Understanding the Output**:\n",
    "- The output, `array([0, 2])`, indicates that the sentences at indices 0 (\"I love programming\") and 2 (\"I enjoy software development\") are the most similar to the first sentence, based on the TF-IDF vector representations.\n",
    "- The sentence \"I enjoy software development\" is found to be more similar to \"I love programming\" compared to \"I adore coding\", possibly because the words 'enjoy' and 'love' have closer TF-IDF vectors in the context of the given corpus than 'adore' and 'love'.\n",
    "\n",
    "This script illustrates the use of TF-IDF vectors and cosine similarity to analyze semantic similarity between sentences in a data engineering task, providing insights into the semantic relationships in the text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d1cb5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Information Retrieval - Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa935a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document    The sky is blue and beautiful\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a Pandas DataFrame with some sample data\n",
    "data = {'Document': ['The sky is blue and beautiful', 'I love blueberries', 'Blue whales are the largest animals']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Using TfidfVectorizer to convert the document data to vectors\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "doc_vectors = vectorizer.fit_transform(df['Document'])\n",
    "\n",
    "# Semantic search: Finding documents semantically similar to a query\n",
    "query = \"I am feeling blue\"\n",
    "query_vector = vectorizer.transform([query])\n",
    "cosine_sim = cosine_similarity(query_vector, doc_vectors)\n",
    "\n",
    "# Finding the document most similar to the query\n",
    "most_similar_doc = df.iloc[cosine_sim.argmax()]\n",
    "most_similar_doc\n",
    "\n",
    "# Output: Document    The sky is blue and beautiful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d21d2e",
   "metadata": {},
   "source": [
    "In this script, we are performing a semantic search to find the document that is most semantically similar to a given query using TF-IDF (Term Frequency-Inverse Document Frequency) vectors and cosine similarity as the metric. Here's a step-by-step breakdown of the script and an explanation of its output:\n",
    "\n",
    "1. **Importing Necessary Libraries and Creating a DataFrame**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from sklearn.metrics.pairwise import cosine_similarity\n",
    "   from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "   \n",
    "   # Create a Pandas DataFrame with some sample data\n",
    "   data = {'Document': ['The sky is blue and beautiful', 'I love blueberries', 'Blue whales are the largest animals']}\n",
    "   df = pd.DataFrame(data)\n",
    "   ```\n",
    "   Initially, we import the required libraries and create a Pandas DataFrame that contains a set of documents.\n",
    "\n",
    "2. **Vectorizing the Document Data**:\n",
    "   ```python\n",
    "   # Using TfidfVectorizer to convert the document data to vectors\n",
    "   vectorizer = TfidfVectorizer(stop_words='english')\n",
    "   doc_vectors = vectorizer.fit_transform(df['Document'])\n",
    "   ```\n",
    "   We then use `TfidfVectorizer` to convert the documents into TF-IDF vectors. The `stop_words='english'` parameter tells the vectorizer to ignore common English stop words (like 'is', 'and', etc.) which generally do not carry much semantic meaning.\n",
    "\n",
    "3. **Performing Semantic Search**:\n",
    "   ```python\n",
    "   # Semantic search: Finding documents semantically similar to a query\n",
    "   query = \"I am feeling blue\"\n",
    "   query_vector = vectorizer.transform([query])\n",
    "   cosine_sim = cosine_similarity(query_vector, doc_vectors)\n",
    "   ```\n",
    "   We define a query and convert it to a TF-IDF vector using the same `vectorizer` object. We then calculate the cosine similarity between the query vector and the vectors of all documents in the DataFrame to find how semantically similar the query is to each document.\n",
    "\n",
    "4. **Finding the Most Similar Document**:\n",
    "   ```python\n",
    "   # Finding the document most similar to the query\n",
    "   most_similar_doc = df.iloc[cosine_sim.argmax()]\n",
    "   most_similar_doc\n",
    "   # Output: Document    The sky is blue and beautiful\n",
    "   ```\n",
    "   Based on the cosine similarity scores, we identify the document that is most similar to the query. The `argmax` function is used to find the index of the document with the highest cosine similarity score to the query.\n",
    "\n",
    "**Understanding the Output**:\n",
    "- The output, `Document    The sky is blue and beautiful`, indicates that among the documents in the DataFrame, the document \"The sky is blue and beautiful\" is the most semantically similar to the query \"I am feeling blue\" based on the TF-IDF vector representations.\n",
    "- This suggests that the TF-IDF vector representation of the query shares the highest cosine similarity with the vector representation of the document \"The sky is blue and beautiful\", possibly because of the presence of the word \"blue\" and the semantic context surrounding it in both the query and the document.\n",
    "\n",
    "This script demonstrates how to use TF-IDF vectors and cosine similarity to perform semantic search in a collection of documents, providing a method to find documents that are semantically related to a given query in data engineering tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02774839",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Content Recommendation - Personalized Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c02cc162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Preferences</th>\n",
       "      <th>Content Description</th>\n",
       "      <th>Recommended Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love programming and coding</td>\n",
       "      <td>A guide to programming in Python</td>\n",
       "      <td>A guide to programming in Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I enjoy outdoor activities</td>\n",
       "      <td>Top 10 hiking trails</td>\n",
       "      <td>Top 10 hiking trails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I like reading books</td>\n",
       "      <td>Bestselling fiction books of 2022</td>\n",
       "      <td>Bestselling fiction books of 2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                User Preferences                Content Description  \\\n",
       "0  I love programming and coding   A guide to programming in Python   \n",
       "1     I enjoy outdoor activities               Top 10 hiking trails   \n",
       "2           I like reading books  Bestselling fiction books of 2022   \n",
       "\n",
       "                 Recommended Content  \n",
       "0   A guide to programming in Python  \n",
       "1               Top 10 hiking trails  \n",
       "2  Bestselling fiction books of 2022  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a Pandas DataFrame with some sample data (user preferences and content descriptions)\n",
    "data = {'User Preferences': ['I love programming and coding', 'I enjoy outdoor activities', 'I like reading books'],\n",
    "        'Content Description': ['A guide to programming in Python', 'Top 10 hiking trails', 'Bestselling fiction books of 2022']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Load the Universal Sentence Encoder model\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Create embeddings for content descriptions and user preferences using USE\n",
    "content_embeddings = use_model(df['Content Description'])\n",
    "preference_embeddings = use_model(df['User Preferences'])\n",
    "\n",
    "# Calculate cosine similarity between user preferences and content descriptions\n",
    "cosine_sim = cosine_similarity(preference_embeddings, content_embeddings)\n",
    "\n",
    "# Find the content most similar to each user's preferences\n",
    "recommendations = cosine_sim.argmax(axis=1)\n",
    "df['Recommended Content'] = df['Content Description'][recommendations].values\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad8f3ca",
   "metadata": {},
   "source": [
    "In this modified script, we have improved the content recommendation system by using the Universal Sentence Encoder (USE) to create embeddings for the user preferences and content descriptions. The USE is proficient at capturing deeper semantic relationships between sentences, which enhances the recommendation quality. Here's a step-by-step explanation of the script and the interpretation of its output:\n",
    "\n",
    "1. **Importing Necessary Libraries and Creating a DataFrame**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   import tensorflow_hub as hub\n",
    "   from sklearn.metrics.pairwise import cosine_similarity\n",
    "   \n",
    "   # Create a Pandas DataFrame with some sample data (user preferences and content descriptions)\n",
    "   data = {'User Preferences': ['I love programming and coding', 'I enjoy outdoor activities', 'I like reading books'],\n",
    "           'Content Description': ['A guide to programming in Python', 'Top 10 hiking trails', 'Bestselling fiction books of 2022']}\n",
    "   df = pd.DataFrame(data)\n",
    "   ```\n",
    "   Initially, we import the necessary libraries and create a Pandas DataFrame containing user preferences and content descriptions.\n",
    "\n",
    "2. **Loading the Universal Sentence Encoder and Creating Embeddings**:\n",
    "   ```python\n",
    "   # Load the Universal Sentence Encoder model\n",
    "   use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "   \n",
    "   # Create embeddings for content descriptions and user preferences using USE\n",
    "   content_embeddings = use_model(df['Content Description'])\n",
    "   preference_embeddings = use_model(df['User Preferences'])\n",
    "   ```\n",
    "   We load the Universal Sentence Encoder (USE) model from TensorFlow Hub and use it to create embeddings for the content descriptions and user preferences. The USE creates embeddings that capture the semantic nuances of the sentences, providing a rich representation of the content and preferences.\n",
    "\n",
    "3. **Calculating Cosine Similarity and Finding Recommendations**:\n",
    "   ```python\n",
    "   # Calculate cosine similarity between user preferences and content descriptions\n",
    "   cosine_sim = cosine_similarity(preference_embeddings, content_embeddings)\n",
    "   \n",
    "   # Find the content most similar to each user's preferences\n",
    "   recommendations = cosine_sim.argmax(axis=1)\n",
    "   df['Recommended Content'] = df['Content Description'][recommendations].values\n",
    "   df\n",
    "   ```\n",
    "   We then calculate the cosine similarity between the embeddings of user preferences and content descriptions to find the semantic similarity between them. Based on these similarity scores, we identify the most similar content for each user by finding the content with the highest cosine similarity score for each user's preferences.\n",
    "\n",
    "**Understanding the Output**:\n",
    "- The output is a DataFrame that displays the recommended content for each user based on their preferences. The recommendations are determined based on the semantic similarity between the user preferences and content descriptions, as captured by the USE embeddings.\n",
    "- The Universal Sentence Encoder is able to understand the deeper semantic relationships between sentences, which helps in providing more accurate and contextually relevant recommendations compared to methods based on simple word frequency counts.\n",
    "\n",
    "This modified script demonstrates an enhanced approach to building a content recommendation system by utilizing the Universal Sentence Encoder to understand the semantic relationships between user preferences and content descriptions, thereby offering more personalized and relevant content recommendations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4b076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
