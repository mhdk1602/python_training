{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03528d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers -U > /dev/null\n",
    "!pip install elasticsearch > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c14c13",
   "metadata": {},
   "source": [
    "### Elasticsearch - Enhancing the Search Interface\n",
    "\n",
    "**Elasticsearch** is a highly scalable open-source full-text search and analytics engine. It allows you to store, search, and analyze big volumes of data quickly and in near real time. Elasticsearch is generally used as the underlying engine/technology that powers applications that have complex search features and requirements.\n",
    "\n",
    "Here are the ways integrating Elasticsearch can significantly enhance the search interface:\n",
    "\n",
    "1. **Full-Text Search**: Elasticsearch is designed to help you find the most relevant information quickly by performing advanced full-text search operations on large datasets. It can analyze the text contents and find the best matches based on various criteria such as term frequency, proximity, and so forth.\n",
    "\n",
    "2. **Scalability and Speed**: Elasticsearch is distributed, which means that indices can be divided into shards and each shard can have zero or more replicas. This distribution of data facilitates search operations to be scaled horizontally, improving the speed and efficiency of searches, especially when dealing with large volumes of data.\n",
    "\n",
    "3. **Complex Query Language**: Elasticsearch supports a rich and flexible query language (Query DSL) that allows for the formulation of complex queries to find exactly what you need.\n",
    "\n",
    "4. **Relevance Scoring and Ranking**: Elasticsearch calculates the relevance score for each document in response to a query, allowing for the ranking of results based on their relevance, which can help in providing more accurate answers to user queries.\n",
    "\n",
    "5. **Analysis and Tokenization**: Elasticsearch can analyze and tokenize text data in various ways, making it possible to handle linguistic nuances such as stemming, synonyms, etc., which can improve the accuracy of the search.\n",
    "\n",
    "6. **Integration with Embeddings**: In our setup, Elasticsearch will be used not only to search the text data but also to search the embeddings created from the text data. This integration allows for semantically intelligent searches, where we can find sentences that are semantically similar to the query, not just textually similar.\n",
    "\n",
    "7. **Aggregations for Analytics**: Elasticsearch provides powerful aggregations that can help you summarize and analyze your data, which can be used to build complex analytics and visualization interfaces.\n",
    "\n",
    "8. **Real-Time Operations**: Elasticsearch performs data indexing and searching in near real-time, which means that the latency between indexing a document and making it searchable is very short, providing a real-time search experience.\n",
    "\n",
    "In the context of our project, integrating Elasticsearch will allow us to build a more powerful and flexible search interface where we can perform semantically intelligent searches on the embeddings created from the text data in the PDF files. The search results can be ranked based on various criteria, including textual and semantic similarity, to provide more relevant and accurate answers to user queries.\n",
    "\n",
    "In the following sections, we will explore how to integrate Elasticsearch into our setup and use it to enhance the search functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae12e0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading PDFs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.51file/s]\n",
      "Creating Embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12706/12706 [10:14<00:00, 20.69sentence/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from minio import Minio\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Step 1: Extract text from PDF files and tokenize into sentences\n",
    "directory = 'gitignore-files'\n",
    "pdf_text_data = {}\n",
    "\n",
    "for filename in tqdm(os.listdir(directory), desc=\"Reading PDFs\", unit=\"file\"):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Open the PDF file\n",
    "        doc = fitz.open(file_path)\n",
    "        text = \"\"\n",
    "        \n",
    "        # Combine text from all pages into a single string\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        pdf_text_data[filename] = sent_tokenize(text)  # Tokenize text into sentences\n",
    "\n",
    "# Step 2: Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Step 3: Create sentence-level embeddings for the text from each PDF file\n",
    "embeddings_data = {}\n",
    "\n",
    "# Calculate the total number of sentences across all files\n",
    "total_sentences = sum(len(sentences) for sentences in pdf_text_data.values())\n",
    "\n",
    "# Create a single tqdm progress bar to track progress across all sentences\n",
    "progress_bar = tqdm(total=total_sentences, desc=\"Creating Embeddings\", unit=\"sentence\")\n",
    "\n",
    "for filename, sentences in pdf_text_data.items():\n",
    "    embeddings_data[filename] = []\n",
    "    for sentence in sentences:\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # Skip empty sentences\n",
    "        if not sentence.strip():\n",
    "            continue\n",
    "        \n",
    "        # Tokenize the sentence and create an embedding\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', max_length=512, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings_data[filename].append(outputs.last_hidden_state.mean(dim=1).numpy())\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6af666",
   "metadata": {},
   "source": [
    "## Integrating Elasticsearch with Our Embeddings Application\n",
    "\n",
    "### Step 1: Setting Up an Elasticsearch Account\n",
    "\n",
    "1. **Create an Account on Elasticsearch**: Visit the official [Elasticsearch website](https://www.elastic.co/) and sign up for an account.\n",
    "2. **Start a Deployment**: Once your account is set up, initiate a new deployment from the Elasticsearch console. This setup will provide you with the necessary credentials including the Elasticsearch endpoint, Cloud ID, and an API key. \n",
    "3. **Environment Variables**: For security reasons, store your credentials as environment variables. This way, they can be easily and securely accessed in your Python script. You can use the `os.getenv` method to retrieve these values in your script. Below are the environment variable keys you should set:\n",
    "   - `YOUR_ELASTICSEARCH_ENDPOINT`\n",
    "   - `YOUR_ELASTICSEARCH_CLOUD_ID`\n",
    "   - `YOUR_ELASTICSEARCH_API_KEY`\n",
    "\n",
    "### Step 2: Connecting to Elasticsearch in Python\n",
    "\n",
    "1. **Installing the Elasticsearch Client**: To interact with your Elasticsearch deployment in Python, you need to install the official Elasticsearch client. You can add it to your Python environment using pip.\n",
    "2. **Initializing the Client**: Use the Elasticsearch Python client to establish a connection to your Elasticsearch instance. Utilize the environment variables set earlier to securely use your credentials in the script.\n",
    "\n",
    "### Step 3: Creating an Index with Appropriate Mappings\n",
    "\n",
    "Before you can index your sentence embeddings, you need to create an index with the appropriate mappings. This step involves specifying the structure of your documents, which includes setting the data types of your fields (such as keyword, text, and dense_vector).\n",
    "\n",
    "### Step 4: Indexing Sentence Embeddings\n",
    "\n",
    "Once your index is ready, proceed to index your sentence embeddings. This process involves iterating over your sentences and their corresponding embeddings and adding them to the index one by one.\n",
    "\n",
    "### Step 5: Querying the Index\n",
    "\n",
    "After indexing your data, create a function to query the index. This function should be able to:\n",
    "1. Accept a user query and create an embedding for it.\n",
    "2. Use this embedding to query the Elasticsearch index and retrieve the most similar sentences.\n",
    "3. Display the retrieved sentences along with their similarity scores and the documents they belong to.\n",
    "\n",
    "### Step 6: Testing the Setup\n",
    "\n",
    "Finally, test your setup by entering various queries and observing the results. Ensure the system is able to find and return the most relevant sentences from your indexed documents.\n",
    "\n",
    "Remember to handle the 'quit' command to allow users to exit the query loop gracefully.\n",
    "\n",
    "This setup will allow you to perform semantic searches on your indexed documents, retrieving the most relevant sentences based on the embeddings created from your PDF files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a6c236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/1r4fv_1j0dvb38w891pxz3qw0000gn/T/ipykernel_26652/1806073483.py:12: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.indices.create(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'pdf_embeddings'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Elasticsearch client\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[{\"host\": os.getenv(\"YOUR_ELASTICSEARCH_ENDPOINT\"), \"port\": 443, \"scheme\": \"https\"}],\n",
    "    headers={\"Authorization\": f\"ApiKey {os.getenv('YOUR_ELASTICSEARCH_API_KEY')}\"}\n",
    ")\n",
    "# Create an index with mappings to define the structure of your documents\n",
    "es.indices.create(\n",
    "    index=\"pdf_embeddings\",\n",
    "    body={\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"filename\": {\"type\": \"keyword\"},\n",
    "                \"sentence\": {\"type\": \"text\"},\n",
    "                \"embedding\": {\"type\": \"dense_vector\", \"dims\": 768}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    ignore=400  # Ignore \"Index Already Exist\" error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6a8dad0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing data:   0%|                                                                                                                                                                                                 | 0/12706 [00:53<?, ?embedding/s]\n",
      "Indexing data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12706/12706 [08:53<00:00, 23.80embedding/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Index data (sentences and embeddings) into Elasticsearch\n",
    "\n",
    "# Calculate the total number of embeddings to index\n",
    "total_embeddings = sum(len(embed_list) for embed_list in embeddings_data.values())\n",
    "\n",
    "# Create a single tqdm progress bar to track progress across all embeddings\n",
    "progress_bar = tqdm(total=total_embeddings, desc=\"Indexing data\", unit=\"embedding\")\n",
    "\n",
    "for filename, embeddings in embeddings_data.items():\n",
    "    sentences = pdf_text_data[filename]\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        # Skip empty embeddings (for empty sentences)\n",
    "        if embedding.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Index each sentence and its embedding into Elasticsearch\n",
    "        es.index(index=\"pdf_embeddings\", body={\n",
    "            \"filename\": filename,\n",
    "            \"sentence\": sentences[i],\n",
    "            \"embedding\": embedding.flatten().tolist()  # Flatten the numpy array before converting to list\n",
    "        })\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24666c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query (or type 'quit' to exit): does ahab kill the whale?\n",
      "Match: how the richer or better is Ahab now? from file moby-dick.pdf (score: 1.8019496)\n",
      "Match: Why did the old Persians hold the sea holy? from file moby-dick.pdf (score: 1.7933602)\n",
      "Match: Is Ahab, Ahab? from file moby-dick.pdf (score: 1.7905133)\n",
      "Match: But what is this lesson that the book of Jonah teaches? from file moby-dick.pdf (score: 1.7855389)\n",
      "Match: Doesn’t the devil live for ever;\n",
      "who ever heard that the devil was dead? from file moby-dick.pdf (score: 1.7788833)\n",
      "Please enter your query (or type 'quit' to exit): quit\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search():\n",
    "    while True:\n",
    "        user_query = input(\"Please enter your query (or type 'quit' to exit): \")\n",
    "        \n",
    "        if user_query.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Step 1: Create an embedding for the user's query\n",
    "        inputs = tokenizer(user_query, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        query_embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "        \n",
    "        # Step 2: Create a script query to calculate the cosine similarity between the query embedding and stored embeddings\n",
    "        script_query = {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                    \"params\": {\"query_vector\": query_embedding[0].tolist()}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Step 3: Execute the query and retrieve the top 5 most similar sentences\n",
    "        response = es.search(\n",
    "            index=\"pdf_embeddings\",\n",
    "            body={\n",
    "                \"size\": 5,\n",
    "                \"query\": script_query,\n",
    "                \"_source\": {\"includes\": [\"filename\", \"sentence\"]}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Print the top 5 most similar sentences\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            print(f\"Match: {hit['_source']['sentence']} from file {hit['_source']['filename']} (score: {hit['_score']})\")\n",
    "\n",
    "search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3edd4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In our ongoing project, we have made significant advancements from the previous iteration where we used `Minio` to the current iteration involving the integration of `Elasticsearch`. \n",
    "\n",
    "Below is a detailed comparison illustrating the improvements achieved:\n",
    "\n",
    "\n",
    "| Aspect                   | Previous Iteration (Using MinIO) | Current Iteration (Using Elasticsearch) |\n",
    "|--------------------------|----------------------------------|-----------------------------------------|\n",
    "| **Storage System**       | Utilized MinIO, an object storage service, for storing the embeddings. | Transitioned to using Elasticsearch, a powerful search and analytics engine, for storing and indexing the embeddings. |\n",
    "| **Indexing Strategy**    | Serialized embeddings were stored in MinIO, with each sentence embedding stored as a separate object. | Embeddings are indexed directly into Elasticsearch, facilitating more efficient data retrieval and search capabilities. |\n",
    "| **Search Capability**    | Employed a basic search strategy where embeddings were retrieved from MinIO and cosine similarity calculations were performed in Python. | Utilizes Elasticsearch's advanced search capabilities, where cosine similarity computations are integrated directly into the search queries, potentially offering more accurate and faster results. |\n",
    "| **Scalability**          | Although MinIO can handle large data storage, the search and retrieval process might face challenges in scalability due to the computational intensity of similarity computations in Python. | Being a distributed system, Elasticsearch can scale horizontally, enhancing both the speed and scalability of the search process, especially with large datasets. |\n",
    "| **Data Retrieval Speed** | The speed of data retrieval and similarity computation might be slower, particularly as the dataset grows, due to the Python-based computation process. | Expected to offer faster data retrieval speeds due to the in-built search and analytics capabilities of Elasticsearch. |\n",
    "| **Data Visualization**   | Did not natively support data visualization; would require integration with other tools for data analysis and visualization. | Can be coupled with Kibana for intuitive data visualization and analysis, paving the way for more advanced data analytics in future developments. |\n",
    "\n",
    "In summary, the current iteration with Elasticsearch integration promises a more robust and scalable solution, capable of handling larger datasets more efficiently and offering sophisticated querying capabilities. It marks a substantial step forward in developing a feature-rich application compared to the previous MinIO-based approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133b7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
