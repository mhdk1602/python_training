{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd52bbe6-c136-48ca-9b49-8867af6a985c",
   "metadata": {},
   "source": [
    "# Introduction to Data Quality and Validation Frameworks \n",
    "\n",
    "Data quality is a critical aspect of data engineering and analytics. Ensuring data is accurate, consistent, and timely helps in making informed decisions based on reliable insights. Data validation frameworks come into play to facilitate the implementation of data quality checks throughout data pipelines. They assist in defining, monitoring, and validating data quality rules, thereby ensuring the reliability and trustworthiness of the data.\n",
    "\n",
    "## dbt (data build tool) in Data Quality\n",
    "\n",
    "[dbt (data build tool)](https://www.getdbt.com/) is a popular open-source software tool that enables data analysts and engineers to transform and test data in the data warehouse. dbt allows for defining, documenting, and executing data transformation workflows, making it a powerful tool for data pipeline orchestration. Here's how dbt stands as a vital tool in ensuring data quality:\n",
    "\n",
    "### 1. Data Transformation\n",
    "\n",
    "- **SQL-Based Transformations**: dbt leverages the power of SQL for data transformation, allowing for the creation of complex data models with ease.\n",
    "- **Version Control**: dbt supports version control of data models, enabling tracking of changes and facilitating collaboration among team members.\n",
    "\n",
    "### 2. Data Testing\n",
    "\n",
    "- **Built-in Data Tests**: dbt offers a range of built-in tests that can be easily implemented to check data quality, such as testing for uniqueness, not_null, and referential integrity.\n",
    "- **Custom Data Tests**: Apart from built-in tests, dbt allows for the creation of custom data tests, enabling the definition of business-specific data quality rules.\n",
    "\n",
    "### 3. Documentation and Data Lineage\n",
    "\n",
    "- **Automatic Documentation**: dbt automatically generates documentation for the data models, providing a clear view of the data structure and transformations.\n",
    "- **Data Lineage**: dbt supports the visualization of data lineage, helping in understanding the flow of data and dependencies between different data models.\n",
    "\n",
    "### 4. Integration with Data Pipelines\n",
    "\n",
    "- **Automation and Scheduling**: dbt can be integrated into data pipelines for automated execution of data transformations and tests, ensuring data quality checks are performed in each run.\n",
    "- **Compatibility with Various Data Warehouses**: dbt supports various data warehouses, making it a flexible choice for different data environments.\n",
    "\n",
    "In the subsequent sections, we will demonstrate how to set up a mock data pipeline, ingest data into a PostgreSQL database, and use dbt to implement data quality checks and validations, showcasing the best practices for incorporating data quality checks in data pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f28541c-65bf-4b5b-b591-00ac17a0d19b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2024-01-01 00:57:55   2024-01-01 01:17:43              1.0   \n",
      "1         1  2024-01-01 00:03:00   2024-01-01 00:09:36              1.0   \n",
      "2         1  2024-01-01 00:17:06   2024-01-01 00:35:01              1.0   \n",
      "3         1  2024-01-01 00:36:38   2024-01-01 00:44:56              1.0   \n",
      "4         1  2024-01-01 00:46:51   2024-01-01 00:52:57              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           1.72         1.0                  N           186            79   \n",
      "1           1.80         1.0                  N           140           236   \n",
      "2           4.70         1.0                  N           236            79   \n",
      "3           1.40         1.0                  N            79           211   \n",
      "4           0.80         1.0                  N           211           148   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             2         17.7    1.0      0.5        0.00           0.0   \n",
      "1             1         10.0    3.5      0.5        3.75           0.0   \n",
      "2             1         23.3    3.5      0.5        3.00           0.0   \n",
      "3             1         10.0    3.5      0.5        2.00           0.0   \n",
      "4             1          7.9    3.5      0.5        3.20           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
      "0                    1.0         22.70                   2.5          0.0  \n",
      "1                    1.0         18.75                   2.5          0.0  \n",
      "2                    1.0         31.30                   2.5          0.0  \n",
      "3                    1.0         17.00                   2.5          0.0  \n",
      "4                    1.0         16.10                   2.5          0.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2964624 entries, 0 to 2964623\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int32         \n",
      " 1   tpep_pickup_datetime   datetime64[us]\n",
      " 2   tpep_dropoff_datetime  datetime64[us]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             float64       \n",
      " 6   store_and_fwd_flag     object        \n",
      " 7   PULocationID           int32         \n",
      " 8   DOLocationID           int32         \n",
      " 9   payment_type           int64         \n",
      " 10  fare_amount            float64       \n",
      " 11  extra                  float64       \n",
      " 12  mta_tax                float64       \n",
      " 13  tip_amount             float64       \n",
      " 14  tolls_amount           float64       \n",
      " 15  improvement_surcharge  float64       \n",
      " 16  total_amount           float64       \n",
      " 17  congestion_surcharge   float64       \n",
      " 18  Airport_fee            float64       \n",
      "dtypes: datetime64[us](2), float64(12), int32(3), int64(1), object(1)\n",
      "memory usage: 395.8+ MB\n",
      "None\n",
      "           VendorID        tpep_pickup_datetime       tpep_dropoff_datetime  \\\n",
      "count  2.964624e+06                     2964624                     2964624   \n",
      "mean   1.754204e+00  2024-01-17 00:46:36.431092  2024-01-17 01:02:13.208130   \n",
      "min    1.000000e+00         2002-12-31 22:59:39         2002-12-31 23:05:41   \n",
      "25%    2.000000e+00  2024-01-09 15:59:19.750000         2024-01-09 16:16:23   \n",
      "50%    2.000000e+00  2024-01-17 10:45:37.500000  2024-01-17 11:03:51.500000   \n",
      "75%    2.000000e+00  2024-01-24 18:23:52.250000         2024-01-24 18:40:29   \n",
      "max    6.000000e+00         2024-02-01 00:01:15         2024-02-02 13:56:52   \n",
      "std    4.325902e-01                         NaN                         NaN   \n",
      "\n",
      "       passenger_count  trip_distance    RatecodeID  PULocationID  \\\n",
      "count     2.824462e+06   2.964624e+06  2.824462e+06  2.964624e+06   \n",
      "mean      1.339281e+00   3.652169e+00  2.069359e+00  1.660179e+02   \n",
      "min       0.000000e+00   0.000000e+00  1.000000e+00  1.000000e+00   \n",
      "25%       1.000000e+00   1.000000e+00  1.000000e+00  1.320000e+02   \n",
      "50%       1.000000e+00   1.680000e+00  1.000000e+00  1.620000e+02   \n",
      "75%       1.000000e+00   3.110000e+00  1.000000e+00  2.340000e+02   \n",
      "max       9.000000e+00   3.127223e+05  9.900000e+01  2.650000e+02   \n",
      "std       8.502817e-01   2.254626e+02  9.823219e+00  6.362391e+01   \n",
      "\n",
      "       DOLocationID  payment_type   fare_amount         extra       mta_tax  \\\n",
      "count  2.964624e+06  2.964624e+06  2.964624e+06  2.964624e+06  2.964624e+06   \n",
      "mean   1.651167e+02  1.161271e+00  1.817506e+01  1.451598e+00  4.833823e-01   \n",
      "min    1.000000e+00  0.000000e+00 -8.990000e+02 -7.500000e+00 -5.000000e-01   \n",
      "25%    1.140000e+02  1.000000e+00  8.600000e+00  0.000000e+00  5.000000e-01   \n",
      "50%    1.620000e+02  1.000000e+00  1.280000e+01  1.000000e+00  5.000000e-01   \n",
      "75%    2.340000e+02  1.000000e+00  2.050000e+01  2.500000e+00  5.000000e-01   \n",
      "max    2.650000e+02  4.000000e+00  5.000000e+03  1.425000e+01  4.000000e+00   \n",
      "std    6.931535e+01  5.808686e-01  1.894955e+01  1.804102e+00  1.177600e-01   \n",
      "\n",
      "         tip_amount  tolls_amount  improvement_surcharge  total_amount  \\\n",
      "count  2.964624e+06  2.964624e+06           2.964624e+06  2.964624e+06   \n",
      "mean   3.335870e+00  5.270212e-01           9.756319e-01  2.680150e+01   \n",
      "min   -8.000000e+01 -8.000000e+01          -1.000000e+00 -9.000000e+02   \n",
      "25%    1.000000e+00  0.000000e+00           1.000000e+00  1.538000e+01   \n",
      "50%    2.700000e+00  0.000000e+00           1.000000e+00  2.010000e+01   \n",
      "75%    4.120000e+00  0.000000e+00           1.000000e+00  2.856000e+01   \n",
      "max    4.280000e+02  1.159200e+02           1.000000e+00  5.000000e+03   \n",
      "std    3.896551e+00  2.128310e+00           2.183645e-01  2.338558e+01   \n",
      "\n",
      "       congestion_surcharge   Airport_fee  \n",
      "count          2.824462e+06  2.824462e+06  \n",
      "mean           2.256122e+00  1.411611e-01  \n",
      "min           -2.500000e+00 -1.750000e+00  \n",
      "25%            2.500000e+00  0.000000e+00  \n",
      "50%            2.500000e+00  0.000000e+00  \n",
      "75%            2.500000e+00  0.000000e+00  \n",
      "max            2.500000e+00  1.750000e+00  \n",
      "std            8.232747e-01  4.876239e-01  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/Users/malempatiharidines/Code/GitHub/training/python_training/datasets/yellow_tripdata_2024-01.parquet\"\n",
    "data = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(data.head())\n",
    "\n",
    "# Get basic information about the dataset\n",
    "print(data.info())\n",
    "\n",
    "# Get descriptive statistics for the numerical columns\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ad0fa-9c97-4393-9fc4-3aa54a70b527",
   "metadata": {},
   "source": [
    "### Reading and Profiling the Data\n",
    "\n",
    "Before we embark on setting up data transformations and quality checks with dbt, it's imperative to understand the dataset we are working with. In this section, we read the NYC Taxi Trip dataset for January 2024 and performed a cursory data profiling to get acquainted with the data structure and contents.\n",
    "\n",
    "#### Step 1: Reading the Data\n",
    "\n",
    "Using pandas, a popular data manipulation library in Python, we read the dataset from the parquet file. Here's the snippet of Python code we used to read the data:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"file_path/yellow_tripdata_2024-01.parquet\"\n",
    "data = pd.read_parquet(file_path)\n",
    "```\n",
    "\n",
    "#### Step 2: Basic Data Profiling\n",
    "\n",
    "After loading the data, we performed some basic profiling to understand the structure and contents of the dataset. We used the following commands to explore the data:\n",
    "\n",
    "```python\n",
    "# Display the first few rows of the dataframe\n",
    "print(data.head())\n",
    "\n",
    "# Get basic information about the dataset\n",
    "print(data.info())\n",
    "\n",
    "# Get descriptive statistics for the numerical columns\n",
    "print(data.describe())\n",
    "```\n",
    "\n",
    "#### Data Overview\n",
    "\n",
    "From the data profiling, we observed that the dataset contains 2,964,624 records and 19 columns, including details about the trip (pickup and dropoff times, locations), fare breakdown (amount, tips, tolls), and other attributes like payment type and rate code. \n",
    "\n",
    "Understanding the data's structure and contents will guide us in setting up appropriate data transformations and quality checks in the subsequent steps, where we will be using dbt to implement data quality checks and validations.\n",
    "\n",
    "In the next section, we will proceed to set up dbt and create transformation models to clean and structure the data, preparing it for data quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c412d9-3cf4-4e24-88fa-468fe1bc4cb2",
   "metadata": {},
   "source": [
    "### Setting Up dbt (data build tool)\n",
    "\n",
    "In this section, we will focus on setting up dbt, a popular tool for data transformations and testing in the data warehouse. dbt allows us to define, document, and execute data transformation workflows, making it a powerful tool for setting up data quality checks.\n",
    "\n",
    "#### Step 1: Installing dbt\n",
    "\n",
    "Before we can start using dbt, it needs to be installed in your Python environment. You can install dbt using the following command:\n",
    "\n",
    "```shell\n",
    "pip install dbt\n",
    "```\n",
    "\n",
    "This command will install dbt along with its dependencies, preparing your environment for setting up a dbt project.\n",
    "\n",
    "#### Step 2: Initializing a dbt Project\n",
    "\n",
    "Once dbt is installed, the next step is to initialize a dbt project. Navigate to the directory where you want to create your dbt project and run the following command:\n",
    "\n",
    "```shell\n",
    "dbt init my_dbt_project\n",
    "```\n",
    "\n",
    "This command will create a new dbt project with the necessary directory structure and configuration files to get started with dbt.\n",
    "\n",
    "#### Step 3: Configuring the dbt Profile\n",
    "\n",
    "To connect dbt to your PostgreSQL database, you need to configure the dbt profile. The profile configuration file is located at `~/.dbt/profiles.yml`. In this file, you'll set up the connection details for your PostgreSQL database. Here's an example configuration:\n",
    "\n",
    "```yaml\n",
    "my_dbt_project:\n",
    "  target: dev\n",
    "  outputs:\n",
    "    dev:\n",
    "      type: postgres\n",
    "      host: localhost\n",
    "      user: your_username\n",
    "      pass: your_password\n",
    "      port: 5432\n",
    "      dbname: your_database_name\n",
    "      schema: your_schema_name\n",
    "```\n",
    "\n",
    "Replace `your_username`, `your_password`, `your_database_name`, and `your_schema_name` with the appropriate details for your PostgreSQL database.\n",
    "\n",
    "Once the dbt project is set up and configured, we are ready to start creating dbt models for data transformation and setting up data quality checks.\n",
    "\n",
    "In the next section, we will create dbt models to transform the raw data and set up data quality tests using dbt's testing functionalities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16360a86-f295-4f2e-be51-0f21e3359a0b",
   "metadata": {},
   "source": [
    "### Understanding dbt Components\n",
    "\n",
    "Before we delve into setting up dbt models and data ingestion, let's understand the core components of dbt:\n",
    "\n",
    "1. **Models**: Models are the central artifacts in dbt. They are essentially SQL queries defined in `.sql` files which represent the transformations that need to be applied to your source data. dbt builds models by executing these SQL queries in the specified materialization format (tables, views, etc.).\n",
    "\n",
    "2. **Macros**: Macros are reusable pieces of SQL code that can be utilized across different models. They help in avoiding code repetition and can be used to encapsulate logic that can be reused in various models.\n",
    "\n",
    "3. **Seeds**: Seeds are csv files that store static data, which can be used in transformations or to augment the source data. They are useful for storing data like mapping tables, which do not change over time.\n",
    "\n",
    "4. **Sources**: Sources in dbt are a way of documenting and testing the raw data in your warehouse. They help in defining the schema of your raw data and can be used to create tests to validate the quality of the source data.\n",
    "\n",
    "5. **Tests**: Tests in dbt are SQL queries that help in validating the data quality. You can create tests to check for things like uniqueness, not null constraints, etc., in your transformed data.\n",
    "\n",
    "6. **Snapshots**: Snapshots are used to capture historical changes in your data. They help in tracking how data changes over time by creating a snapshot of the data at regular intervals.\n",
    "\n",
    "Now, let's move on to the steps we undertook for setting up dbt models and data ingestion.\n",
    "\n",
    "### Setting Up dbt Models and Data Ingestion\n",
    "\n",
    "#### Step 1: Setting Up the dbt Model\n",
    "\n",
    "1. We first created a dbt model to define the structure of the PostgreSQL table where the data will be loaded. The model file, named `nyc_taxi_data.sql`, contains a SQL query that creates an empty table with the desired schema to hold the NYC taxi data.\n",
    "\n",
    "\n",
    "    ```sql\n",
    "        {{ config(materialized='table') }}\n",
    "        \n",
    "        SELECT \n",
    "            NULL::INT AS VendorID,\n",
    "            NULL::TIMESTAMP AS tpep_pickup_datetime,\n",
    "            NULL::TIMESTAMP AS tpep_dropoff_datetime,\n",
    "            NULL::FLOAT AS passenger_count,\n",
    "            NULL::FLOAT AS trip_distance,\n",
    "            NULL::FLOAT AS RatecodeID,\n",
    "            NULL::VARCHAR AS store_and_fwd_flag,\n",
    "            NULL::INT AS PULocationID,\n",
    "            NULL::INT AS DOLocationID,\n",
    "            NULL::INT AS payment_type,\n",
    "            NULL::FLOAT AS fare_amount,\n",
    "            NULL::FLOAT AS extra,\n",
    "            NULL::FLOAT AS mta_tax,\n",
    "            NULL::FLOAT AS tip_amount,\n",
    "            NULL::FLOAT AS tolls_amount,\n",
    "            NULL::FLOAT AS improvement_surcharge,\n",
    "            NULL::FLOAT AS total_amount,\n",
    "            NULL::FLOAT AS congestion_surcharge,\n",
    "            NULL::FLOAT AS Airport_fee\n",
    "        WHERE FALSE\n",
    "    ```\n",
    "\n",
    "#### Step 2: Data Ingestion using Python\n",
    "\n",
    "1. **Reading the Data**: We started by reading the NYC taxi data (stored in a parquet file) into a pandas DataFrame to explore and understand the structure of the data.\n",
    "\n",
    "2. **Creating a SQLAlchemy Engine**: We created a SQLAlchemy engine to connect to the PostgreSQL database where the data will be ingested.\n",
    "\n",
    "3. **Loading Data into PostgreSQL**: To load the data from the DataFrame into the PostgreSQL table, we used the `to_sql` method of pandas. We encountered some issues initially with the method not recognizing the SQLAlchemy engine object correctly. After adjusting the script, we were able to successfully load the data into the PostgreSQL table using the following script:\n",
    "\n",
    "   ```python\n",
    "   from sqlalchemy import create_engine\n",
    "   from tqdm.notebook import tqdm\n",
    "\n",
    "   # Create a SQLAlchemy engine\n",
    "   engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "\n",
    "   # Convert the DataFrame to a list of dictionaries\n",
    "   data_dict = data.to_dict('records')\n",
    "\n",
    "   # Define the INSERT query with parameter placeholders\n",
    "   query = \"\"\"INSERT INTO nyc_taxi_data \n",
    "              (VendorID, tpep_pickup_datetime, ... , congestion_surcharge, Airport_fee) \n",
    "              VALUES \n",
    "              (:VendorID, :tpep_pickup_datetime, ... , :congestion_surcharge, :Airport_fee)\"\"\"\n",
    "\n",
    "   # Batch insert data into PostgreSQL\n",
    "   batch_size = 1000\n",
    "   batches = [data_dict[i:i + batch_size] for i in range(0, len(data_dict), batch_size)]\n",
    "   \n",
    "   # Execute the query with each batch of records in the DataFrame\n",
    "   for batch in tqdm(batches):\n",
    "       engine.execute(text(query), batch)\n",
    "   ```\n",
    "\n",
    "   Here, we batched the data insert operation to insert multiple rows at a time, making the process more efficient. We also used `tqdm` to display a progress bar during the data ingestion process.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f735546-79bd-4334-8296-c86930516c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "\n",
    "# Define the INSERT query with parameter placeholders\n",
    "query = \"\"\"INSERT INTO nyc_taxi_data \n",
    "           (VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, \n",
    "            trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, \n",
    "            payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, \n",
    "            improvement_surcharge, total_amount, congestion_surcharge, Airport_fee) \n",
    "           VALUES \n",
    "           (:VendorID, :tpep_pickup_datetime, :tpep_dropoff_datetime, :passenger_count, \n",
    "            :trip_distance, :RatecodeID, :store_and_fwd_flag, :PULocationID, :DOLocationID, \n",
    "            :payment_type, :fare_amount, :extra, :mta_tax, :tip_amount, :tolls_amount, \n",
    "            :improvement_surcharge, :total_amount, :congestion_surcharge, :Airport_fee)\"\"\"\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data_dict = data.to_dict('records')\n",
    "\n",
    "# Determine the batch size\n",
    "batch_size = 10000\n",
    "batches = [data_dict[i:i + batch_size] for i in range(0, len(data_dict), batch_size)]\n",
    "\n",
    "# Execute the query with each batch of records in the DataFrame\n",
    "for batch in tqdm(batches):\n",
    "    engine.execute(text(query), batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833ac44-2e05-460a-b968-d0f152ce854b",
   "metadata": {},
   "source": [
    "## Setting Up Data Quality Tests with dbt\n",
    "\n",
    "After successfully ingesting the NYC Taxi data into a PostgreSQL database, the next step in ensuring data reliability is to set up data quality tests. Data quality tests help in verifying that the data meets certain quality standards before it is used in analysis or reporting. In this process, we utilized dbt (Data Build Tool), a popular open-source tool that enables data analysts and engineers to transform and test data using SQL.\n",
    "\n",
    "### Creating Custom Test Macros\n",
    "\n",
    "Before we dive into setting up tests in dbt, we created custom test macros. These macros are SQL scripts that define the logic of the data tests. We defined a custom macro to test that values in certain columns (like `fare_amount` and `total_amount`) are greater than zero, which is a basic validation check for our dataset.\n",
    "\n",
    "```sql\n",
    "        {% macro test_expression_is_greater_than_zero(model, column_name) %}\n",
    "        \n",
    "          select count(*)\n",
    "          \n",
    "          from {{ model }}\n",
    "          \n",
    "          where {{ column_name }} <= 0\n",
    "          \n",
    "        {% endmacro %}\n",
    "```\n",
    "\n",
    "### Updating the schema.yml File\n",
    "\n",
    "Next, we moved on to updating the `schema.yml` file, which is a configuration file that allows us to define various properties and tests for our dbt models. In this file, we specified the columns present in our `nyc_taxi_data` table along with the tests we wanted to run on each column. \n",
    "\n",
    "We included various tests such as:\n",
    "- `not_null`: To check that certain columns do not contain null values.\n",
    "- `accepted_values`: To verify that values in a column match one of a set of accepted values.\n",
    "- Custom tests: To ensure that values in columns like `fare_amount` and `total_amount` are greater than zero.\n",
    "\n",
    "```yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: nyc_taxi_data\n",
    "    description: \"The raw NYC Taxi data ingested into the database\"\n",
    "    columns:\n",
    "      - name: vendorid\n",
    "        description: \"The unique identifier for the vendor\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: tpep_pickup_datetime\n",
    "        description: \"The pickup datetime for the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: tpep_dropoff_datetime\n",
    "        description: \"The dropoff datetime for the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: passenger_count\n",
    "        description: \"The number of passengers in the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values:\n",
    "              values: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "      - name: trip_distance\n",
    "        description: \"The trip distance of the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: pulocationid\n",
    "        description: \"The pickup location ID\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: dolocationid\n",
    "        description: \"The dropoff location ID\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: fare_amount\n",
    "        description: \"The fare amount for the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - expression_is_greater_than_zero\n",
    "      - name: total_amount\n",
    "        description: \"The total amount for the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - expression_is_greater_than_zero\n",
    "```\n",
    "\n",
    "### Running the Tests\n",
    "\n",
    "After setting up the `schema.yml` file, we ran the dbt tests using the command `dbt test`. This command checks the data in the database against the tests defined in the `schema.yml` file and returns the results.\n",
    "\n",
    "### Test Results\n",
    "\n",
    "The test results indicated that there were records in the dataset where the `total_amount` and `fare_amount` were less than or equal to zero. These tests help in identifying potential data quality issues, which can then be addressed to maintain the reliability and accuracy of the dataset.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Setting up data quality tests using dbt is a vital step in building a reliable data pipeline. These tests help in identifying and rectifying data issues early in the data pipeline, ensuring that only high-quality data is used in further analysis and reporting.\n",
    "\n",
    "In the next steps, we would look into rectifying the identified data quality issues and potentially setting up more complex data tests to further ensure the reliability of our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c3da8-dd92-4c10-be35-3d08e44395a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
