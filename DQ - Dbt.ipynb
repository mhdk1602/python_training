{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd52bbe6-c136-48ca-9b49-8867af6a985c",
   "metadata": {},
   "source": [
    "# Introduction to Data Quality and Validation Frameworks \n",
    "\n",
    "Data quality is a critical aspect of data engineering and analytics. Ensuring data is accurate, consistent, and timely helps in making informed decisions based on reliable insights. Data validation frameworks come into play to facilitate the implementation of data quality checks throughout data pipelines. They assist in defining, monitoring, and validating data quality rules, thereby ensuring the reliability and trustworthiness of the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac43c91-a8eb-4311-a86a-9a6e51d4778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dbt-postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be83d00-e9bb-40b0-b740-94fdb5231f70",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "If you are planning to install dbt using `pip install dbt`, please be aware that this method is no longer supported as of version v1.0.0. Attempting to do so will raise an explicit error, and the dbt package on PyPI has ceased receiving updates. Previously, from version 0.13, the PyPI dbt package was simply a pass-through of `dbt-core` and the four original database adapter plugins.\n",
    "In the fall of 2023, the dbt package on PyPI transitioned to being a supported method for installing the <a href='https://docs.getdbt.com/docs/cloud/cloud-cli-installation?install=pip#install-dbt-cloud-cli-in-pip'>dbt cloud CLI</a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c014a-e2c9-4460-91f6-214620f26845",
   "metadata": {},
   "source": [
    "## dbt (data build tool) in Data Quality\n",
    "\n",
    "[dbt (data build tool)](https://www.getdbt.com/) is a popular open-source software tool that enables data analysts and engineers to transform and test data in the data warehouse. dbt allows for defining, documenting, and executing data transformation workflows, making it a powerful tool for data pipeline orchestration. Here's how dbt stands as a vital tool in ensuring data quality:\n",
    "\n",
    "### 1. Data Transformation\n",
    "\n",
    "- **SQL-Based Transformations**: dbt leverages the power of SQL for data transformation, allowing for the creation of complex data models with ease.\n",
    "- **Version Control**: dbt supports version control of data models, enabling tracking of changes and facilitating collaboration among team members.\n",
    "\n",
    "### 2. Data Testing\n",
    "\n",
    "- **Built-in Data Tests**: dbt offers a range of built-in tests that can be easily implemented to check data quality, such as testing for uniqueness, not_null, and referential integrity.\n",
    "- **Custom Data Tests**: Apart from built-in tests, dbt allows for the creation of custom data tests, enabling the definition of business-specific data quality rules.\n",
    "\n",
    "### 3. Documentation and Data Lineage\n",
    "\n",
    "- **Automatic Documentation**: dbt automatically generates documentation for the data models, providing a clear view of the data structure and transformations.\n",
    "- **Data Lineage**: dbt supports the visualization of data lineage, helping in understanding the flow of data and dependencies between different data models.\n",
    "\n",
    "### 4. Integration with Data Pipelines\n",
    "\n",
    "- **Automation and Scheduling**: dbt can be integrated into data pipelines for automated execution of data transformations and tests, ensuring data quality checks are performed in each run.\n",
    "- **Compatibility with Various Data Warehouses**: dbt supports various data warehouses, making it a flexible choice for different data environments.\n",
    "\n",
    "In the subsequent sections, we will demonstrate how to set up a mock data pipeline, ingest data into a PostgreSQL database, and use dbt to implement data quality checks and validations, showcasing the best practices for incorporating data quality checks in data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f28541c-65bf-4b5b-b591-00ac17a0d19b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2024-01-01 00:57:55   2024-01-01 01:17:43              1.0   \n",
      "1         1  2024-01-01 00:03:00   2024-01-01 00:09:36              1.0   \n",
      "2         1  2024-01-01 00:17:06   2024-01-01 00:35:01              1.0   \n",
      "3         1  2024-01-01 00:36:38   2024-01-01 00:44:56              1.0   \n",
      "4         1  2024-01-01 00:46:51   2024-01-01 00:52:57              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           1.72         1.0                  N           186            79   \n",
      "1           1.80         1.0                  N           140           236   \n",
      "2           4.70         1.0                  N           236            79   \n",
      "3           1.40         1.0                  N            79           211   \n",
      "4           0.80         1.0                  N           211           148   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             2         17.7    1.0      0.5        0.00           0.0   \n",
      "1             1         10.0    3.5      0.5        3.75           0.0   \n",
      "2             1         23.3    3.5      0.5        3.00           0.0   \n",
      "3             1         10.0    3.5      0.5        2.00           0.0   \n",
      "4             1          7.9    3.5      0.5        3.20           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
      "0                    1.0         22.70                   2.5          0.0  \n",
      "1                    1.0         18.75                   2.5          0.0  \n",
      "2                    1.0         31.30                   2.5          0.0  \n",
      "3                    1.0         17.00                   2.5          0.0  \n",
      "4                    1.0         16.10                   2.5          0.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2964624 entries, 0 to 2964623\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int32         \n",
      " 1   tpep_pickup_datetime   datetime64[us]\n",
      " 2   tpep_dropoff_datetime  datetime64[us]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             float64       \n",
      " 6   store_and_fwd_flag     object        \n",
      " 7   PULocationID           int32         \n",
      " 8   DOLocationID           int32         \n",
      " 9   payment_type           int64         \n",
      " 10  fare_amount            float64       \n",
      " 11  extra                  float64       \n",
      " 12  mta_tax                float64       \n",
      " 13  tip_amount             float64       \n",
      " 14  tolls_amount           float64       \n",
      " 15  improvement_surcharge  float64       \n",
      " 16  total_amount           float64       \n",
      " 17  congestion_surcharge   float64       \n",
      " 18  Airport_fee            float64       \n",
      "dtypes: datetime64[us](2), float64(12), int32(3), int64(1), object(1)\n",
      "memory usage: 395.8+ MB\n",
      "None\n",
      "           VendorID        tpep_pickup_datetime       tpep_dropoff_datetime  \\\n",
      "count  2.964624e+06                     2964624                     2964624   \n",
      "mean   1.754204e+00  2024-01-17 00:46:36.431092  2024-01-17 01:02:13.208130   \n",
      "min    1.000000e+00         2002-12-31 22:59:39         2002-12-31 23:05:41   \n",
      "25%    2.000000e+00  2024-01-09 15:59:19.750000         2024-01-09 16:16:23   \n",
      "50%    2.000000e+00  2024-01-17 10:45:37.500000  2024-01-17 11:03:51.500000   \n",
      "75%    2.000000e+00  2024-01-24 18:23:52.250000         2024-01-24 18:40:29   \n",
      "max    6.000000e+00         2024-02-01 00:01:15         2024-02-02 13:56:52   \n",
      "std    4.325902e-01                         NaN                         NaN   \n",
      "\n",
      "       passenger_count  trip_distance    RatecodeID  PULocationID  \\\n",
      "count     2.824462e+06   2.964624e+06  2.824462e+06  2.964624e+06   \n",
      "mean      1.339281e+00   3.652169e+00  2.069359e+00  1.660179e+02   \n",
      "min       0.000000e+00   0.000000e+00  1.000000e+00  1.000000e+00   \n",
      "25%       1.000000e+00   1.000000e+00  1.000000e+00  1.320000e+02   \n",
      "50%       1.000000e+00   1.680000e+00  1.000000e+00  1.620000e+02   \n",
      "75%       1.000000e+00   3.110000e+00  1.000000e+00  2.340000e+02   \n",
      "max       9.000000e+00   3.127223e+05  9.900000e+01  2.650000e+02   \n",
      "std       8.502817e-01   2.254626e+02  9.823219e+00  6.362391e+01   \n",
      "\n",
      "       DOLocationID  payment_type   fare_amount         extra       mta_tax  \\\n",
      "count  2.964624e+06  2.964624e+06  2.964624e+06  2.964624e+06  2.964624e+06   \n",
      "mean   1.651167e+02  1.161271e+00  1.817506e+01  1.451598e+00  4.833823e-01   \n",
      "min    1.000000e+00  0.000000e+00 -8.990000e+02 -7.500000e+00 -5.000000e-01   \n",
      "25%    1.140000e+02  1.000000e+00  8.600000e+00  0.000000e+00  5.000000e-01   \n",
      "50%    1.620000e+02  1.000000e+00  1.280000e+01  1.000000e+00  5.000000e-01   \n",
      "75%    2.340000e+02  1.000000e+00  2.050000e+01  2.500000e+00  5.000000e-01   \n",
      "max    2.650000e+02  4.000000e+00  5.000000e+03  1.425000e+01  4.000000e+00   \n",
      "std    6.931535e+01  5.808686e-01  1.894955e+01  1.804102e+00  1.177600e-01   \n",
      "\n",
      "         tip_amount  tolls_amount  improvement_surcharge  total_amount  \\\n",
      "count  2.964624e+06  2.964624e+06           2.964624e+06  2.964624e+06   \n",
      "mean   3.335870e+00  5.270212e-01           9.756319e-01  2.680150e+01   \n",
      "min   -8.000000e+01 -8.000000e+01          -1.000000e+00 -9.000000e+02   \n",
      "25%    1.000000e+00  0.000000e+00           1.000000e+00  1.538000e+01   \n",
      "50%    2.700000e+00  0.000000e+00           1.000000e+00  2.010000e+01   \n",
      "75%    4.120000e+00  0.000000e+00           1.000000e+00  2.856000e+01   \n",
      "max    4.280000e+02  1.159200e+02           1.000000e+00  5.000000e+03   \n",
      "std    3.896551e+00  2.128310e+00           2.183645e-01  2.338558e+01   \n",
      "\n",
      "       congestion_surcharge   Airport_fee  \n",
      "count          2.824462e+06  2.824462e+06  \n",
      "mean           2.256122e+00  1.411611e-01  \n",
      "min           -2.500000e+00 -1.750000e+00  \n",
      "25%            2.500000e+00  0.000000e+00  \n",
      "50%            2.500000e+00  0.000000e+00  \n",
      "75%            2.500000e+00  0.000000e+00  \n",
      "max            2.500000e+00  1.750000e+00  \n",
      "std            8.232747e-01  4.876239e-01  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/Users/malempatiharidines/Code/GitHub/training/python_training/datasets/yellow_tripdata_2023-12.parquet\"\n",
    "data = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(data.head())\n",
    "\n",
    "# Get basic information about the dataset\n",
    "print(data.info())\n",
    "\n",
    "# Get descriptive statistics for the numerical columns\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ad0fa-9c97-4393-9fc4-3aa54a70b527",
   "metadata": {},
   "source": [
    "### Reading and Profiling the Data\n",
    "\n",
    "Before we embark on setting up data transformations and quality checks with dbt, it's imperative to understand the dataset we are working with. In this section, we read the NYC Taxi Trip dataset for January 2024 and performed a cursory data profiling to get acquainted with the data structure and contents.\n",
    "\n",
    "#### Step 1: Reading the Data\n",
    "\n",
    "Using pandas, a popular data manipulation library in Python, we read the dataset from the parquet file. Here's the snippet of Python code we used to read the data:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"file_path/yellow_tripdata_2024-01.parquet\"\n",
    "data = pd.read_parquet(file_path)\n",
    "```\n",
    "\n",
    "#### Step 2: Basic Data Profiling\n",
    "\n",
    "After loading the data, we performed some basic profiling to understand the structure and contents of the dataset. We used the following commands to explore the data:\n",
    "\n",
    "```python\n",
    "# Display the first few rows of the dataframe\n",
    "print(data.head())\n",
    "\n",
    "# Get basic information about the dataset\n",
    "print(data.info())\n",
    "\n",
    "# Get descriptive statistics for the numerical columns\n",
    "print(data.describe())\n",
    "```\n",
    "\n",
    "#### Data Overview\n",
    "\n",
    "From the data profiling, we observed that the dataset contains 2,964,624 records and 19 columns, including details about the trip (pickup and dropoff times, locations), fare breakdown (amount, tips, tolls), and other attributes like payment type and rate code. \n",
    "\n",
    "Understanding the data's structure and contents will guide us in setting up appropriate data transformations and quality checks in the subsequent steps, where we will be using dbt to implement data quality checks and validations.\n",
    "\n",
    "In the next section, we will proceed to set up dbt and create transformation models to clean and structure the data, preparing it for data quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c412d9-3cf4-4e24-88fa-468fe1bc4cb2",
   "metadata": {},
   "source": [
    "### Setting Up dbt (data build tool)\n",
    "\n",
    "In this section, we will focus on setting up dbt, a popular tool for data transformations and testing in the data warehouse. dbt allows us to define, document, and execute data transformation workflows, making it a powerful tool for setting up data quality checks.\n",
    "\n",
    "#### Step 1: Installing dbt\n",
    "\n",
    "Before we can start using dbt, it needs to be installed in your Python environment. You can install dbt using the following command:\n",
    "\n",
    "```shell\n",
    "pip install dbt\n",
    "```\n",
    "\n",
    "This command will install dbt along with its dependencies, preparing your environment for setting up a dbt project.\n",
    "\n",
    "#### Step 2: Initializing a dbt Project\n",
    "\n",
    "Once dbt is installed, the next step is to initialize a dbt project. Navigate to the directory where you want to create your dbt project and run the following command:\n",
    "\n",
    "```shell\n",
    "dbt init my_dbt_project\n",
    "```\n",
    "\n",
    "This command will create a new dbt project with the necessary directory structure and configuration files to get started with dbt.\n",
    "\n",
    "#### Step 3: Configuring the dbt Profile\n",
    "\n",
    "To connect dbt to your PostgreSQL database, you need to configure the dbt profile. The profile configuration file is located at `~/.dbt/profiles.yml`. In this file, you'll set up the connection details for your PostgreSQL database. Here's an example configuration:\n",
    "\n",
    "```yaml\n",
    "my_dbt_project:\n",
    "  target: dev\n",
    "  outputs:\n",
    "    dev:\n",
    "      type: postgres\n",
    "      host: localhost\n",
    "      user: your_username\n",
    "      pass: your_password\n",
    "      port: 5432\n",
    "      dbname: your_database_name\n",
    "      schema: your_schema_name\n",
    "```\n",
    "\n",
    "Replace `your_username`, `your_password`, `your_database_name`, and `your_schema_name` with the appropriate details for your PostgreSQL database.\n",
    "\n",
    "Once the dbt project is set up and configured, we are ready to start creating dbt models for data transformation and setting up data quality checks.\n",
    "\n",
    "In the next section, we will create dbt models to transform the raw data and set up data quality tests using dbt's testing functionalities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16360a86-f295-4f2e-be51-0f21e3359a0b",
   "metadata": {},
   "source": [
    "### Understanding dbt Components\n",
    "\n",
    "Before we delve into setting up dbt models and data ingestion, let's understand the core components of dbt:\n",
    "\n",
    "1. **Models**: Models are the central artifacts in dbt. They are essentially SQL queries defined in `.sql` files which represent the transformations that need to be applied to your source data. dbt builds models by executing these SQL queries in the specified materialization format (tables, views, etc.).\n",
    "\n",
    "2. **Macros**: Macros are reusable pieces of SQL code that can be utilized across different models. They help in avoiding code repetition and can be used to encapsulate logic that can be reused in various models.\n",
    "\n",
    "3. **Seeds**: Seeds are csv files that store static data, which can be used in transformations or to augment the source data. They are useful for storing data like mapping tables, which do not change over time.\n",
    "\n",
    "4. **Sources**: Sources in dbt are a way of documenting and testing the raw data in your warehouse. They help in defining the schema of your raw data and can be used to create tests to validate the quality of the source data.\n",
    "\n",
    "5. **Tests**: Tests in dbt are SQL queries that help in validating the data quality. You can create tests to check for things like uniqueness, not null constraints, etc., in your transformed data.\n",
    "\n",
    "6. **Snapshots**: Snapshots are used to capture historical changes in your data. They help in tracking how data changes over time by creating a snapshot of the data at regular intervals.\n",
    "\n",
    "Now, let's move on to the steps we undertook for setting up dbt models and data ingestion.\n",
    "\n",
    "### Setting Up dbt Models and Data Ingestion\n",
    "\n",
    "#### Step 1: Setting Up the dbt Model\n",
    "\n",
    "1. We first created a dbt model to define the structure of the PostgreSQL table where the data will be loaded. The model file, named `nyc_taxi_data.sql`, contains a SQL query that creates an empty table with the desired schema to hold the NYC taxi data.\n",
    "\n",
    "\n",
    "    ```sql\n",
    "        {{ config(materialized='table') }}\n",
    "        \n",
    "        SELECT \n",
    "            NULL::INT AS VendorID,\n",
    "            NULL::TIMESTAMP AS tpep_pickup_datetime,\n",
    "            NULL::TIMESTAMP AS tpep_dropoff_datetime,\n",
    "            NULL::FLOAT AS passenger_count,\n",
    "            NULL::FLOAT AS trip_distance,\n",
    "            NULL::FLOAT AS RatecodeID,\n",
    "            NULL::VARCHAR AS store_and_fwd_flag,\n",
    "            NULL::INT AS PULocationID,\n",
    "            NULL::INT AS DOLocationID,\n",
    "            NULL::INT AS payment_type,\n",
    "            NULL::FLOAT AS fare_amount,\n",
    "            NULL::FLOAT AS extra,\n",
    "            NULL::FLOAT AS mta_tax,\n",
    "            NULL::FLOAT AS tip_amount,\n",
    "            NULL::FLOAT AS tolls_amount,\n",
    "            NULL::FLOAT AS improvement_surcharge,\n",
    "            NULL::FLOAT AS total_amount,\n",
    "            NULL::FLOAT AS congestion_surcharge,\n",
    "            NULL::FLOAT AS Airport_fee\n",
    "        WHERE FALSE\n",
    "    ```\n",
    "\n",
    "#### Step 2: Data Ingestion using Python\n",
    "\n",
    "1. **Reading the Data**: We started by reading the NYC taxi data (stored in a parquet file) into a pandas DataFrame to explore and understand the structure of the data.\n",
    "\n",
    "2. **Creating a SQLAlchemy Engine**: We created a SQLAlchemy engine to connect to the PostgreSQL database where the data will be ingested.\n",
    "\n",
    "3. **Loading Data into PostgreSQL**: To load the data from the DataFrame into the PostgreSQL table, we used the `to_sql` method of pandas. We encountered some issues initially with the method not recognizing the SQLAlchemy engine object correctly. After adjusting the script, we were able to successfully load the data into the PostgreSQL table using the following script:\n",
    "\n",
    "   ```python\n",
    "   from sqlalchemy import create_engine\n",
    "   from tqdm.notebook import tqdm\n",
    "\n",
    "   # Create a SQLAlchemy engine\n",
    "   engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "\n",
    "   # Convert the DataFrame to a list of dictionaries\n",
    "   data_dict = data.to_dict('records')\n",
    "\n",
    "   # Define the INSERT query with parameter placeholders\n",
    "   query = \"\"\"INSERT INTO nyc_taxi_data \n",
    "              (VendorID, tpep_pickup_datetime, ... , congestion_surcharge, Airport_fee) \n",
    "              VALUES \n",
    "              (:VendorID, :tpep_pickup_datetime, ... , :congestion_surcharge, :Airport_fee)\"\"\"\n",
    "\n",
    "   # Batch insert data into PostgreSQL\n",
    "   batch_size = 1000\n",
    "   batches = [data_dict[i:i + batch_size] for i in range(0, len(data_dict), batch_size)]\n",
    "   \n",
    "   # Execute the query with each batch of records in the DataFrame\n",
    "   for batch in tqdm(batches):\n",
    "       engine.execute(text(query), batch)\n",
    "   ```\n",
    "\n",
    "   Here, we batched the data insert operation to insert multiple rows at a time, making the process more efficient. We also used `tqdm` to display a progress bar during the data ingestion process.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c34b4e93-5cfa-41e2-af00-e291ccf1498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/malempatiharidines/Code/GitHub/training/python_training/datasets/yellow_tripdata_2023-12.parquet\"\n",
    "data = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f735546-79bd-4334-8296-c86930516c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "\n",
    "# Define the INSERT query with parameter placeholders\n",
    "query = \"\"\"INSERT INTO nyc_taxi_data \n",
    "           (VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, \n",
    "            trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, \n",
    "            payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, \n",
    "            improvement_surcharge, total_amount, congestion_surcharge, Airport_fee) \n",
    "           VALUES \n",
    "           (:VendorID, :tpep_pickup_datetime, :tpep_dropoff_datetime, :passenger_count, \n",
    "            :trip_distance, :RatecodeID, :store_and_fwd_flag, :PULocationID, :DOLocationID, \n",
    "            :payment_type, :fare_amount, :extra, :mta_tax, :tip_amount, :tolls_amount, \n",
    "            :improvement_surcharge, :total_amount, :congestion_surcharge, :Airport_fee)\"\"\"\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data_dict = data.to_dict('records')\n",
    "\n",
    "# Determine the batch size\n",
    "batch_size = 20000\n",
    "batches = [data_dict[i:i + batch_size] for i in range(0, len(data_dict), batch_size)]\n",
    "\n",
    "# Execute the query with each batch of records in the DataFrame\n",
    "for batch in tqdm(batches):\n",
    "    engine.execute(text(query), batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833ac44-2e05-460a-b968-d0f152ce854b",
   "metadata": {},
   "source": [
    "## Setting Up Data Quality Tests with dbt\n",
    "\n",
    "After successfully ingesting the NYC Taxi data into a PostgreSQL database, the next step in ensuring data reliability is to set up data quality tests. Data quality tests help in verifying that the data meets certain quality standards before it is used in analysis or reporting. In this process, we utilized dbt (Data Build Tool), a popular open-source tool that enables data analysts and engineers to transform and test data using SQL.\n",
    "\n",
    "### Creating Custom Test Macros\n",
    "\n",
    "Before we dive into setting up tests in dbt, we created custom test macros. These macros are SQL scripts that define the logic of the data tests. We defined a custom macro to test that values in certain columns (like `fare_amount` and `total_amount`) are greater than zero, which is a basic validation check for our dataset.\n",
    "\n",
    "```sql\n",
    "        {% macro test_expression_is_greater_than_zero(model, column_name) %}\n",
    "        \n",
    "          select count(*)\n",
    "          \n",
    "          from {{ model }}\n",
    "          \n",
    "          where {{ column_name }} <= 0\n",
    "          \n",
    "        {% endmacro %}\n",
    "```\n",
    "\n",
    "### Updating the schema.yml File\n",
    "\n",
    "Next, we moved on to updating the `schema.yml` file, which is a configuration file that allows us to define various properties and tests for our dbt models. In this file, we specified the columns present in our `nyc_taxi_data` table along with the tests we wanted to run on each column. \n",
    "\n",
    "We included various tests such as:\n",
    "- `not_null`: To check that certain columns do not contain null values.\n",
    "- `accepted_values`: To verify that values in a column match one of a set of accepted values.\n",
    "- Custom tests: To ensure that values in columns like `fare_amount` and `total_amount` are greater than zero.\n",
    "\n",
    "```yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: nyc_taxi_data\n",
    "    description: \"The raw NYC Taxi data ingested into the database\"\n",
    "    columns:\n",
    "      - name: vendorid\n",
    "        description: \"The unique identifier for the vendor\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: tpep_pickup_datetime\n",
    "        description: \"The pickup datetime for the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: tpep_dropoff_datetime\n",
    "        description: \"The dropoff datetime for the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: passenger_count\n",
    "        description: \"The number of passengers in the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values:\n",
    "              values: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "      - name: trip_distance\n",
    "        description: \"The trip distance of the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: pulocationid\n",
    "        description: \"The pickup location ID\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: dolocationid\n",
    "        description: \"The dropoff location ID\"\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: fare_amount\n",
    "        description: \"The fare amount for the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - expression_is_greater_than_zero\n",
    "      - name: total_amount\n",
    "        description: \"The total amount for the taxi trip\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - expression_is_greater_than_zero\n",
    "```\n",
    "\n",
    "### Running the Tests\n",
    "\n",
    "After setting up the `schema.yml` file, we ran the dbt tests using the command `dbt test`. This command checks the data in the database against the tests defined in the `schema.yml` file and returns the results.\n",
    "\n",
    "### Test Results\n",
    "\n",
    "The test results indicated that there were records in the dataset where the `total_amount` and `fare_amount` were less than or equal to zero. These tests help in identifying potential data quality issues, which can then be addressed to maintain the reliability and accuracy of the dataset.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Setting up data quality tests using dbt is a vital step in building a reliable data pipeline. These tests help in identifying and rectifying data issues early in the data pipeline, ensuring that only high-quality data is used in further analysis and reporting.\n",
    "\n",
    "In the next steps, we would look into rectifying the identified data quality issues and potentially setting up more complex data tests to further ensure the reliability of our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a6cfb-bed7-4293-b35a-2ab9e51874d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Quality Setup in an Ingestion Pipeline using DBT\n",
    "\n",
    "Ensuring data quality is a critical aspect that necessitates meticulous attention. Setting up data quality checks during data ingestion can significantly enhance the reliability and accuracy of the data, facilitating more precise analyses and insights downstream. In this guide, we will walk through setting up data quality checks in an ingestion pipeline using DBT (Data Build Tool), that leverages SQL for data transformation and testing.\n",
    "\n",
    "### Step 1: Understanding the Data and Identifying Transformations\n",
    "\n",
    "Before diving into the technical aspects of setting up data quality checks, it's pivotal to understand the data at hand and identify potential transformations that could augment the data's utility. Based on the initial inspection of the NYC Taxi dataset, we have identified a set of transformations that could potentially unveil more insightful aspects of the data. Here are the transformations we plan to implement:\n",
    "\n",
    "#### 1. Trip Duration\n",
    "Calculate the duration of each trip in minutes. This metric is derived by finding the difference between the `tpep_pickup_datetime` and `tpep_dropoff_datetime` fields.\n",
    "\n",
    "#### 2. Speed\n",
    "Compute the average speed of the trip by dividing the `trip_distance` by the trip duration (calculated in the previous step) and converting it to an hourly rate.\n",
    "\n",
    "#### 3. Day of the Week and Hour\n",
    "Extract valuable time-based information, such as the day of the week and the hour from the `tpep_pickup_datetime`. This transformation will enable us to analyze trip patterns across different times of the week.\n",
    "\n",
    "#### 4. Fare Analysis\n",
    "Perform an analysis of the fare structure by calculating metrics such as fare per mile and fare per minute. These metrics can offer insights into the fare patterns observed in the data.\n",
    "\n",
    "#### 5. Tip Percentage\n",
    "Determine the tip percentage by calculating the tip amount as a percentage of the total fare amount. This metric can provide insights into the tipping behavior of passengers.\n",
    "\n",
    "In the following steps, we will be utilizing DBT to implement these transformations and set up data quality checks to ensure the reliability of the transformed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de5bed32-8336-4b68-9952-a1b6f183c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "import holidays\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Initialize US holidays\n",
    "us_holidays = holidays.UnitedStates(years=[2023, 2024])\n",
    "\n",
    "# Generate date series from 2023-01-01 to 2024-12-31\n",
    "start_date = date(2023, 1, 1)\n",
    "end_date = date(2024, 12, 31)\n",
    "delta = timedelta(days=1)\n",
    "current_date = start_date\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "\n",
    "# SQL Insert Query String\n",
    "insert_query = \"INSERT INTO calendar (cal_date, weekend, holiday) VALUES \"\n",
    "\n",
    "# List to hold the rows to be inserted\n",
    "values_list = []\n",
    "\n",
    "while current_date <= end_date:\n",
    "    weekend_flag = 'Y' if current_date.weekday() >= 5 else 'N'\n",
    "    holiday_flag = 'Y' if current_date in us_holidays else 'N'\n",
    "    \n",
    "    values_list.append(f\"('{current_date}', '{weekend_flag}', '{holiday_flag}')\")\n",
    "    current_date += delta\n",
    "\n",
    "# Insert all rows in values_list\n",
    "if values_list:\n",
    "    engine.execute(insert_query + ', '.join(values_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f5465-e386-4e01-996c-9c29c1504e44",
   "metadata": {},
   "source": [
    "## Creating a Calendar Table to Enhance NYC Taxi Data Analysis\n",
    "\n",
    "In any data analysis, having a detailed calendar table can be a boon. It enables the analyst to easily perform time series analysis and extract patterns related to specific time frames such as weekends or holidays. In our case, we want to examine if there's a correlation between taxi ride prices and weekends or holidays in the New York City Taxi data for the years 2023 and 2024.\n",
    "\n",
    "### Step 1: Setting Up the Calendar Table Structure in DBT\n",
    "\n",
    "The first step in this process was to set up the structure of the calendar table using dbt. We created a dbt model with the following SQL:\n",
    "\n",
    "```sql\n",
    "{{ \n",
    "  config(\n",
    "    materialized='table'\n",
    "  )\n",
    "}}\n",
    "\n",
    "SELECT \n",
    "  NULL::DATE as cal_date,\n",
    "  NULL::VARCHAR(1) as weekend,\n",
    "  NULL::VARCHAR(1) as holiday\n",
    "WHERE FALSE\n",
    "```\n",
    "\n",
    "This dbt model sets up a calendar table with columns `cal_date` (date type), `weekend` and `holiday` (both varchar type), but doesn't insert any data. It uses a WHERE clause that never holds true to ensure no rows are inserted.\n",
    "\n",
    "### Step 2: Generating Accurate Data Using Python\n",
    "\n",
    "The next step was to populate this table with accurate data. We used a Python script to generate a dataset for the years 2023 and 2024, marking weekends (Friday to Sunday) and US holidays accurately. We used the `holidays` Python package to get the US holidays and a loop to generate dates and check if each date is a weekend or a holiday. \n",
    "\n",
    "### Step 3: Inserting Data into PostgreSQL\n",
    "\n",
    "Once the data was generated, we used SQLAlchemy's `create_engine` to establish a connection to our PostgreSQL database and inserted the data into the `calendar` table. Here's a snippet from the Python script showing the insertion part:\n",
    "\n",
    "```python\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine('postgresql://godzilla:Mrawww@localhost:5437/monsterverse')\n",
    "\n",
    "# SQL Insert Query String\n",
    "insert_query = \"INSERT INTO calendar (cal_date, weekend, holiday) VALUES \"\n",
    "\n",
    "# List to hold the rows to be inserted\n",
    "values_list = []\n",
    "\n",
    "# Loop to generate data and append to values_list (not shown here)\n",
    "\n",
    "# Insert all rows in values_list\n",
    "if values_list:\n",
    "    engine.execute(insert_query + ', '.join(values_list))\n",
    "```\n",
    "\n",
    "This script inserts all the data in one go into the `calendar` table in PostgreSQL.\n",
    "\n",
    "In the next steps, we will be creating dbt models to perform complex transformations on the NYC Taxi data, and join it with this calendar table to analyze the correlation between ride prices and weekends/holidays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079889f5-7b33-4b9d-8549-1222f7b60558",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploring Seeding as an Alternative Data Loading Method in dbt\n",
    "\n",
    "In data transformation pipelines, it is often necessary to incorporate lookup tables to augment the main dataset with additional information. In our case, we aimed to enrich the NYC Taxi data with location details using a lookup table. Here, we explore how to use dbt's seeding functionality as an alternative method to load this lookup table into our database.\n",
    "\n",
    "### Step 1: Preparing the Seed Data\n",
    "\n",
    "First, prepare your seed data in a CSV file. In our case, we used a file named `taxi_zone_lookup.csv`, which contains details like LocationID, Borough, Zone, and Service Zone. This file serves as the data source for our lookup table.\n",
    "\n",
    "### Step 2: Configuring the Seed File\n",
    "\n",
    "Next, create a YAML file with the same name (`taxi_zone_lookup.yml`) in the `seeds` directory of your dbt project. This file holds the configuration and column properties for the seed data. Here you can specify column data types, descriptions, and also apply tests to columns to maintain data quality right from the ingestion stage. Here is a snippet showing how to define column properties and tests:\n",
    "\n",
    "```yaml\n",
    "version: 2\n",
    "\n",
    "seeds:\n",
    "  - name: taxi_zone_lookup\n",
    "    columns:\n",
    "      - name: LocationID\n",
    "        description: The ID of the location\n",
    "        tests:\n",
    "          - not_null\n",
    "          - unique\n",
    "        data_type: integer\n",
    "      # ... (other column configurations)\n",
    "```\n",
    "\n",
    "### Step 3: Seeding the Data\n",
    "\n",
    "After configuring the seed file, run the command `dbt seed` in your terminal. This command instructs dbt to create a new table in your database (named `taxi_zone_lookup` in our case) and populate it with the data from the CSV file. The table will adhere to the column properties and data types defined in the YAML file.\n",
    "\n",
    "Using dbt's seeding functionality provides a streamlined and efficient method to load lookup tables or smaller datasets into your database. It allows for the enforcement of data types and initial data quality checks, setting a solid foundation for the subsequent steps in your data transformation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31ee17b-ab50-4475-b0c5-9976b170a1df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Updating the schema.yml with Seeds Entry and Escaping Double Quotes\n",
    "\n",
    "### Adding Seeds Entry to schema.yml\n",
    "\n",
    "In the dbt (data build tool) workflow, the `schema.yml` file serves as a blueprint that outlines the structure and relationships of your models and seeds. Adding a seeds entry to your `schema.yml` file allows dbt to understand and recognize the schema of your static data files. This is a crucial step in setting up data tests and validations to maintain data integrity and quality. In our case, the seeds entry was utilized to define the structure of the `taxi_zone_lookup` table. Here is a snippet from the updated `schema.yml` file (refer to `schema.yml` in the repository):\n",
    "\n",
    "```yml\n",
    "seeds:\n",
    "  - name: taxi_zone_lookup\n",
    "    columns:\n",
    "      - name: \"LocationID\"\n",
    "        description: The ID of the location\n",
    "        tests:\n",
    "          - not_null\n",
    "          - unique\n",
    "      - name: \"Borough\"\n",
    "        description: The borough where the pickup or dropoff occurred\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: \"Zone\"\n",
    "        description: The specific zone of the pickup or dropoff\n",
    "        tests:\n",
    "          - not_null\n",
    "      - name: service_zone\n",
    "        description: The service zone of the pickup or dropoff\n",
    "        tests:\n",
    "          - not_null\n",
    "```\n",
    "\n",
    "### Creating the nyc_taxi_transform Model\n",
    "\n",
    "To enrich our main dataset, `nyc_taxi_data`, we created a new model named `nyc_taxi_transform`. This model integrates data from the `calendar` and `taxi_zone_lookup` tables to add more depth and context to the taxi trip records. Here's a placeholder from the `nyc_taxi_transform.sql` file demonstrating how the data from these tables was joined:\n",
    "\n",
    "```sql\n",
    "WITH taxi_data AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        DATE_PART('hour', tpep_pickup_datetime) AS pickup_hour,\n",
    "        CASE \n",
    "            WHEN DATE_PART('dow', tpep_pickup_datetime) IN (0, 6) THEN 'Y'\n",
    "            ELSE 'N'\n",
    "        END AS weekend_flag\n",
    "    FROM {{ ref('nyc_taxi_data') }} AS n\n",
    "),\n",
    "pickup_location AS (\n",
    "    SELECT \n",
    "        *\n",
    "    FROM {{ ref('taxi_zone_lookup') }} AS pc\n",
    "),\n",
    "dropoff_location AS (\n",
    "    SELECT \n",
    "        *\n",
    "    FROM {{ ref('taxi_zone_lookup') }} AS dc\n",
    "),\n",
    "calendar_data AS (\n",
    "    SELECT \n",
    "        *\n",
    "    FROM {{ ref('calendar') }} AS c\n",
    ")\n",
    "SELECT\n",
    "    t.*,\n",
    "    pc.\\\"Borough\\\" AS pickup_borough,\n",
    "    pc.\\\"Zone\\\" AS pickup_zone,\n",
    "    pc.service_zone AS pickup_service_zone,\n",
    "    dc.\\\"Borough\\\" AS dropoff_borough,\n",
    "    dc.\\\"Zone\\\" AS dropoff_zone,\n",
    "    dc.service_zone AS dropoff_service_zone,\n",
    "    cd.weekend,\n",
    "    cd.holiday\n",
    "FROM taxi_data AS t\n",
    "LEFT JOIN pickup_location AS pc\n",
    "    ON t.pulocationid = pc.\\\"LocationID\\\"\n",
    "LEFT JOIN dropoff_location AS dc\n",
    "    ON t.dolocationid = dc.\\\"LocationID\\\"\n",
    "LEFT JOIN calendar_data AS cd\n",
    "    ON t.tpep_pickup_datetime::date = cd.cal_date\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Installing dbt_utils and its Significance\n",
    "\n",
    "Next, in our transformation pipeline, we integrated the `dbt_utils` package. This package is a collection of community-contributed macros that assist in simplifying many common SQL patterns and computations. By leveraging `dbt_utils`, we could streamline our dbt project with efficient and reusable components. Installing this package involved adding it to the `packages.yml` file and running the `dbt deps` command. Here is a snippet demonstrating the addition to the `packages.yml` file (refer to `packages.yml` in the repository):\n",
    "\n",
    "```yml\n",
    "packages:\n",
    "  - package: dbt-labs/dbt_utils\n",
    "    version: 1.1.1\n",
    "```\n",
    "\n",
    "Beyond streamlining, `dbt_utils` also significantly aids in data validation and quality checks, enhancing the reliability of the data pipelines. Here are a couple of test suite examples we utilized:\n",
    "\n",
    "1. **Recency Test (`dbt_utils.recency`)**: This test helps in ensuring the freshness of the data. By applying this test, we can validate if the most recent record in a timestamp column is within an acceptable range. It’s an excellent way to monitor data ingestion pipelines and ensure timely data updates. Here is an example of how to implement the recency test in the schema.yml file:\n",
    "\n",
    "   ```yml\n",
    "   models:\n",
    "     - name: your_model_name\n",
    "       columns:\n",
    "         - name: your_timestamp_column\n",
    "           tests:\n",
    "             - dbt_utils.recency:\n",
    "                 datepart: day\n",
    "                 interval: 1\n",
    "                 field: your_timestamp_column\n",
    "   ```\n",
    "\n",
    "2. **At Least One Test (`dbt_utils.at_least_one`)**: This test verifies that at least one record in the specified column satisfies the given condition. It's a useful check to ensure that certain essential conditions are met in the data set, serving as a guardrail for data quality. Here is how it can be set up in the schema.yml file:\n",
    "\n",
    "   ```yml\n",
    "   models:\n",
    "     - name: your_model_name\n",
    "       columns:\n",
    "         - name: your_column_name\n",
    "           tests:\n",
    "             - dbt_utils.at_least_one:\n",
    "                 condition: 'your_column_name = your_desired_value'\n",
    "   ```\n",
    "\n",
    "These tests, along with others from the `dbt_utils` package, serve as powerful tools in maintaining the integrity and reliability of our data transformation pipeline.\n",
    "\n",
    "## Enhancing the Test Suite\n",
    "\n",
    "As we progressed, we realized the necessity to enhance our test suite to maintain data integrity and ensure the reliability of our transformation pipeline. We introduced new tests in the `schema.yml` file to validate the correctness of our data at different stages of the pipeline. These tests included not-null validations, uniqueness checks, and checks to validate data against specific patterns or conditions. Here's a snippet illustrating how these tests were defined within the `schema.yml` file:\n",
    "\n",
    "```yml\n",
    "models:\n",
    "  - name: nyc_taxi_transform\n",
    "    columns:\n",
    "      - name: trip_distance\n",
    "        description: \"The distance of the trip in miles\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - expression_is_greater_than:\n",
    "              expr: \"0\"\n",
    "```\n",
    "\n",
    "## Updating profiles.yml to Incorporate Logging Path\n",
    "\n",
    "In the final stages of setting up our dbt project, we updated the `profiles.yml` file to incorporate a logging path. This modification allowed us to maintain a log of the dbt runs, capturing essential details that assist in monitoring and debugging the pipeline. The logging path is specified within the `profiles.yml` file, directing dbt to store the logs at the designated location. Below is a placeholder illustrating the addition to the `profiles.yml` file (refer to `profiles.yml` in the repository):\n",
    "\n",
    "```yml\n",
    "dbt_dq:\n",
    "  outputs:\n",
    "    dev:\n",
    "      type: postgres\n",
    "      threads: 4\n",
    "      dbname: your_database_name\n",
    "      schema: your_schema_name\n",
    "      host: your_host_address\n",
    "      user: your_username\n",
    "      pass: your_password\n",
    "      port: your_port_number\n",
    "      logs:\n",
    "        db: /path/to/your/log/directory\n",
    "```\n",
    "\n",
    "This comprehensive update to the `profiles.yml` file ensures that the dbt runs are logged, facilitating a smoother monitoring and debugging process in the data transformation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9fff6b7-9f10-4115-97a7-12507608aabb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Test_Name</th>\n",
       "      <th>Status</th>\n",
       "      <th>Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02:00:13.647716</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=0</td>\n",
       "      <td>WARN=0 ERROR=1 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02:01:04.471671</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=0</td>\n",
       "      <td>WARN=0 ERROR=1 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02:03:12.836071</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=1</td>\n",
       "      <td>WARN=0 ERROR=0 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02:06:08.052798</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=1</td>\n",
       "      <td>WARN=0 ERROR=0 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02:27:21.079319</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=1</td>\n",
       "      <td>WARN=0 ERROR=0 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02:28:27.218318</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=0</td>\n",
       "      <td>WARN=0 ERROR=4 SKIP=0 TOTAL=4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>02:38:56.784411</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=8</td>\n",
       "      <td>WARN=0 ERROR=3 SKIP=0 TOTAL=11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>02:42:23.961912</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=10</td>\n",
       "      <td>WARN=0 ERROR=1 SKIP=0 TOTAL=11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>02:43:54.604764</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=10</td>\n",
       "      <td>WARN=0 ERROR=2 SKIP=0 TOTAL=12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15:32:49.393390</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=1</td>\n",
       "      <td>WARN=0 ERROR=0 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15:54:21.273183</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=0</td>\n",
       "      <td>WARN=0 ERROR=1 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15:56:07.071999</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=1</td>\n",
       "      <td>WARN=0 ERROR=0 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16:15:55.703953</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=1</td>\n",
       "      <td>WARN=0 ERROR=0 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16:18:13.173884</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=1</td>\n",
       "      <td>WARN=0 ERROR=0 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16:42:43.286535</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=0</td>\n",
       "      <td>WARN=0 ERROR=1 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16:46:41.094118</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=0</td>\n",
       "      <td>WARN=0 ERROR=1 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16:48:34.252437</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=0</td>\n",
       "      <td>WARN=0 ERROR=1 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16:50:10.342803</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=1</td>\n",
       "      <td>WARN=0 ERROR=0 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16:58:31.606291</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=1</td>\n",
       "      <td>WARN=0 ERROR=0 SKIP=0 TOTAL=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17:09:17.006775</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=10</td>\n",
       "      <td>WARN=0 ERROR=4 SKIP=0 TOTAL=14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12:28:35.538158</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=10</td>\n",
       "      <td>WARN=0 ERROR=4 SKIP=0 TOTAL=14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12:46:35.123691</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=10</td>\n",
       "      <td>WARN=0 ERROR=4 SKIP=0 TOTAL=14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12:50:12.911727</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=10</td>\n",
       "      <td>WARN=0 ERROR=2 SKIP=0 TOTAL=12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13:01:31.263950</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=21</td>\n",
       "      <td>WARN=0 ERROR=8 SKIP=0 TOTAL=29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13:08:19.110399</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=21</td>\n",
       "      <td>WARN=0 ERROR=8 SKIP=0 TOTAL=29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13:13:51.503675</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=21</td>\n",
       "      <td>WARN=0 ERROR=8 SKIP=0 TOTAL=29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13:14:54.586027</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=21</td>\n",
       "      <td>WARN=0 ERROR=8 SKIP=0 TOTAL=29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13:19:57.939777</td>\n",
       "      <td>Done.</td>\n",
       "      <td>PASS=25</td>\n",
       "      <td>WARN=0 ERROR=4 SKIP=0 TOTAL=29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Timestamp Test_Name   Status                         Details\n",
       "0   02:00:13.647716     Done.   PASS=0   WARN=0 ERROR=1 SKIP=0 TOTAL=1\n",
       "1   02:01:04.471671     Done.   PASS=0   WARN=0 ERROR=1 SKIP=0 TOTAL=1\n",
       "2   02:03:12.836071     Done.   PASS=1   WARN=0 ERROR=0 SKIP=0 TOTAL=1\n",
       "3   02:06:08.052798     Done.   PASS=1   WARN=0 ERROR=0 SKIP=0 TOTAL=1\n",
       "4   02:27:21.079319     Done.   PASS=1   WARN=0 ERROR=0 SKIP=0 TOTAL=1\n",
       "5   02:28:27.218318     Done.   PASS=0   WARN=0 ERROR=4 SKIP=0 TOTAL=4\n",
       "6   02:38:56.784411     Done.   PASS=8  WARN=0 ERROR=3 SKIP=0 TOTAL=11\n",
       "7   02:42:23.961912     Done.  PASS=10  WARN=0 ERROR=1 SKIP=0 TOTAL=11\n",
       "8   02:43:54.604764     Done.  PASS=10  WARN=0 ERROR=2 SKIP=0 TOTAL=12\n",
       "9   15:32:49.393390     Done.   PASS=1   WARN=0 ERROR=0 SKIP=0 TOTAL=1\n",
       "10  15:54:21.273183     Done.   PASS=0   WARN=0 ERROR=1 SKIP=0 TOTAL=1\n",
       "11  15:56:07.071999     Done.   PASS=1   WARN=0 ERROR=0 SKIP=0 TOTAL=1\n",
       "12  16:15:55.703953     Done.   PASS=1   WARN=0 ERROR=0 SKIP=0 TOTAL=1\n",
       "13  16:18:13.173884     Done.   PASS=1   WARN=0 ERROR=0 SKIP=0 TOTAL=1\n",
       "14  16:42:43.286535     Done.   PASS=0   WARN=0 ERROR=1 SKIP=0 TOTAL=1\n",
       "15  16:46:41.094118     Done.   PASS=0   WARN=0 ERROR=1 SKIP=0 TOTAL=1\n",
       "16  16:48:34.252437     Done.   PASS=0   WARN=0 ERROR=1 SKIP=0 TOTAL=1\n",
       "17  16:50:10.342803     Done.   PASS=1   WARN=0 ERROR=0 SKIP=0 TOTAL=1\n",
       "18  16:58:31.606291     Done.   PASS=1   WARN=0 ERROR=0 SKIP=0 TOTAL=1\n",
       "19  17:09:17.006775     Done.  PASS=10  WARN=0 ERROR=4 SKIP=0 TOTAL=14\n",
       "20  12:28:35.538158     Done.  PASS=10  WARN=0 ERROR=4 SKIP=0 TOTAL=14\n",
       "21  12:46:35.123691     Done.  PASS=10  WARN=0 ERROR=4 SKIP=0 TOTAL=14\n",
       "22  12:50:12.911727     Done.  PASS=10  WARN=0 ERROR=2 SKIP=0 TOTAL=12\n",
       "23  13:01:31.263950     Done.  PASS=21  WARN=0 ERROR=8 SKIP=0 TOTAL=29\n",
       "24  13:08:19.110399     Done.  PASS=21  WARN=0 ERROR=8 SKIP=0 TOTAL=29\n",
       "25  13:13:51.503675     Done.  PASS=21  WARN=0 ERROR=8 SKIP=0 TOTAL=29\n",
       "26  13:14:54.586027     Done.  PASS=21  WARN=0 ERROR=8 SKIP=0 TOTAL=29\n",
       "27  13:19:57.939777     Done.  PASS=25  WARN=0 ERROR=4 SKIP=0 TOTAL=29"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Read the dbt log file\n",
    "with open('dbt/dbt_dq/logs/dbt.log', 'r') as file:\n",
    "    log_data = file.readlines()\n",
    "\n",
    "# Initialize an empty list to store test results\n",
    "test_results = []\n",
    "\n",
    "# Use regular expressions to find lines with test results\n",
    "for line in log_data:\n",
    "    match = re.search(r'(\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\[info\\s*\\] \\[MainThread\\]: (.+? (PASS|ERROR|FAIL).+)', line)\n",
    "    if match:\n",
    "        timestamp, test_info = match.groups()[0], match.groups()[1]\n",
    "        test_name, status, *details = test_info.split(' ')\n",
    "        details = ' '.join(details)\n",
    "        test_results.append([timestamp, test_name, status, details])\n",
    "\n",
    "# Create a DataFrame from the test results\n",
    "df = pd.DataFrame(test_results, columns=['Timestamp', 'Test_Name', 'Status', 'Details'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3621ed52-452d-457b-a7e0-8a8c5290f1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/1r4fv_1j0dvb38w891pxz3qw0000gn/T/ipykernel_24133/4150849072.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
      "/var/folders/4k/1r4fv_1j0dvb38w891pxz3qw0000gn/T/ipykernel_24133/4150849072.py:22: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df_resampled = df.resample('30T').sum()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert 'Timestamp' to datetime format\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "\n",
    "# Set 'Timestamp' as the index\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Function to extract status details\n",
    "def extract_status_detail(detail_str, status_key):\n",
    "    try:\n",
    "        return int(detail_str.split(f'{status_key}=')[1].split(' ')[0])\n",
    "    except (IndexError, ValueError):\n",
    "        return 0\n",
    "\n",
    "# Extract numerical data from 'Status' and 'Details' columns\n",
    "df['PASS'] = df['Details'].apply(lambda x: extract_status_detail(x, 'PASS'))\n",
    "df['WARN'] = df['Details'].apply(lambda x: extract_status_detail(x, 'WARN'))\n",
    "df['ERROR'] = df['Details'].apply(lambda x: extract_status_detail(x, 'ERROR'))\n",
    "\n",
    "# Resample the data every 30 minutes and sum the values\n",
    "df_resampled = df.resample('30T').sum()\n",
    "\n",
    "# Plot the time series data\n",
    "fig, ax = plt.subplots()\n",
    "df_resampled[['PASS', 'WARN', 'ERROR']].plot(kind='line', ax=ax)\n",
    "plt.title('Test Status Counts Every 30 Minutes')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d588af-7820-4eb3-af0e-e7298d474e9c",
   "metadata": {},
   "source": [
    "## Leveraging Python for dbt Log Analysis\n",
    "\n",
    "To maintain the quality of our data pipeline and keep track of the dbt runs, it's essential to analyze the logs generated during the dbt runs. Python, with its extensive library support, comes in handy to script the log analysis and present it in a structured manner, facilitating easy monitoring and troubleshooting. Here, we detail a Python script which parses the dbt logs to extract and display the test results:\n",
    "\n",
    "```python\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Read the dbt log file\n",
    "with open('dbt/dbt_dq/logs/dbt.log', 'r') as file:\n",
    "    log_data = file.readlines()\n",
    "\n",
    "# Initialize an empty list to store test results\n",
    "test_results = []\n",
    "\n",
    "# Use regular expressions to find lines with test results\n",
    "for line in log_data:\n",
    "    match = re.search(r'(\\d{2}:\\d{2}:\\d{2}\\.\\d{6}) \\[info\\s*\\] \\[MainThread\\]: (.+? (PASS|ERROR|FAIL).+)', line)\n",
    "    if match:\n",
    "        timestamp, test_info = match.groups()[0], match.groups()[1]\n",
    "        test_name, status, *details = test_info.split(' ')\n",
    "        details = ' '.join(details)\n",
    "        test_results.append([timestamp, test_name, status, details])\n",
    "\n",
    "# Create a DataFrame from the test results\n",
    "df = pd.DataFrame(test_results, columns=['Timestamp', 'Test_Name', 'Status', 'Details'])\n",
    "\n",
    "df\n",
    "```\n",
    "\n",
    "Here's a breakdown of the script:\n",
    "\n",
    "1. **Importing Necessary Libraries**: We import the `re` module for regex operations and `pandas` for data manipulation.\n",
    "   \n",
    "2. **Reading the dbt Log File**: We open the dbt log file in read mode and read all lines into a list named `log_data`.\n",
    "\n",
    "3. **Initializing an Empty List**: An empty list `test_results` is initialized to store the parsed test results.\n",
    "\n",
    "4. **Regex for Line Matching**: We iterate over each line in `log_data` and use a regex pattern to find lines that contain test results, extracting the timestamp and test info.\n",
    "\n",
    "5. **Parsing Test Info**: The test info string is split to extract individual components like test name, status, and any additional details.\n",
    "\n",
    "6. **Creating a DataFrame**: We use pandas to create a DataFrame from the `test_results` list, which provides a structured view of the test results extracted from the logs.\n",
    "\n",
    "The output, a pandas DataFrame, presents a structured view of the test results, making it easier to monitor the dbt run status over time and ensuring the health of our data pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20173b55-1ebf-4759-a452-5971a1008dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
